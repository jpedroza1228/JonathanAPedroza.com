{
  "hash": "24eb78815de09f525ad4b52438c1df80",
  "result": {
    "markdown": "---\ntitle: \"Prophet Model\" \nsubtitle: |\n  Creation of a Prophet model to forecast stock prices.\nimage: stock_prices.jpg\ncategories: [Visualizations, Analysis, Forecast, TidyModels, Modeltime]\ndate: 2022-06-02\n# citation:\n  # url: \nparams:\n  slug: Prophet-Model\n  date: 2022-06-02\n---\n\n\nAs I start looking for non-academic positions, I wanted to practice forecasting as I didn't really have much experience with these types of models. **NOTE: This is for practicing forecasting skills and you should not trust this model with your own stocks.** After plenty of reading,\n\n![reading book](https://media.giphy.com/media/ql6hxonaYC6EE/giphy.gif) I finally have some understanding of how to utilize these models. This post started because even after a BA, 2 masters degrees, and a doctorate, my brother still has no clue what I do. He, along with most of my family think I am a Clinical Psychologist. ![psychologist](https://media.giphy.com/media/l4EpgzkiwfFJKADLy/giphy.gif)\n\nSo for me to try and make my brother understand what I do, I thought I would show him with something that he has become interested with recently; stocks. So for this post, I'll \n\n- load stock data from [Google Finance](https://www.google.com/finance/)\n\n- visualize some data\n\n- train a prophet model \n\n- test that prophet model\n\n- see how well that model predicts other stock data\n\nBelow are all the sites for the packages I used.\n\n- [tidyverse](https://www.tidyverse.org/)\n\n- [timetk](https://business-science.github.io/timetk/)\n\n- [modeltime](https://business-science.github.io/modeltime/)\n\n- [tidymodels](https://www.tidymodels.org/)\n\n- [prophet](https://facebook.github.io/prophet/)\n\n- [googlesheets4](https://googlesheets4.tidyverse.org/)\n\n- [lubridate](https://lubridate.tidyverse.org/)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(prophet)\nlibrary(lubridate)\nlibrary(modeltime)\nlibrary(timetk)\n```\n:::\n\n\n\n## Loading Data\n\nTo load the Google Finance data, I decided to pick a stock that my brother had, which in this case was [JetBlue](https://www.google.com/finance/quote/JBLU:NASDAQ). A cool feature about Google Finance and Google Sheets is that you can use the following formula in a Google Sheet on the first cell of the first column `=GOOGLEFINANCE(\"JBLU\", \"price\", DATE(2000,1,1), DATE(2025, 1, 1), \"DAILY\")` and it will give you the date and stock closing values for whatever period you'd like. The example above provides Google financial data for `JBLU` or the abbreviation for JetBlue stock. It also provides the price of the stock from the first day that there is data on JetBlue stocks, which in this case is April 12th 2002. You can also choose the period of time for the stock prices. I decided to look at daily data. \n\n[JetBlue Sheet](https://docs.google.com/spreadsheets/d/1wYpTEQqreipZVeZRq0VV9r-o8FPgM8hGoFT_QhGCOVI/edit#gid=0)\n\nHere I have a copy of my Google Sheet for JetBlue that I will use to train and test my Prophet model. Instead of having a `.csv` file on my local machine, I decided to keep this on Google Drive so that it constantly updates with the Google Finance function. This meant that I had to use the `googlesheets4` package to load the data from a Google Sheet. I also changed the name and class of the date variable to make it a date variable instead of a date and time variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngooglesheets4::gs4_deauth()\n\ntheme_set(theme_light())\n\njet <- \n  googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1SpRXsC3kXDaQLUfC6cPIOvsqxDF6updhgHRJeT8PTog/edit#gid=0\", sheet = 1, range = \"A1:B1000\") %>% \n  janitor::clean_names() %>%\n  mutate(ds = as_date(date))\n```\n:::\n\n\n### Cleaning Up the Data\n\nBased on some visualizations below, I also decided to create some additional variables from the date variable. Specifically, I used `lubridate's wday()` function to create a new variable that gives you the actual day from the corresponding cell's date. I also used the `ts_clean_vec` function from `time_tk` to clean for outliers in the stock price values. There are additional arguments for the function, like applying a Box-Cox transformation but that is for a multiplicative trend, which this model does not appear to fit since the variation in the outcome does not grow exponentially. I'll also include 2002 as the reference year for the year variable and make sure that my data is arranged by date.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njetblue <- jet %>% \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) %>% \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %>% \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002')) %>% \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') %>%\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) %>% \n  select(-drop) %>% \n  arrange(ds)\n```\n:::\n\n\n\n## Visualizing Data\n\nStarting with some quick visualizations, we can see that the only area that there is a difference in the variation of the stock prices is in the beginning of 2020. I wonder what that could have been ![friends](https://media.giphy.com/media/lOIuDYVIo8C1y5oMQB/giphy.gif).\n\n\n::: {.cell}\n\n```{.r .cell-code}\njetblue %>% \n  group_by(year_num, month_num) %>% \n  summarize(var_value = sd(close)^2) %>% \n  ungroup() %>% \n  ggplot(aes(month_num, var_value)) + \n  geom_point() + \n  facet_wrap(vars(year_num))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/variation over time-1.png){width=672}\n:::\n:::\n\n\nNext, we can look at the histograms for the outcome of interest. If we look at the histograms, we can see that there are potential outliers in the original stock prices data. We can also see that cleaning the variable removed the potential outliers. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nonly_numeric <- jetblue %>% \n  select(close, clean)\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~ggplot(data = only_numeric,\n             aes(.x)) + \n       geom_histogram(color = 'white',\n                      fill = 'dodgerblue') +\n       geom_vline(xintercept = mean(.x) +\n                    sd(.x) +\n                    sd(.x) +\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       geom_vline(xintercept = mean(.x) -\n                    sd(.x) -\n                    sd(.x) -\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       labs(title = .y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$close\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/histograms-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n$clean\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/histograms-2.png){width=672}\n:::\n:::\n\n\nThere will also be a lot of use of the `purrr` package and the `map` functions, which are part of the tidyverse. We can also see that in the plot series visualization using `modeltime's plot_time_series` function, that the cleaned stock prices remove the outliers. So from here on out, I'll be using the cleaned stock prices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap2(only_numeric,\n     names(only_numeric),\n     ~only_numeric %>% \n       plot_time_series(jetblue$ds,\n                        .x,\n                        .interactive = FALSE) + \n       labs(title = .y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$close\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot time series-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n$clean\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot time series-2.png){width=672}\n:::\n:::\n\n\nWe can also look for anomalies, or points that deviate from the trend. Using the `plot_anomaly_diagnostics` function from the `modeltime` package, I can see all the anomalies in the data. I also used ggplot to create my own visualization using the same data. Lastly, we'll deal with those anomalies by removing them from the dataset. This is not too much of a problem because the Prophet model should be able to handle this fairly easy.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\njetblue %>% \n  plot_anomaly_diagnostics(ds,\n                           clean,\n                           .facet_ncol = 1,\n                           .interactive = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/anomaly detection-1.png){width=672}\n:::\n\n```{.r .cell-code}\njetblue %>% \n  tk_anomaly_diagnostics(ds,\n                         clean) %>% \n  ggplot(aes(ds, observed)) + \n  geom_line() + \n  geom_point(aes(color = anomaly)) +\n  viridis::scale_color_viridis(option = 'D',\n                               discrete = TRUE,\n                               begin = .5,\n                               end = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/anomaly detection-2.png){width=672}\n:::\n\n```{.r .cell-code}\nanomaly <- jetblue %>%\n  tk_anomaly_diagnostics(ds,\n                         clean)\n\njetblue <- left_join(jetblue, anomaly) %>%\n  filter(anomaly != 'Yes')\n```\n:::\n\n\nWe can also look into additional regressors to include in the model by looking into seasonality. We can see some fluctuation in stock prices across the years. We'll include the year variable as another regressor on the stock prices. \n\n\n::: {.cell}\n\n```{.r .cell-code}\njetblue %>% \n  plot_seasonal_diagnostics(ds,\n                            clean,\n                            .interactive = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/seasonality-1.png){width=672}\n:::\n:::\n\n\n## Training the Prophet Model\n\nBefore we begin, I'm going to designate 10 cores to process any models run.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05262022)\n\nparallel::detectCores()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n```\n:::\n\n```{.r .cell-code}\nparallel_start(10,\n               .method = 'parallel')\n```\n:::\n\n\nFirst, instead of the normal `initial_split` used for training and testing splits, we'll use the `initial_time_split` function from `tidymodels` to separate the first 80% of the data into training set and the other 20% into the testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05262022)\njet_split <- initial_time_split(jetblue)\n```\n:::\n\n\n### Prophet Model Function\n\nI decided to create my own Prophet function to be able to use for both training the model and testing it. In this function, I've also included parameters that can be changed to see if the model performs better or worse. Lastly, the `train = TRUE` allows us to practice with the training dataset and then when we're happy with the model, we can use it to test our model. For our model, we'll be predicting stock prices with date and comparing each year to the reference year (2002). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nprophet_mod <- function(splits,\n                        changepoints = .05,\n                        seasonality = .01,\n                        holiday = .01,\n                        season_type = 'additive',\n                        day_season = 'auto',\n                        week_season = 'auto',\n                        year_season = 'auto',\n                        train = TRUE){\n  library(tidyverse)\n  library(tidymodels)\n  library(modeltime)\n  library(prophet)\n  \n  analy_data <- analysis(splits)\n  assess_data <- assessment(splits)\n  \n  model <- prophet_reg() %>% \n    set_engine(engine = 'prophet',\n               verbose = TRUE) %>% \n    set_args(prior_scale_changepoints = changepoints,\n             prior_scale_seasonality = seasonality,\n             prior_scale_holidays = holiday,\n             season = season_type,\n             seasonality_daily = day_season,\n             seasonality_weekly = week_season,\n             seasonality_yearly = year_season) %>% \n    fit(clean ~ ds + year_num, \n        data = analy_data)\n  \n  if(train == TRUE){\n    train_cali <- model %>% \n      modeltime_calibrate(new_data = analy_data)\n    \n    train_acc <- train_cali %>% \n      modeltime_accuracy()\n    \n    return(list(train_cali, train_acc))\n  }\n  \n  else{\n    test_cali <- model %>% \n      modeltime_calibrate(new_data = assess_data)\n    \n    test_acc <- test_cali %>% \n      modeltime_accuracy()\n    \n    return(list(test_cali, test_acc))\n  }\n}\n```\n:::\n\n\nIt is worth noting that I'm using the `modeltime` package to run the prophet model because I believe it is easier to use (especially for later steps) than from Prophet but both can be implemented in this function. Let's try running this model with the some random parameters I chose from the [Prophet website](https://facebook.github.io/prophet/docs/diagnostics.html#hyperparameter-tuning) until realizing that the modeltime parameters are log transformed. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05262022)\nbaseline <- prophet_mod(jet_split,\n                 train = TRUE) %>% \n  pluck(2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nConverting to Modeltime Table.\n```\n:::\n\n```{.r .cell-code}\nbaseline\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      <int> <chr>                 <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 PROPHET W/ REGRESSORS Fitted 0.629  4.06  1.93  4.05 0.831 0.957\n```\n:::\n:::\n\n\nSo with the model, we can see that the Mean Absolute Scaled Error (MASE) is 1.9270147 and the Root Mean Square Error (RMSE) is 0.8312475. Not bad for an initial run. Let's look at how the model fits the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprophet_mod(jet_split,\n                 train = TRUE) %>%  \n  pluck(1) %>% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Prophet Baseline Model')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/baseline visual-1.png){width=672}\n:::\n:::\n\n\nSo the model appears to follow the trend line. We'll try to tune some of these parameters to see if we can make the model better. \n\n### Tuning the Model\n\nNow, I'll tune the prior scale values for the model. I'll use the `grid_latin_hypercube` from the dials package in `tidymodels` to choose 5 sets of parameter values to run. I'm also using the `rolling_origin` from the rsample package in `tidymodels` because we are working with time series data. This does not create random samples but instead has samples with data points with consecutive values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(05262022)\n\nproph_model <- prophet_reg() %>%\n  set_engine(engine = 'prophet',\n             verbose = TRUE) %>%\n  set_args(prior_scale_changepoints = tune(),\n           prior_scale_seasonality = tune(),\n           prior_scale_holidays = tune(),\n           season = 'additive',\n           seasonality_daily = 'auto',\n           seasonality_weekly = 'auto',\n           seasonality_yearly = 'auto')\n\nproph_rec <-\n  recipe(clean ~ ds + year_num,\n         data = training(jet_split))\n\n\nset.seed(05262022)\ntrain_fold <-\n  rolling_origin(training(jet_split),\n                 initial = 270,  \n                 assess = 90, \n                 skip = 30,\n                 cumulative = TRUE)\n\nset.seed(05262022)\ngrid_values <-\n  grid_latin_hypercube(prior_scale_changepoints(),\n                       prior_scale_seasonality(),\n                       prior_scale_holidays(),\n                       size = 5)\n\nset.seed(05262022)\nproph_fit <- tune_grid(object = proph_model,\n                       preprocessor = proph_rec,\n                       resamples = train_fold,\n                       grid = grid_values,\n                       control = control_grid(verbose = TRUE,\n                                              save_pred = TRUE,\n                                              allow_par = TRUE))\n\n\ntuned_metrics <- collect_metrics(proph_fit)\ntuned_metrics %>%\n  filter(.metric == 'rmse') %>% \n  arrange(mean)\n\nsaveRDS(tuned_metrics,\n        file = 'tuned_metrics.rds')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <-\n  readr::read_rds('C:/Users/cpppe/Desktop/github_projects/log-of-jandp/posts/2022-06-02-prophet-model/tuned_metrics.rds')\n\nmetrics %>% \n  filter(.metric == 'rmse') %>% \n  arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  prior_scale_changepoints prior_scale_seasonality prior_scale_holidays .metric\n                     <dbl>                   <dbl>                <dbl> <chr>  \n1                  3.53                    0.0170               1.12    rmse   \n2                  0.884                  36.4                  0.0131  rmse   \n3                  0.00139                 0.00166              0.00172 rmse   \n4                  0.0549                  0.261                0.231   rmse   \n5                 43.0                     3.80                12.2     rmse   \n# ℹ 5 more variables: .estimator <chr>, mean <dbl>, n <int>, std_err <dbl>,\n#   .config <chr>\n```\n:::\n:::\n\n\nFor the sake of not waiting for this to render, I decided to make a RDS file of the metrics gathered from the tuned Prophet model. We can see that the RMSE value was 2.4252669 and the prior scale changepoint value was 3.5347457, the prior scale seasonality value was 0.0170306, and the prior scale holiday value was 1.1198542. \n\n### Final Training Model\n\nI then decided to run the prophet model on the training dataset with the new parameter values. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_train <- prophet_mod(jet_split,\n                 changepoints = 3.53,\n                 seasonality = .017,\n                 holiday = 1.12,\n                 train = TRUE) %>%  \n  pluck(2)\n\nfinal_train\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      <int> <chr>                 <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 PROPHET W/ REGRESSORS Fitted 0.501  3.24  1.54  3.23 0.659 0.973\n```\n:::\n\n```{.r .cell-code}\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = TRUE) %>%  \n  pluck(1) %>% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Training Model')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/final train-1.png){width=672}\n:::\n:::\n\n\nWe can see that when using the whole training set, we have a RMSE of 0.6588232 and a MASE of 1.5354125 so both metrics reduced slightly.\n\n## Testing the Model\n\nFinally, let's test our Prophet model to see how well the model fits. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %>%\n  pluck(1) %>% \n  modeltime_forecast(new_data = testing(jet_split),\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Testing Model')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/testing-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntest_model <- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %>%\n  pluck(2)\n\ntest_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n  .model_id .model_desc           .type   mae  mape  mase smape  rmse    rsq\n      <int> <chr>                 <chr> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1         1 PROPHET W/ REGRESSORS Test   1.11  8.80  4.57  8.58  1.35 0.0196\n```\n:::\n:::\n\n\nWell, that doesn't look very good and we can see that with the metrics. The MASE has gotten much worse (4.5736587) and so has the RMSE (1.3542052) ![sweating](https://media.giphy.com/media/32mC2kXYWCsg0/giphy.gif) \n\n## Forecasting Ahead a Year\n\nWell our model did not fit well to the testing data, but let's see how it model looks when refit to the full data and forecasted forward a year. So in a year, it seems that JetBlue stock will remain roughly around the same value. It is important to note that the confidence intervals are large and with 95% confidence that values could be between 52.49 and -28.39 (not possible), there is not much confidence that JetBlue stock prices will remain where they are now in a year.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture <- jetblue %>% \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture <-\n  future %>%\n  select(-year_num, -month_num, -day_num) %>%\n  mutate(date2 = ds) %>%\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %>%\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num)) %>% \n  arrange(ds)\n\nglimpse(future)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,357\nColumns: 17\n$ close         <dbl> 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ ds            <date> 2002-04-12, 2002-04-15, 2002-04-16, 2002-04-17, 2002-04…\n$ actual_day    <ord> Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, M…\n$ clean         <dbl> 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ observed      <dbl> 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ season        <dbl> 0.017592887, 0.001065775, -0.001828751, -0.005043169, -0…\n$ trend         <dbl> 13.47847, 13.48774, 13.49702, 13.50629, 13.51556, 13.524…\n$ remainder     <dbl> -0.16606354, -0.08880934, 0.07481227, -0.14124623, -0.40…\n$ seasadj       <dbl> 13.31241, 13.39893, 13.57183, 13.36504, 13.11179, 12.912…\n$ remainder_l1  <dbl> -3.129268, -3.129268, -3.129268, -3.129268, -3.129268, -…\n$ remainder_l2  <dbl> 3.177322, 3.177322, 3.177322, 3.177322, 3.177322, 3.1773…\n$ anomaly       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ recomposed_l1 <dbl> 10.36680, 10.35954, 10.36592, 10.37198, 10.37451, 10.413…\n$ recomposed_l2 <dbl> 16.67339, 16.66613, 16.67251, 16.67857, 16.68110, 16.719…\n$ year_num      <fct> 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 20…\n$ month_num     <fct> 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 05, …\n$ day_num       <dbl> 12, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 29, 30, 1, 2…\n```\n:::\n\n```{.r .cell-code}\ntest_model1 <- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %>%\n  pluck(1)\n\ntest_model1 %>% \n  modeltime_refit(data = future) %>% \n  modeltime_forecast(new_data = future,\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Forecasted JetBlue Stock Prices')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/forecasting-1.png){width=672}\n:::\n:::\n\n\n# Testing the Algorithm on a Different Airline's Stock Prices\n\nLet's take this a step further and see how well our algorithm fits on a different airline's stock price data. We will use the final Prophet model to see if it works well using all of American Airlines data to make predictions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlink <- 'https://docs.google.com/spreadsheets/d/11DWSWLFXT84uGg_mBvVYJevQOsN7ghYovJefH87BJXc/edit#gid=0'\n\namer <- \n  googlesheets4::read_sheet(link, , sheet = 1, range = \"A1:B1000\") %>% \n  janitor::clean_names() %>% \n  mutate(ds = as_date(date))\n\namerican <-\n  amer %>% \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) %>% \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %>% \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2013')) %>% \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') %>%\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) %>% \n  select(-drop) %>% \n  arrange(ds)\n\n\nmodel <- prophet_reg() %>% \n    set_engine(engine = 'prophet',\n               verbose = TRUE) %>% \n    set_args(prior_scale_changepoints = 3.53,\n             prior_scale_seasonality = .017,\n             prior_scale_holidays = 1.12,\n             season = 'additive',\n             seasonality_daily = 'auto',\n             seasonality_weekly = 'auto',\n             seasonality_yearly = 'auto') %>% \n    fit(clean ~ ds + year_num, \n        data = american)\n\nmodel_cali <- model %>% \n  modeltime_calibrate(new_data = american)\n    \nmodel_cali %>% \n  modeltime_accuracy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      <int> <chr>                 <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 PROPHET W/ REGRESSORS Fitted  1.29  3.12  1.86  3.11  1.65 0.931\n```\n:::\n\n```{.r .cell-code}\nfuture_amer <- american %>% \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture_amer <-\n  future_amer %>%\n  select(-year_num, -month_num, -day_num) %>%\n  mutate(date2 = ds) %>%\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %>%\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2013'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num))\n\nmodel_cali %>% \n  modeltime_forecast(new_data = american,\n                     actual_data = american) %>% \n  plot_modeltime_forecast(.interactive = FALSE)  + \n  labs(title = 'Predicted American Airlines Stock Prices')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/american airlines-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel_cali %>% \n  modeltime_refit(data = future_amer) %>%\n  modeltime_forecast(new_data = future_amer,\n                     actual_data = american) %>% \n  plot_modeltime_forecast(.interactive = FALSE) + \n  labs(title = 'Forecasted American Airlines Stock Prices')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/american airlines-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}