{
  "hash": "2c7be485a4e8ce82a1b6a00e94765d80",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes Net Pt. 2\" \nsubtitle: |\n  Estimation\nimage: /posts/2024-03-09-bayes-net-part2-estimation/network_playground.jpg\ncategories: [bayesian, bayesian network, bayes net, R, stan, cmdstanr]\ndate: 2024-03-16\n# citation:\n  # url: \nparams:\n  slug: Bayes-Net-part-2\n  date: 2024-03-16\n---\n\n\n**Under Development - Not Complete**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n```\n:::\n\n\n\n\n\ndata {\n  int<lower=1> J; // number of examinees\n  int<lower=1> I; // number of items\n  int<lower=1> K; // number of latent variables\n  int<lower=1> C; // number of classes\n  matrix[J, I] X; // response matrix x\n  matrix[I, K] Q; // Q matrix Q\n  matrix[C, K] alpha; // attribute profile matrix\n}\nparameters {\n  simplex[C] nu; // class probabilities\n  vector<lower=0, upper=1>[I] false_pos;\n  vector<lower=0, upper=1>[I] true_pos;\n  real<lower=0, upper=1> lambda1;\n  real<lower=0, upper=1> lambda20;\n  real<lower=0, upper=1> lambda21;\n  real<lower=0, upper=1> lambda30;\n  real<lower=0, upper=1> lambda31;\n  real<lower=0, upper=1> lambda40;\n  real<lower=0, upper=1> lambda41;\n  real<lower=0, upper=1> lambda50;\n  real<lower=0, upper=1> lambda51;\n}\ntransformed parameters {\n  vector[C] log_nu;\n  log_nu = log(nu);\n}\nmodel {\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[2] theta_log4;\n  vector[2] theta_log5;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  vector[C] theta4;\n  vector[C] theta5;\n  matrix[I, C] delta;\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(25, 5);\n  lambda20 ~ beta(10, 20);\n  lambda21 ~ beta(20, 10);\n  lambda30 ~ beta(5, 25);\n  lambda31 ~ beta(25, 5);\n  lambda40 ~ beta(5, 25);\n  lambda41 ~ beta(25, 5);\n  lambda50 ~ beta(12, 18);\n  lambda51 ~ beta(18, 12);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(4, 26);\n    true_pos[i] ~ beta(26, 4);\n  }\n  \n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  theta_log4[1] = bernoulli_lpmf(1 | lambda40);\n  theta_log4[2] = bernoulli_lpmf(1 | lambda41);\n  \n  theta_log5[1] = bernoulli_lpmf(1 | lambda50);\n  theta_log5[2] = bernoulli_lpmf(1 | lambda51);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] > 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] > 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] > 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n    if (alpha[c, 4] > 0) {\n      theta4[c] = theta_log4[2];\n    } else {\n      theta4[c] = theta_log4[1];\n    }\n    if (alpha[c, 5] > 0) {\n      theta5[c] = theta_log5[2];\n    } else {\n      theta5[c] = theta_log5[1];\n    }\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4])\n                      * pow(exp(theta5[c]), Q[i, 5]);\n        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\ngenerated quantities {\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[2] theta_log4;\n  vector[2] theta_log5;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  vector[C] theta4;\n  vector[C] theta5;\n  matrix[I, C] delta;\n  real pie;\n  vector[I] log_item;\n  \n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  theta_log4[1] = bernoulli_lpmf(1 | lambda40);\n  theta_log4[2] = bernoulli_lpmf(1 | lambda41);\n  \n  theta_log5[1] = bernoulli_lpmf(1 | lambda50);\n  theta_log5[2] = bernoulli_lpmf(1 | lambda51);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] > 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] > 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] > 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n    if (alpha[c, 4] > 0) {\n      theta4[c] = theta_log4[2];\n    } else {\n      theta4[c] = theta_log4[1];\n    }\n    if (alpha[c, 5] > 0) {\n      theta5[c] = theta_log5[2];\n    } else {\n      theta5[c] = theta_log5[1];\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4])\n                      * pow(exp(theta5[c]), Q[i, 5]);\n        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}