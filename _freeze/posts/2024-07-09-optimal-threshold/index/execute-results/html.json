{
  "hash": "49b5671a410a46aa912181e342f37fd4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Finding Optimal Thresholds\" \nsubtitle: |\n  Using Support Vector Machines\nimage: \"buttons.jpg\"\ncategories: [svm, classification, optimal threshold, threshold]\ndate: 2024-07-09\n# citation:\n  # url: \nparams:\n  slug: optimal-threshold\n  date: 2024-07-09\n---\n\n\nNow that I'm playing catch up with some posts I have wanted to write, I thought now would be an excellent time to write about this method I have been trying out to figure out the closest optimal threshold. While I found other ways to find the optimal threshold at a much faster rate, I still thought this was an interesting use of machine learning to try and figure out the optimal threshold. Particularly, this is a method to try and find an optimal threshold when the truth is not known. From what I could find there was not much literature on trying to find an optimal threshold when the truth was not known. So I'll first show this method when the truth is known followed by trying this method out when there is no truth.\n\n## Fabricating Some Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(e1071)\ncat_map <- purrr::map\n\nn <- 500\nseed <- 12345\ntruth <- rbinom(n = n, size = 1, prob = .6)\nestimates <- bayestestR::distribution_beta(n = n, shape1 = 12, shape2 = 18)\n\ndata <- tibble(\n  estimates = estimates,\n  truth = truth\n)\n\ndata |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  estimates truth\n      <dbl> <int>\n1     0.160     1\n2     0.181     1\n3     0.192     0\n4     0.200     1\n5     0.206     0\n6     0.212     1\n```\n\n\n:::\n\n```{.r .cell-code}\ndata_train <- data |> slice_sample(prop = .75)\ndata_test <- anti_join(data, data_train)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(estimates, truth)`\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed)\nclass_weights <- data_train |> count(truth) |> arrange(n)\ncls_weights <- class_weights[2, 2]/class_weights[1, 2]\n\nsvm_mod <- svm(\n  truth ~ estimates,\n  data = data_train,\n  type = \"C-classification\",\n  kernel = \"radial\",\n  cost = 10,\n  class.weights = c(\"0\" = as.numeric(cls_weights), \"1\" = 1),\n  probability = TRUE\n)\nsummary(svm_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm(formula = truth ~ estimates, data = data_train, type = \"C-classification\", \n    kernel = \"radial\", cost = 10, class.weights = c(`0` = as.numeric(cls_weights), \n        `1` = 1), probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  10 \n\nNumber of Support Vectors:  329\n\n ( 128 201 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsvm_pred <- predict(svm_mod, data_test)\nsvm_pred |> as_tibble() |> count(value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  value     n\n  <fct> <int>\n1 0        53\n2 1        72\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dbl(\n  data$estimates,\n  ~rbinom(n = 1, size = 1, prob = .x)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n [38] 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1\n [75] 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n[112] 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1\n[149] 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0\n[186] 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n[223] 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n[260] 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1\n[297] 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n[334] 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n[371] 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n[408] 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n[445] 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1\n[482] 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprobably_threshold <- function(\n  r6,\n  task,\n  threshold,\n  output = TRUE\n){\n  thresholds <- c(threhold, 1 - threshold)\n  names(thresholds) <- {{task}}$class_names\n\n  if(output == TRUE){\n    confuse <- {{r6}}$clone(deep = TRUE)$set_threshold(threhsolds)$confusion |> t()\n    score <- {{r6}}$clone(deep = TRUE)$set_threshold$score(msrs(c(\"classif.mcc\", \"classif.acc\", \"classif.auc\", \"classif.ce\", \"classif.sensitivity\", \"classif.specificity\")))\n\n    list(confuse, score)\n  }\n  else{\n    preds <- {{r6}}$clone(deep = TRUE)$set_threshold(thresholds)\n\n    preds\n  }\n}\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}