{
  "hash": "8d08f2330439e632d83f241b2e1184a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes Net Pt. 2\" \nsubtitle: |\n  Estimation\nimage: /posts/2024-07-10-bayes-net-part2-estimation/network_playground.jpg\ncategories: [Bayesian, Bayesian Network, bayes net, R, stan, cmdstanr]\ndate: 2024-07-10\n# citation:\n  # url: \nparams:\n  slug: Bayes-Net-part-2\n  date: 2024-07-10\n---\n\n\n**Under Development - Not Complete**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table <- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n```\n:::\n\n\n\nAs mentioned in the [previous post](https://log-of-jandp.com/posts/2024-07-09-bayes-net-introduction/), the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the `cmdstanr` package to call on Stan for the Bayesian analyses, I am using the `posterior` package to manipulate the chains, iterations, and draws from the analyses and the `bayesplot` package to visualize the convergence of each parameter included in the bayes net model. I also love to use whatever table producing package I am interested at the time and create a function with html functionality. Specifically, I always include a feature to filter and highlight specific rows. This time I decided to use the `reactable` package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nbern_dist <- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny <- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |>\n  rowid_to_column() |>\n  rename(\n    studentid = rowid\n  )\n\nq_matrix <- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills <- 3\nskill_combo <- rep(list(0:1), skills)\nalpha <- expand.grid(skill_combo)\n\nalpha <- alpha |>\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |>\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n```\n:::\n\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_file <- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n```\n:::\n\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The **J**, **I**, **K**, and **C** list values are all important for looping through:\n\n- J = The number of rows of data; in this case there are 30 \"students\"\n\n- I = The number of columns in the dataset; which is 15 excluding the first column\n\n- K = The number of latent attributes/skills\n\n- C = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from **y** for the actual data and then **X** in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nmod <- cmdstan_model(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in readLines(stan_file): incomplete final line found on\n'C:/Users/Jonathan/Documents/GitHubRepos/log-of-jandp/posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan'\n```\n\n\n:::\n\n```{.r .cell-code}\nfit <- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 chains, at most 12 in parallel...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 37.1 seconds.\nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 40.9 seconds.\nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 41.0 seconds.\nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 41.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 40.1 seconds.\nTotal execution time: 41.4 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\n# fit$save_object(\"simple_bayes_net.RDS\")\n```\n:::\n\n\nSo this next part will be different depending on whether or not you are using `RStan` or like in this case `cmdstanR`. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For `cmdstanR`, you call on your Stan file. Below is the Stan code or if you'd like to see it side-by-side, the Stan file can be found [here](https://raw.githubusercontent.com/jpedroza1228/log-of-jandp/main/posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan). I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations. \n\n```{.stan include=\"simple_bayes_net.stan\"}\n\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit <- read_rds(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9748978 0.9855169 0.9468038 0.9679686\n```\n\n\n:::\n\n```{.r .cell-code}\nbn_converge <- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure <- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |> arrange(desc(rhat)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 4\n  variable      rhat ess_bulk ess_tail\n  <chr>        <dbl>    <dbl>    <dbl>\n1 lp__          1.00    3019.    4683.\n2 pie           1.00   11711.    6570.\n3 log_item[15]  1.00   11711.    6570.\n4 x_rep[1,1]    1.00   11711.    6570.\n5 x_rep[2,1]    1.00   11711.    6570.\n6 x_rep[3,1]    1.00   11711.    6570.\n```\n\n\n:::\n\n```{.r .cell-code}\nbn_measure |> mutate(across(-variable, ~round(.x, 3))) |> react_table()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-3683dbe9836c1f38d1f7\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-3683dbe9836c1f38d1f7\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"variable\":[\"lp__\",\"nu[1]\",\"nu[2]\",\"nu[3]\",\"nu[4]\",\"nu[5]\",\"nu[6]\",\"nu[7]\",\"nu[8]\",\"false_pos[1]\",\"false_pos[2]\",\"false_pos[3]\",\"false_pos[4]\",\"false_pos[5]\",\"false_pos[6]\",\"false_pos[7]\",\"false_pos[8]\",\"false_pos[9]\",\"false_pos[10]\",\"false_pos[11]\",\"false_pos[12]\",\"false_pos[13]\",\"false_pos[14]\",\"false_pos[15]\",\"true_pos[1]\",\"true_pos[2]\",\"true_pos[3]\",\"true_pos[4]\",\"true_pos[5]\",\"true_pos[6]\",\"true_pos[7]\",\"true_pos[8]\",\"true_pos[9]\",\"true_pos[10]\",\"true_pos[11]\",\"true_pos[12]\",\"true_pos[13]\",\"true_pos[14]\",\"true_pos[15]\",\"lambda1\",\"lambda20\",\"lambda21\",\"lambda30\",\"lambda31\",\"log_nu[1]\",\"log_nu[2]\",\"log_nu[3]\",\"log_nu[4]\",\"log_nu[5]\",\"log_nu[6]\",\"log_nu[7]\",\"log_nu[8]\",\"theta_log1[1]\",\"theta_log1[2]\",\"theta_log2[1]\",\"theta_log2[2]\",\"theta_log3[1]\",\"theta_log3[2]\",\"theta1[1]\",\"theta1[2]\",\"theta1[3]\",\"theta1[4]\",\"theta1[5]\",\"theta1[6]\",\"theta1[7]\",\"theta1[8]\",\"theta2[1]\",\"theta2[2]\",\"theta2[3]\",\"theta2[4]\",\"theta2[5]\",\"theta2[6]\",\"theta2[7]\",\"theta2[8]\",\"theta3[1]\",\"theta3[2]\",\"theta3[3]\",\"theta3[4]\",\"theta3[5]\",\"theta3[6]\",\"theta3[7]\",\"theta3[8]\",\"delta[1,1]\",\"delta[2,1]\",\"delta[3,1]\",\"delta[4,1]\",\"delta[5,1]\",\"delta[6,1]\",\"delta[7,1]\",\"delta[8,1]\",\"delta[9,1]\",\"delta[10,1]\",\"delta[11,1]\",\"delta[12,1]\",\"delta[13,1]\",\"delta[14,1]\",\"delta[15,1]\",\"delta[1,2]\",\"delta[2,2]\",\"delta[3,2]\",\"delta[4,2]\",\"delta[5,2]\",\"delta[6,2]\",\"delta[7,2]\",\"delta[8,2]\",\"delta[9,2]\",\"delta[10,2]\",\"delta[11,2]\",\"delta[12,2]\",\"delta[13,2]\",\"delta[14,2]\",\"delta[15,2]\",\"delta[1,3]\",\"delta[2,3]\",\"delta[3,3]\",\"delta[4,3]\",\"delta[5,3]\",\"delta[6,3]\",\"delta[7,3]\",\"delta[8,3]\",\"delta[9,3]\",\"delta[10,3]\",\"delta[11,3]\",\"delta[12,3]\",\"delta[13,3]\",\"delta[14,3]\",\"delta[15,3]\",\"delta[1,4]\",\"delta[2,4]\",\"delta[3,4]\",\"delta[4,4]\",\"delta[5,4]\",\"delta[6,4]\",\"delta[7,4]\",\"delta[8,4]\",\"delta[9,4]\",\"delta[10,4]\",\"delta[11,4]\",\"delta[12,4]\",\"delta[13,4]\",\"delta[14,4]\",\"delta[15,4]\",\"delta[1,5]\",\"delta[2,5]\",\"delta[3,5]\",\"delta[4,5]\",\"delta[5,5]\",\"delta[6,5]\",\"delta[7,5]\",\"delta[8,5]\",\"delta[9,5]\",\"delta[10,5]\",\"delta[11,5]\",\"delta[12,5]\",\"delta[13,5]\",\"delta[14,5]\",\"delta[15,5]\",\"delta[1,6]\",\"delta[2,6]\",\"delta[3,6]\",\"delta[4,6]\",\"delta[5,6]\",\"delta[6,6]\",\"delta[7,6]\",\"delta[8,6]\",\"delta[9,6]\",\"delta[10,6]\",\"delta[11,6]\",\"delta[12,6]\",\"delta[13,6]\",\"delta[14,6]\",\"delta[15,6]\",\"delta[1,7]\",\"delta[2,7]\",\"delta[3,7]\",\"delta[4,7]\",\"delta[5,7]\",\"delta[6,7]\",\"delta[7,7]\",\"delta[8,7]\",\"delta[9,7]\",\"delta[10,7]\",\"delta[11,7]\",\"delta[12,7]\",\"delta[13,7]\",\"delta[14,7]\",\"delta[15,7]\",\"delta[1,8]\",\"delta[2,8]\",\"delta[3,8]\",\"delta[4,8]\",\"delta[5,8]\",\"delta[6,8]\",\"delta[7,8]\",\"delta[8,8]\",\"delta[9,8]\",\"delta[10,8]\",\"delta[11,8]\",\"delta[12,8]\",\"delta[13,8]\",\"delta[14,8]\",\"delta[15,8]\",\"pie\",\"log_item[1]\",\"log_item[2]\",\"log_item[3]\",\"log_item[4]\",\"log_item[5]\",\"log_item[6]\",\"log_item[7]\",\"log_item[8]\",\"log_item[9]\",\"log_item[10]\",\"log_item[11]\",\"log_item[12]\",\"log_item[13]\",\"log_item[14]\",\"log_item[15]\",\"prob_resp_class[1,1]\",\"prob_resp_class[2,1]\",\"prob_resp_class[3,1]\",\"prob_resp_class[4,1]\",\"prob_resp_class[5,1]\",\"prob_resp_class[6,1]\",\"prob_resp_class[7,1]\",\"prob_resp_class[8,1]\",\"prob_resp_class[9,1]\",\"prob_resp_class[10,1]\",\"prob_resp_class[11,1]\",\"prob_resp_class[12,1]\",\"prob_resp_class[13,1]\",\"prob_resp_class[14,1]\",\"prob_resp_class[15,1]\",\"prob_resp_class[16,1]\",\"prob_resp_class[17,1]\",\"prob_resp_class[18,1]\",\"prob_resp_class[19,1]\",\"prob_resp_class[20,1]\",\"prob_resp_class[21,1]\",\"prob_resp_class[22,1]\",\"prob_resp_class[23,1]\",\"prob_resp_class[24,1]\",\"prob_resp_class[25,1]\",\"prob_resp_class[26,1]\",\"prob_resp_class[27,1]\",\"prob_resp_class[28,1]\",\"prob_resp_class[29,1]\",\"prob_resp_class[30,1]\",\"prob_resp_class[1,2]\",\"prob_resp_class[2,2]\",\"prob_resp_class[3,2]\",\"prob_resp_class[4,2]\",\"prob_resp_class[5,2]\",\"prob_resp_class[6,2]\",\"prob_resp_class[7,2]\",\"prob_resp_class[8,2]\",\"prob_resp_class[9,2]\",\"prob_resp_class[10,2]\",\"prob_resp_class[11,2]\",\"prob_resp_class[12,2]\",\"prob_resp_class[13,2]\",\"prob_resp_class[14,2]\",\"prob_resp_class[15,2]\",\"prob_resp_class[16,2]\",\"prob_resp_class[17,2]\",\"prob_resp_class[18,2]\",\"prob_resp_class[19,2]\",\"prob_resp_class[20,2]\",\"prob_resp_class[21,2]\",\"prob_resp_class[22,2]\",\"prob_resp_class[23,2]\",\"prob_resp_class[24,2]\",\"prob_resp_class[25,2]\",\"prob_resp_class[26,2]\",\"prob_resp_class[27,2]\",\"prob_resp_class[28,2]\",\"prob_resp_class[29,2]\",\"prob_resp_class[30,2]\",\"prob_resp_class[1,3]\",\"prob_resp_class[2,3]\",\"prob_resp_class[3,3]\",\"prob_resp_class[4,3]\",\"prob_resp_class[5,3]\",\"prob_resp_class[6,3]\",\"prob_resp_class[7,3]\",\"prob_resp_class[8,3]\",\"prob_resp_class[9,3]\",\"prob_resp_class[10,3]\",\"prob_resp_class[11,3]\",\"prob_resp_class[12,3]\",\"prob_resp_class[13,3]\",\"prob_resp_class[14,3]\",\"prob_resp_class[15,3]\",\"prob_resp_class[16,3]\",\"prob_resp_class[17,3]\",\"prob_resp_class[18,3]\",\"prob_resp_class[19,3]\",\"prob_resp_class[20,3]\",\"prob_resp_class[21,3]\",\"prob_resp_class[22,3]\",\"prob_resp_class[23,3]\",\"prob_resp_class[24,3]\",\"prob_resp_class[25,3]\",\"prob_resp_class[26,3]\",\"prob_resp_class[27,3]\",\"prob_resp_class[28,3]\",\"prob_resp_class[29,3]\",\"prob_resp_class[30,3]\",\"prob_resp_class[1,4]\",\"prob_resp_class[2,4]\",\"prob_resp_class[3,4]\",\"prob_resp_class[4,4]\",\"prob_resp_class[5,4]\",\"prob_resp_class[6,4]\",\"prob_resp_class[7,4]\",\"prob_resp_class[8,4]\",\"prob_resp_class[9,4]\",\"prob_resp_class[10,4]\",\"prob_resp_class[11,4]\",\"prob_resp_class[12,4]\",\"prob_resp_class[13,4]\",\"prob_resp_class[14,4]\",\"prob_resp_class[15,4]\",\"prob_resp_class[16,4]\",\"prob_resp_class[17,4]\",\"prob_resp_class[18,4]\",\"prob_resp_class[19,4]\",\"prob_resp_class[20,4]\",\"prob_resp_class[21,4]\",\"prob_resp_class[22,4]\",\"prob_resp_class[23,4]\",\"prob_resp_class[24,4]\",\"prob_resp_class[25,4]\",\"prob_resp_class[26,4]\",\"prob_resp_class[27,4]\",\"prob_resp_class[28,4]\",\"prob_resp_class[29,4]\",\"prob_resp_class[30,4]\",\"prob_resp_class[1,5]\",\"prob_resp_class[2,5]\",\"prob_resp_class[3,5]\",\"prob_resp_class[4,5]\",\"prob_resp_class[5,5]\",\"prob_resp_class[6,5]\",\"prob_resp_class[7,5]\",\"prob_resp_class[8,5]\",\"prob_resp_class[9,5]\",\"prob_resp_class[10,5]\",\"prob_resp_class[11,5]\",\"prob_resp_class[12,5]\",\"prob_resp_class[13,5]\",\"prob_resp_class[14,5]\",\"prob_resp_class[15,5]\",\"prob_resp_class[16,5]\",\"prob_resp_class[17,5]\",\"prob_resp_class[18,5]\",\"prob_resp_class[19,5]\",\"prob_resp_class[20,5]\",\"prob_resp_class[21,5]\",\"prob_resp_class[22,5]\",\"prob_resp_class[23,5]\",\"prob_resp_class[24,5]\",\"prob_resp_class[25,5]\",\"prob_resp_class[26,5]\",\"prob_resp_class[27,5]\",\"prob_resp_class[28,5]\",\"prob_resp_class[29,5]\",\"prob_resp_class[30,5]\",\"prob_resp_class[1,6]\",\"prob_resp_class[2,6]\",\"prob_resp_class[3,6]\",\"prob_resp_class[4,6]\",\"prob_resp_class[5,6]\",\"prob_resp_class[6,6]\",\"prob_resp_class[7,6]\",\"prob_resp_class[8,6]\",\"prob_resp_class[9,6]\",\"prob_resp_class[10,6]\",\"prob_resp_class[11,6]\",\"prob_resp_class[12,6]\",\"prob_resp_class[13,6]\",\"prob_resp_class[14,6]\",\"prob_resp_class[15,6]\",\"prob_resp_class[16,6]\",\"prob_resp_class[17,6]\",\"prob_resp_class[18,6]\",\"prob_resp_class[19,6]\",\"prob_resp_class[20,6]\",\"prob_resp_class[21,6]\",\"prob_resp_class[22,6]\",\"prob_resp_class[23,6]\",\"prob_resp_class[24,6]\",\"prob_resp_class[25,6]\",\"prob_resp_class[26,6]\",\"prob_resp_class[27,6]\",\"prob_resp_class[28,6]\",\"prob_resp_class[29,6]\",\"prob_resp_class[30,6]\",\"prob_resp_class[1,7]\",\"prob_resp_class[2,7]\",\"prob_resp_class[3,7]\",\"prob_resp_class[4,7]\",\"prob_resp_class[5,7]\",\"prob_resp_class[6,7]\",\"prob_resp_class[7,7]\",\"prob_resp_class[8,7]\",\"prob_resp_class[9,7]\",\"prob_resp_class[10,7]\",\"prob_resp_class[11,7]\",\"prob_resp_class[12,7]\",\"prob_resp_class[13,7]\",\"prob_resp_class[14,7]\",\"prob_resp_class[15,7]\",\"prob_resp_class[16,7]\",\"prob_resp_class[17,7]\",\"prob_resp_class[18,7]\",\"prob_resp_class[19,7]\",\"prob_resp_class[20,7]\",\"prob_resp_class[21,7]\",\"prob_resp_class[22,7]\",\"prob_resp_class[23,7]\",\"prob_resp_class[24,7]\",\"prob_resp_class[25,7]\",\"prob_resp_class[26,7]\",\"prob_resp_class[27,7]\",\"prob_resp_class[28,7]\",\"prob_resp_class[29,7]\",\"prob_resp_class[30,7]\",\"prob_resp_class[1,8]\",\"prob_resp_class[2,8]\",\"prob_resp_class[3,8]\",\"prob_resp_class[4,8]\",\"prob_resp_class[5,8]\",\"prob_resp_class[6,8]\",\"prob_resp_class[7,8]\",\"prob_resp_class[8,8]\",\"prob_resp_class[9,8]\",\"prob_resp_class[10,8]\",\"prob_resp_class[11,8]\",\"prob_resp_class[12,8]\",\"prob_resp_class[13,8]\",\"prob_resp_class[14,8]\",\"prob_resp_class[15,8]\",\"prob_resp_class[16,8]\",\"prob_resp_class[17,8]\",\"prob_resp_class[18,8]\",\"prob_resp_class[19,8]\",\"prob_resp_class[20,8]\",\"prob_resp_class[21,8]\",\"prob_resp_class[22,8]\",\"prob_resp_class[23,8]\",\"prob_resp_class[24,8]\",\"prob_resp_class[25,8]\",\"prob_resp_class[26,8]\",\"prob_resp_class[27,8]\",\"prob_resp_class[28,8]\",\"prob_resp_class[29,8]\",\"prob_resp_class[30,8]\",\"prob_resp_attr[1,1]\",\"prob_resp_attr[2,1]\",\"prob_resp_attr[3,1]\",\"prob_resp_attr[4,1]\",\"prob_resp_attr[5,1]\",\"prob_resp_attr[6,1]\",\"prob_resp_attr[7,1]\",\"prob_resp_attr[8,1]\",\"prob_resp_attr[9,1]\",\"prob_resp_attr[10,1]\",\"prob_resp_attr[11,1]\",\"prob_resp_attr[12,1]\",\"prob_resp_attr[13,1]\",\"prob_resp_attr[14,1]\",\"prob_resp_attr[15,1]\",\"prob_resp_attr[16,1]\",\"prob_resp_attr[17,1]\",\"prob_resp_attr[18,1]\",\"prob_resp_attr[19,1]\",\"prob_resp_attr[20,1]\",\"prob_resp_attr[21,1]\",\"prob_resp_attr[22,1]\",\"prob_resp_attr[23,1]\",\"prob_resp_attr[24,1]\",\"prob_resp_attr[25,1]\",\"prob_resp_attr[26,1]\",\"prob_resp_attr[27,1]\",\"prob_resp_attr[28,1]\",\"prob_resp_attr[29,1]\",\"prob_resp_attr[30,1]\",\"prob_resp_attr[1,2]\",\"prob_resp_attr[2,2]\",\"prob_resp_attr[3,2]\",\"prob_resp_attr[4,2]\",\"prob_resp_attr[5,2]\",\"prob_resp_attr[6,2]\",\"prob_resp_attr[7,2]\",\"prob_resp_attr[8,2]\",\"prob_resp_attr[9,2]\",\"prob_resp_attr[10,2]\",\"prob_resp_attr[11,2]\",\"prob_resp_attr[12,2]\",\"prob_resp_attr[13,2]\",\"prob_resp_attr[14,2]\",\"prob_resp_attr[15,2]\",\"prob_resp_attr[16,2]\",\"prob_resp_attr[17,2]\",\"prob_resp_attr[18,2]\",\"prob_resp_attr[19,2]\",\"prob_resp_attr[20,2]\",\"prob_resp_attr[21,2]\",\"prob_resp_attr[22,2]\",\"prob_resp_attr[23,2]\",\"prob_resp_attr[24,2]\",\"prob_resp_attr[25,2]\",\"prob_resp_attr[26,2]\",\"prob_resp_attr[27,2]\",\"prob_resp_attr[28,2]\",\"prob_resp_attr[29,2]\",\"prob_resp_attr[30,2]\",\"prob_resp_attr[1,3]\",\"prob_resp_attr[2,3]\",\"prob_resp_attr[3,3]\",\"prob_resp_attr[4,3]\",\"prob_resp_attr[5,3]\",\"prob_resp_attr[6,3]\",\"prob_resp_attr[7,3]\",\"prob_resp_attr[8,3]\",\"prob_resp_attr[9,3]\",\"prob_resp_attr[10,3]\",\"prob_resp_attr[11,3]\",\"prob_resp_attr[12,3]\",\"prob_resp_attr[13,3]\",\"prob_resp_attr[14,3]\",\"prob_resp_attr[15,3]\",\"prob_resp_attr[16,3]\",\"prob_resp_attr[17,3]\",\"prob_resp_attr[18,3]\",\"prob_resp_attr[19,3]\",\"prob_resp_attr[20,3]\",\"prob_resp_attr[21,3]\",\"prob_resp_attr[22,3]\",\"prob_resp_attr[23,3]\",\"prob_resp_attr[24,3]\",\"prob_resp_attr[25,3]\",\"prob_resp_attr[26,3]\",\"prob_resp_attr[27,3]\",\"prob_resp_attr[28,3]\",\"prob_resp_attr[29,3]\",\"prob_resp_attr[30,3]\",\"prob_joint[1]\",\"prob_joint[2]\",\"prob_joint[3]\",\"prob_joint[4]\",\"prob_joint[5]\",\"prob_joint[6]\",\"prob_joint[7]\",\"prob_joint[8]\",\"prob_attr_class[1]\",\"prob_attr_class[2]\",\"prob_attr_class[3]\",\"prob_attr_class[4]\",\"prob_attr_class[5]\",\"prob_attr_class[6]\",\"prob_attr_class[7]\",\"prob_attr_class[8]\",\"x_rep[1,1]\",\"x_rep[2,1]\",\"x_rep[3,1]\",\"x_rep[4,1]\",\"x_rep[5,1]\",\"x_rep[6,1]\",\"x_rep[7,1]\",\"x_rep[8,1]\",\"x_rep[9,1]\",\"x_rep[10,1]\",\"x_rep[11,1]\",\"x_rep[12,1]\",\"x_rep[13,1]\",\"x_rep[14,1]\",\"x_rep[15,1]\",\"x_rep[16,1]\",\"x_rep[17,1]\",\"x_rep[18,1]\",\"x_rep[19,1]\",\"x_rep[20,1]\",\"x_rep[21,1]\",\"x_rep[22,1]\",\"x_rep[23,1]\",\"x_rep[24,1]\",\"x_rep[25,1]\",\"x_rep[26,1]\",\"x_rep[27,1]\",\"x_rep[28,1]\",\"x_rep[29,1]\",\"x_rep[30,1]\",\"x_rep[1,2]\",\"x_rep[2,2]\",\"x_rep[3,2]\",\"x_rep[4,2]\",\"x_rep[5,2]\",\"x_rep[6,2]\",\"x_rep[7,2]\",\"x_rep[8,2]\",\"x_rep[9,2]\",\"x_rep[10,2]\",\"x_rep[11,2]\",\"x_rep[12,2]\",\"x_rep[13,2]\",\"x_rep[14,2]\",\"x_rep[15,2]\",\"x_rep[16,2]\",\"x_rep[17,2]\",\"x_rep[18,2]\",\"x_rep[19,2]\",\"x_rep[20,2]\",\"x_rep[21,2]\",\"x_rep[22,2]\",\"x_rep[23,2]\",\"x_rep[24,2]\",\"x_rep[25,2]\",\"x_rep[26,2]\",\"x_rep[27,2]\",\"x_rep[28,2]\",\"x_rep[29,2]\",\"x_rep[30,2]\",\"x_rep[1,3]\",\"x_rep[2,3]\",\"x_rep[3,3]\",\"x_rep[4,3]\",\"x_rep[5,3]\",\"x_rep[6,3]\",\"x_rep[7,3]\",\"x_rep[8,3]\",\"x_rep[9,3]\",\"x_rep[10,3]\",\"x_rep[11,3]\",\"x_rep[12,3]\",\"x_rep[13,3]\",\"x_rep[14,3]\",\"x_rep[15,3]\",\"x_rep[16,3]\",\"x_rep[17,3]\",\"x_rep[18,3]\",\"x_rep[19,3]\",\"x_rep[20,3]\",\"x_rep[21,3]\",\"x_rep[22,3]\",\"x_rep[23,3]\",\"x_rep[24,3]\",\"x_rep[25,3]\",\"x_rep[26,3]\",\"x_rep[27,3]\",\"x_rep[28,3]\",\"x_rep[29,3]\",\"x_rep[30,3]\",\"x_rep[1,4]\",\"x_rep[2,4]\",\"x_rep[3,4]\",\"x_rep[4,4]\",\"x_rep[5,4]\",\"x_rep[6,4]\",\"x_rep[7,4]\",\"x_rep[8,4]\",\"x_rep[9,4]\",\"x_rep[10,4]\",\"x_rep[11,4]\",\"x_rep[12,4]\",\"x_rep[13,4]\",\"x_rep[14,4]\",\"x_rep[15,4]\",\"x_rep[16,4]\",\"x_rep[17,4]\",\"x_rep[18,4]\",\"x_rep[19,4]\",\"x_rep[20,4]\",\"x_rep[21,4]\",\"x_rep[22,4]\",\"x_rep[23,4]\",\"x_rep[24,4]\",\"x_rep[25,4]\",\"x_rep[26,4]\",\"x_rep[27,4]\",\"x_rep[28,4]\",\"x_rep[29,4]\",\"x_rep[30,4]\",\"x_rep[1,5]\",\"x_rep[2,5]\",\"x_rep[3,5]\",\"x_rep[4,5]\",\"x_rep[5,5]\",\"x_rep[6,5]\",\"x_rep[7,5]\",\"x_rep[8,5]\",\"x_rep[9,5]\",\"x_rep[10,5]\",\"x_rep[11,5]\",\"x_rep[12,5]\",\"x_rep[13,5]\",\"x_rep[14,5]\",\"x_rep[15,5]\",\"x_rep[16,5]\",\"x_rep[17,5]\",\"x_rep[18,5]\",\"x_rep[19,5]\",\"x_rep[20,5]\",\"x_rep[21,5]\",\"x_rep[22,5]\",\"x_rep[23,5]\",\"x_rep[24,5]\",\"x_rep[25,5]\",\"x_rep[26,5]\",\"x_rep[27,5]\",\"x_rep[28,5]\",\"x_rep[29,5]\",\"x_rep[30,5]\",\"x_rep[1,6]\",\"x_rep[2,6]\",\"x_rep[3,6]\",\"x_rep[4,6]\",\"x_rep[5,6]\",\"x_rep[6,6]\",\"x_rep[7,6]\",\"x_rep[8,6]\",\"x_rep[9,6]\",\"x_rep[10,6]\",\"x_rep[11,6]\",\"x_rep[12,6]\",\"x_rep[13,6]\",\"x_rep[14,6]\",\"x_rep[15,6]\",\"x_rep[16,6]\",\"x_rep[17,6]\",\"x_rep[18,6]\",\"x_rep[19,6]\",\"x_rep[20,6]\",\"x_rep[21,6]\",\"x_rep[22,6]\",\"x_rep[23,6]\",\"x_rep[24,6]\",\"x_rep[25,6]\",\"x_rep[26,6]\",\"x_rep[27,6]\",\"x_rep[28,6]\",\"x_rep[29,6]\",\"x_rep[30,6]\",\"x_rep[1,7]\",\"x_rep[2,7]\",\"x_rep[3,7]\",\"x_rep[4,7]\",\"x_rep[5,7]\",\"x_rep[6,7]\",\"x_rep[7,7]\",\"x_rep[8,7]\",\"x_rep[9,7]\",\"x_rep[10,7]\",\"x_rep[11,7]\",\"x_rep[12,7]\",\"x_rep[13,7]\",\"x_rep[14,7]\",\"x_rep[15,7]\",\"x_rep[16,7]\",\"x_rep[17,7]\",\"x_rep[18,7]\",\"x_rep[19,7]\",\"x_rep[20,7]\",\"x_rep[21,7]\",\"x_rep[22,7]\",\"x_rep[23,7]\",\"x_rep[24,7]\",\"x_rep[25,7]\",\"x_rep[26,7]\",\"x_rep[27,7]\",\"x_rep[28,7]\",\"x_rep[29,7]\",\"x_rep[30,7]\",\"x_rep[1,8]\",\"x_rep[2,8]\",\"x_rep[3,8]\",\"x_rep[4,8]\",\"x_rep[5,8]\",\"x_rep[6,8]\",\"x_rep[7,8]\",\"x_rep[8,8]\",\"x_rep[9,8]\",\"x_rep[10,8]\",\"x_rep[11,8]\",\"x_rep[12,8]\",\"x_rep[13,8]\",\"x_rep[14,8]\",\"x_rep[15,8]\",\"x_rep[16,8]\",\"x_rep[17,8]\",\"x_rep[18,8]\",\"x_rep[19,8]\",\"x_rep[20,8]\",\"x_rep[21,8]\",\"x_rep[22,8]\",\"x_rep[23,8]\",\"x_rep[24,8]\",\"x_rep[25,8]\",\"x_rep[26,8]\",\"x_rep[27,8]\",\"x_rep[28,8]\",\"x_rep[29,8]\",\"x_rep[30,8]\",\"x_rep[1,9]\",\"x_rep[2,9]\",\"x_rep[3,9]\",\"x_rep[4,9]\",\"x_rep[5,9]\",\"x_rep[6,9]\",\"x_rep[7,9]\",\"x_rep[8,9]\",\"x_rep[9,9]\",\"x_rep[10,9]\",\"x_rep[11,9]\",\"x_rep[12,9]\",\"x_rep[13,9]\",\"x_rep[14,9]\",\"x_rep[15,9]\",\"x_rep[16,9]\",\"x_rep[17,9]\",\"x_rep[18,9]\",\"x_rep[19,9]\",\"x_rep[20,9]\",\"x_rep[21,9]\",\"x_rep[22,9]\",\"x_rep[23,9]\",\"x_rep[24,9]\",\"x_rep[25,9]\",\"x_rep[26,9]\",\"x_rep[27,9]\",\"x_rep[28,9]\",\"x_rep[29,9]\",\"x_rep[30,9]\",\"x_rep[1,10]\",\"x_rep[2,10]\",\"x_rep[3,10]\",\"x_rep[4,10]\",\"x_rep[5,10]\",\"x_rep[6,10]\",\"x_rep[7,10]\",\"x_rep[8,10]\",\"x_rep[9,10]\",\"x_rep[10,10]\",\"x_rep[11,10]\",\"x_rep[12,10]\",\"x_rep[13,10]\",\"x_rep[14,10]\",\"x_rep[15,10]\",\"x_rep[16,10]\",\"x_rep[17,10]\",\"x_rep[18,10]\",\"x_rep[19,10]\",\"x_rep[20,10]\",\"x_rep[21,10]\",\"x_rep[22,10]\",\"x_rep[23,10]\",\"x_rep[24,10]\",\"x_rep[25,10]\",\"x_rep[26,10]\",\"x_rep[27,10]\",\"x_rep[28,10]\",\"x_rep[29,10]\",\"x_rep[30,10]\",\"x_rep[1,11]\",\"x_rep[2,11]\",\"x_rep[3,11]\",\"x_rep[4,11]\",\"x_rep[5,11]\",\"x_rep[6,11]\",\"x_rep[7,11]\",\"x_rep[8,11]\",\"x_rep[9,11]\",\"x_rep[10,11]\",\"x_rep[11,11]\",\"x_rep[12,11]\",\"x_rep[13,11]\",\"x_rep[14,11]\",\"x_rep[15,11]\",\"x_rep[16,11]\",\"x_rep[17,11]\",\"x_rep[18,11]\",\"x_rep[19,11]\",\"x_rep[20,11]\",\"x_rep[21,11]\",\"x_rep[22,11]\",\"x_rep[23,11]\",\"x_rep[24,11]\",\"x_rep[25,11]\",\"x_rep[26,11]\",\"x_rep[27,11]\",\"x_rep[28,11]\",\"x_rep[29,11]\",\"x_rep[30,11]\",\"x_rep[1,12]\",\"x_rep[2,12]\",\"x_rep[3,12]\",\"x_rep[4,12]\",\"x_rep[5,12]\",\"x_rep[6,12]\",\"x_rep[7,12]\",\"x_rep[8,12]\",\"x_rep[9,12]\",\"x_rep[10,12]\",\"x_rep[11,12]\",\"x_rep[12,12]\",\"x_rep[13,12]\",\"x_rep[14,12]\",\"x_rep[15,12]\",\"x_rep[16,12]\",\"x_rep[17,12]\",\"x_rep[18,12]\",\"x_rep[19,12]\",\"x_rep[20,12]\",\"x_rep[21,12]\",\"x_rep[22,12]\",\"x_rep[23,12]\",\"x_rep[24,12]\",\"x_rep[25,12]\",\"x_rep[26,12]\",\"x_rep[27,12]\",\"x_rep[28,12]\",\"x_rep[29,12]\",\"x_rep[30,12]\",\"x_rep[1,13]\",\"x_rep[2,13]\",\"x_rep[3,13]\",\"x_rep[4,13]\",\"x_rep[5,13]\",\"x_rep[6,13]\",\"x_rep[7,13]\",\"x_rep[8,13]\",\"x_rep[9,13]\",\"x_rep[10,13]\",\"x_rep[11,13]\",\"x_rep[12,13]\",\"x_rep[13,13]\",\"x_rep[14,13]\",\"x_rep[15,13]\",\"x_rep[16,13]\",\"x_rep[17,13]\",\"x_rep[18,13]\",\"x_rep[19,13]\",\"x_rep[20,13]\",\"x_rep[21,13]\",\"x_rep[22,13]\",\"x_rep[23,13]\",\"x_rep[24,13]\",\"x_rep[25,13]\",\"x_rep[26,13]\",\"x_rep[27,13]\",\"x_rep[28,13]\",\"x_rep[29,13]\",\"x_rep[30,13]\",\"x_rep[1,14]\",\"x_rep[2,14]\",\"x_rep[3,14]\",\"x_rep[4,14]\",\"x_rep[5,14]\",\"x_rep[6,14]\",\"x_rep[7,14]\",\"x_rep[8,14]\",\"x_rep[9,14]\",\"x_rep[10,14]\",\"x_rep[11,14]\",\"x_rep[12,14]\",\"x_rep[13,14]\",\"x_rep[14,14]\",\"x_rep[15,14]\",\"x_rep[16,14]\",\"x_rep[17,14]\",\"x_rep[18,14]\",\"x_rep[19,14]\",\"x_rep[20,14]\",\"x_rep[21,14]\",\"x_rep[22,14]\",\"x_rep[23,14]\",\"x_rep[24,14]\",\"x_rep[25,14]\",\"x_rep[26,14]\",\"x_rep[27,14]\",\"x_rep[28,14]\",\"x_rep[29,14]\",\"x_rep[30,14]\",\"x_rep[1,15]\",\"x_rep[2,15]\",\"x_rep[3,15]\",\"x_rep[4,15]\",\"x_rep[5,15]\",\"x_rep[6,15]\",\"x_rep[7,15]\",\"x_rep[8,15]\",\"x_rep[9,15]\",\"x_rep[10,15]\",\"x_rep[11,15]\",\"x_rep[12,15]\",\"x_rep[13,15]\",\"x_rep[14,15]\",\"x_rep[15,15]\",\"x_rep[16,15]\",\"x_rep[17,15]\",\"x_rep[18,15]\",\"x_rep[19,15]\",\"x_rep[20,15]\",\"x_rep[21,15]\",\"x_rep[22,15]\",\"x_rep[23,15]\",\"x_rep[24,15]\",\"x_rep[25,15]\",\"x_rep[26,15]\",\"x_rep[27,15]\",\"x_rep[28,15]\",\"x_rep[29,15]\",\"x_rep[30,15]\"],\"mean\":[-767.481,0.027,0.044,0.032,0.071,0.033,0.083,0.033,0.675,0.14,0.203,0.205,0.188,0.113,0.09,0.173,0.243,0.126,0.134,0.19,0.176,0.09,0.067,0.131,0.87,0.891,0.9,0.902,0.859,0.818,0.888,0.904,0.86,0.861,0.887,0.892,0.813,0.827,0.869,0.859,0.367,0.772,0.177,0.869,-4.155,-3.648,-3.967,-3.109,-3.961,-3.069,-3.955,-0.411,-1.985,-0.153,-1.036,-0.261,-1.817,-0.141,-1.985,-0.153,-1.985,-0.153,-1.985,-0.153,-1.985,-0.153,-1.036,-1.036,-0.261,-0.261,-1.036,-1.036,-0.261,-0.261,-1.817,-1.817,-1.817,-1.817,-0.141,-0.141,-0.141,-0.141,0.141,0.367,0.177,0.141,0.367,0.177,0.141,0.367,0.177,0.141,0.367,0.177,0.141,0.367,0.177,0.859,0.367,0.177,0.859,0.367,0.177,0.859,0.367,0.177,0.859,0.367,0.177,0.859,0.367,0.177,0.141,0.772,0.177,0.141,0.772,0.177,0.141,0.772,0.177,0.141,0.772,0.177,0.141,0.772,0.177,0.859,0.772,0.177,0.859,0.772,0.177,0.859,0.772,0.177,0.859,0.772,0.177,0.859,0.772,0.177,0.141,0.367,0.869,0.141,0.367,0.869,0.141,0.367,0.869,0.141,0.367,0.869,0.141,0.367,0.869,0.859,0.367,0.869,0.859,0.367,0.869,0.859,0.367,0.869,0.859,0.367,0.869,0.859,0.367,0.869,0.141,0.772,0.869,0.141,0.772,0.869,0.141,0.772,0.869,0.141,0.772,0.869,0.141,0.772,0.869,0.859,0.772,0.869,0.859,0.772,0.869,0.859,0.772,0.869,0.859,0.772,0.869,0.859,0.772,0.869,0.672,-0.41,-0.463,-0.307,-0.332,-0.634,-0.938,-0.36,-0.409,-0.416,-0.426,-0.483,-0.336,-0.891,-0.618,-1.133,0.021,0,0.001,0,0.006,0.001,0.003,0,0,0,0,0,0.002,0,0,0,0,0,0,0,0,0.001,0,0,0,0,0.001,0,0.001,0,0.032,0,0.009,0,0.008,0.088,0.275,0,0.002,0,0.001,0.001,0.035,0.002,0.011,0.002,0.003,0,0.001,0,0.034,0.01,0.005,0.002,0,0.004,0.001,0.001,0.139,0.002,0.197,0,0.008,0,0.005,0.001,0.003,0,0,0.001,0,0,0.001,0,0,0,0,0,0,0,0.001,0.001,0.001,0,0,0,0.001,0,0.004,0,0.366,0.002,0.122,0,0.011,0.105,0.327,0,0.019,0.002,0.002,0.002,0.013,0.002,0.015,0.002,0.017,0.002,0.019,0,0.135,0.017,0.02,0.019,0,0.02,0.002,0.002,0.414,0.021,0.002,0,0.001,0.064,0.056,0.001,0,0.003,0,0.025,0,0.003,0.017,0.001,0.001,0.008,0,0.001,0,0.009,0,0.005,0.003,0,0,0,0.057,0,0,0,0.006,0.009,0.016,0.131,0.123,0.119,0.056,0.054,0.022,0.056,0.054,0.061,0.29,0.145,0.138,0.145,0.047,0.023,0.008,0.146,0.044,0.116,0.053,0.022,0.009,0.049,0.126,0.053,0.025,0.022,0.02,0.008,0.006,0.047,0.044,0.001,0,0.007,0.001,0.055,0.001,0.006,0.004,0.001,0.001,0.006,0.001,0.006,0.001,0.006,0.001,0.005,0.007,0.001,0.001,0.001,0.044,0.001,0,0.001,0.357,0.981,0.837,0.757,0.747,0.686,0.335,0.936,0.956,0.861,0.941,0.928,0.639,0.849,0.835,0.837,0.931,0.968,0.971,0.839,0.785,0.845,0.911,0.955,0.99,0.925,0.767,0.943,0.416,0.953,0.76,0.992,0.984,0.888,0.889,0.997,0.993,0.991,0.999,0.919,0.999,0.991,0.976,0.998,0.998,0.986,0.999,0.993,0.999,0.985,0.998,0.988,0.989,0.999,0.999,0.999,0.896,0.999,0.994,0.999,0.94,0.991,0.974,0.804,0.807,0.792,0.665,0.943,0.976,0.919,0.945,0.936,0.656,0.852,0.85,0.845,0.949,0.976,0.991,0.845,0.921,0.868,0.939,0.976,0.991,0.947,0.814,0.946,0.834,0.976,0.384,0.997,0.86,0.999,0.97,0.806,0.391,1,0.979,0.997,0.997,0.997,0.949,0.996,0.974,0.996,0.979,0.997,0.98,1,0.83,0.971,0.974,0.979,1,0.975,0.995,0.997,0.441,0.977,0,0,0,0,0,0,0,0,0,0,0,0,0,0.022,0.001,0.953,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-1.133,-1.133,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-1.133,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-1.133,-1.133,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-0.401,-0.401,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-1.133,-0.401,-1.133,-0.401,-1.133,-1.133,-0.401,-1.133,-1.133,-0.401,-0.401,-1.133,-1.133,-1.133,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-1.133,-0.401,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-0.401,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133,-0.401,-1.133,-0.401,-1.133,-0.401,-0.401,-1.133,-1.133],\"median\":[-767.173,0.02,0.032,0.024,0.056,0.023,0.058,0.024,0.687,0.133,0.198,0.199,0.182,0.107,0.082,0.166,0.24,0.118,0.127,0.185,0.17,0.082,0.061,0.124,0.876,0.898,0.905,0.908,0.867,0.824,0.894,0.91,0.866,0.867,0.893,0.899,0.82,0.836,0.876,0.861,0.363,0.775,0.17,0.871,-3.931,-3.429,-3.746,-2.887,-3.758,-2.851,-3.725,-0.375,-1.971,-0.15,-1.014,-0.255,-1.773,-0.138,-1.971,-0.15,-1.971,-0.15,-1.971,-0.15,-1.971,-0.15,-1.014,-1.014,-0.255,-0.255,-1.014,-1.014,-0.255,-0.255,-1.773,-1.773,-1.773,-1.773,-0.138,-0.138,-0.138,-0.138,0.139,0.363,0.17,0.139,0.363,0.17,0.139,0.363,0.17,0.139,0.363,0.17,0.139,0.363,0.17,0.861,0.363,0.17,0.861,0.363,0.17,0.861,0.363,0.17,0.861,0.363,0.17,0.861,0.363,0.17,0.139,0.775,0.17,0.139,0.775,0.17,0.139,0.775,0.17,0.139,0.775,0.17,0.139,0.775,0.17,0.861,0.775,0.17,0.861,0.775,0.17,0.861,0.775,0.17,0.861,0.775,0.17,0.861,0.775,0.17,0.139,0.363,0.871,0.139,0.363,0.871,0.139,0.363,0.871,0.139,0.363,0.871,0.139,0.363,0.871,0.861,0.363,0.871,0.861,0.363,0.871,0.861,0.363,0.871,0.861,0.363,0.871,0.861,0.363,0.871,0.139,0.775,0.871,0.139,0.775,0.871,0.139,0.775,0.871,0.139,0.775,0.871,0.139,0.775,0.871,0.861,0.775,0.871,0.861,0.775,0.871,0.861,0.775,0.871,0.861,0.775,0.871,0.861,0.775,0.871,0.674,-0.404,-0.456,-0.3,-0.326,-0.625,-0.928,-0.353,-0.403,-0.409,-0.42,-0.476,-0.33,-0.884,-0.608,-1.121,0.009,0,0,0,0.003,0,0.001,0,0,0,0,0,0.001,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0,0.014,0,0.004,0,0.004,0.057,0.23,0,0.001,0,0,0,0.019,0.001,0.006,0.001,0.001,0,0,0,0.019,0.005,0.002,0.001,0,0.002,0.001,0,0.091,0.001,0.149,0,0.004,0,0.002,0,0.001,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0,0,0,0,0,0.002,0,0.345,0.001,0.082,0,0.005,0.073,0.296,0,0.01,0.001,0.001,0.001,0.007,0.001,0.008,0.001,0.009,0.001,0.01,0,0.094,0.009,0.01,0.01,0,0.011,0.001,0.001,0.407,0.011,0.001,0,0,0.04,0.034,0,0,0.001,0,0.014,0,0.001,0.008,0,0,0.004,0,0,0,0.004,0,0.003,0.001,0,0,0,0.035,0,0,0,0.002,0.002,0.005,0.089,0.083,0.079,0.031,0.026,0.008,0.029,0.027,0.031,0.25,0.097,0.092,0.099,0.022,0.008,0.002,0.099,0.022,0.072,0.026,0.008,0.002,0.023,0.085,0.026,0.012,0.008,0.011,0.004,0.003,0.03,0.028,0,0,0.003,0,0.035,0,0.003,0.002,0,0,0.003,0,0.003,0,0.003,0,0.003,0.004,0,0,0,0.028,0,0,0,0.328,0.989,0.876,0.791,0.779,0.713,0.302,0.963,0.973,0.891,0.969,0.957,0.67,0.896,0.877,0.883,0.957,0.982,0.983,0.884,0.821,0.885,0.937,0.973,0.997,0.952,0.799,0.97,0.389,0.972,0.803,0.996,0.99,0.914,0.912,0.998,0.996,0.994,0.999,0.941,0.999,0.995,0.985,0.999,0.999,0.991,0.999,0.996,0.999,0.99,0.999,0.992,0.993,0.999,1,0.999,0.92,0.999,0.997,0.999,0.967,0.998,0.988,0.843,0.844,0.83,0.699,0.971,0.991,0.947,0.973,0.966,0.689,0.9,0.895,0.891,0.975,0.991,0.998,0.892,0.949,0.91,0.967,0.991,0.998,0.973,0.853,0.974,0.882,0.991,0.357,0.999,0.899,1,0.979,0.838,0.361,1,0.988,0.998,0.998,0.999,0.966,0.998,0.983,0.998,0.988,0.999,0.989,1,0.867,0.981,0.984,0.989,1,0.985,0.997,0.998,0.417,0.987,0,0,0,0,0,0,0,0,0,0,0,0,0,0.008,0,0.972,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-1.121,-1.121,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-1.121,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-1.121,-1.121,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-0.395,-0.395,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-1.121,-0.395,-1.121,-0.395,-1.121,-1.121,-0.395,-1.121,-1.121,-0.395,-0.395,-1.121,-1.121,-1.121,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-1.121,-0.395,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-0.395,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121,-0.395,-1.121,-0.395,-1.121,-0.395,-0.395,-1.121,-1.121],\"sd\":[4.826,0.026,0.041,0.03,0.059,0.032,0.083,0.032,0.113,0.058,0.068,0.069,0.067,0.048,0.045,0.066,0.071,0.054,0.057,0.065,0.065,0.046,0.035,0.056,0.055,0.049,0.045,0.045,0.062,0.071,0.049,0.045,0.058,0.058,0.051,0.049,0.074,0.076,0.056,0.033,0.091,0.045,0.071,0.034,1.272,1.246,1.224,1.161,1.259,1.304,1.28,0.231,0.24,0.039,0.263,0.059,0.436,0.039,0.24,0.039,0.24,0.039,0.24,0.039,0.24,0.039,0.263,0.263,0.059,0.059,0.263,0.263,0.059,0.059,0.436,0.436,0.436,0.436,0.039,0.039,0.039,0.039,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.091,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.045,0.071,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.091,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.033,0.045,0.034,0.06,0.086,0.098,0.075,0.074,0.123,0.164,0.079,0.087,0.091,0.088,0.1,0.08,0.148,0.128,0.188,0.034,0,0.002,0,0.01,0.002,0.006,0,0,0.001,0,0,0.004,0,0,0,0,0,0,0,0.001,0.001,0.001,0,0,0,0.002,0,0.003,0,0.051,0,0.018,0,0.012,0.093,0.204,0,0.004,0.001,0.001,0.001,0.047,0.003,0.016,0.003,0.006,0.001,0.002,0,0.046,0.016,0.008,0.004,0,0.008,0.003,0.001,0.141,0.004,0.171,0,0.012,0,0.008,0.001,0.005,0,0,0.002,0,0,0.001,0,0,0,0,0,0,0,0.002,0.001,0.002,0,0,0,0.002,0,0.007,0,0.229,0.004,0.121,0.001,0.016,0.104,0.221,0.001,0.026,0.003,0.004,0.004,0.019,0.004,0.021,0.003,0.024,0.004,0.028,0.001,0.127,0.023,0.027,0.028,0.001,0.029,0.003,0.004,0.241,0.03,0.005,0.001,0.001,0.07,0.062,0.001,0.001,0.004,0,0.033,0.001,0.004,0.024,0.002,0.002,0.012,0.001,0.002,0,0.013,0.001,0.008,0.004,0,0,0,0.065,0.001,0,0,0.015,0.035,0.037,0.132,0.126,0.122,0.07,0.08,0.05,0.078,0.081,0.086,0.211,0.145,0.14,0.145,0.073,0.052,0.034,0.145,0.065,0.126,0.078,0.05,0.038,0.075,0.129,0.08,0.039,0.05,0.025,0.011,0.009,0.052,0.049,0.001,0.001,0.009,0.001,0.059,0.002,0.008,0.005,0.001,0.001,0.008,0.002,0.009,0.002,0.009,0.001,0.007,0.01,0.001,0.001,0.001,0.049,0.002,0.001,0.002,0.194,0.04,0.132,0.153,0.148,0.171,0.189,0.082,0.06,0.109,0.082,0.088,0.206,0.145,0.143,0.146,0.08,0.055,0.047,0.146,0.15,0.131,0.088,0.061,0.038,0.084,0.15,0.08,0.216,0.062,0.178,0.011,0.018,0.09,0.085,0.003,0.01,0.011,0.002,0.071,0.002,0.01,0.028,0.003,0.002,0.016,0.002,0.009,0.002,0.017,0.003,0.013,0.012,0.002,0.002,0.002,0.085,0.002,0.009,0.002,0.076,0.036,0.047,0.148,0.142,0.15,0.208,0.081,0.052,0.09,0.081,0.087,0.208,0.145,0.141,0.146,0.075,0.053,0.035,0.146,0.085,0.129,0.081,0.052,0.038,0.077,0.144,0.08,0.15,0.052,0.202,0.005,0.125,0.001,0.031,0.14,0.206,0.001,0.027,0.005,0.005,0.004,0.055,0.006,0.029,0.005,0.026,0.005,0.028,0.001,0.137,0.032,0.031,0.029,0.001,0.031,0.007,0.005,0.222,0.031,0,0,0,0,0,0,0,0,0,0,0,0,0,0.05,0.002,0.062,0.188,0.188,0.188,0.188,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.188,0.091,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.188,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.188,0.188,0.188,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.091,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.188,0.091,0.188,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.188,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.091,0.188,0.091,0.188,0.091,0.091,0.188,0.188,0.188,0.188,0.188,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.188,0.188,0.188,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.188,0.091,0.091,0.091,0.091,0.188,0.188,0.188,0.188,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.091,0.188,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.091,0.188,0.091,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.188,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.188,0.188,0.188,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.188,0.188,0.091,0.188,0.188,0.091,0.091,0.188,0.091,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.091,0.091,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.188,0.091,0.188,0.091,0.188,0.188,0.091,0.188,0.188,0.091,0.091,0.188,0.188,0.188,0.188,0.188,0.091,0.091,0.091,0.091,0.188,0.188,0.091,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.091,0.091,0.188,0.091,0.091,0.188,0.188,0.091,0.188,0.091,0.188,0.091,0.091,0.188,0.188],\"mad\":[4.767,0.02,0.032,0.024,0.053,0.024,0.059,0.024,0.108,0.057,0.068,0.069,0.067,0.046,0.043,0.066,0.071,0.052,0.056,0.067,0.064,0.043,0.033,0.054,0.055,0.047,0.043,0.044,0.063,0.073,0.049,0.043,0.058,0.059,0.05,0.048,0.076,0.077,0.056,0.032,0.092,0.044,0.072,0.034,1.125,1.082,1.119,1.013,1.111,1.13,1.102,0.156,0.233,0.038,0.254,0.057,0.424,0.039,0.233,0.038,0.233,0.038,0.233,0.038,0.233,0.038,0.254,0.254,0.057,0.057,0.254,0.254,0.057,0.057,0.424,0.424,0.424,0.424,0.039,0.039,0.039,0.039,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.092,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.044,0.072,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.092,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.032,0.044,0.034,0.062,0.084,0.095,0.073,0.074,0.119,0.159,0.077,0.085,0.09,0.087,0.097,0.08,0.148,0.126,0.19,0.011,0,0,0,0.003,0,0.002,0,0,0,0,0,0.001,0,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0,0.001,0,0.017,0,0.004,0,0.005,0.06,0.217,0,0.001,0,0,0,0.021,0.001,0.007,0.001,0.002,0,0,0,0.021,0.006,0.003,0.001,0,0.002,0.001,0,0.101,0.001,0.153,0,0.005,0,0.003,0,0.001,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0,0,0,0,0,0.002,0,0.274,0.001,0.088,0,0.006,0.077,0.253,0,0.011,0.001,0.001,0.001,0.008,0.001,0.009,0.001,0.01,0.001,0.012,0,0.099,0.01,0.012,0.011,0,0.012,0.001,0.001,0.297,0.013,0.001,0,0,0.043,0.038,0,0,0.001,0,0.015,0,0.001,0.01,0.001,0,0.005,0,0,0,0.005,0,0.003,0.001,0,0,0,0.038,0,0,0,0.002,0.002,0.006,0.094,0.088,0.084,0.036,0.031,0.01,0.033,0.031,0.036,0.227,0.103,0.099,0.105,0.026,0.01,0.002,0.105,0.025,0.079,0.031,0.01,0.002,0.027,0.091,0.03,0.014,0.01,0.013,0.005,0.004,0.032,0.03,0,0,0.004,0,0.038,0,0.004,0.002,0,0,0.003,0,0.004,0,0.004,0,0.003,0.004,0,0,0,0.03,0,0,0,0.206,0.01,0.102,0.14,0.135,0.175,0.193,0.035,0.024,0.082,0.032,0.04,0.226,0.103,0.107,0.106,0.038,0.016,0.016,0.107,0.135,0.092,0.049,0.025,0.003,0.042,0.134,0.031,0.243,0.026,0.168,0.005,0.01,0.071,0.069,0.002,0.004,0.006,0.001,0.051,0.001,0.005,0.014,0.001,0.001,0.009,0.001,0.004,0.001,0.009,0.001,0.007,0.007,0.001,0,0.001,0.065,0.001,0.003,0.001,0.035,0.002,0.013,0.122,0.116,0.134,0.226,0.032,0.011,0.049,0.032,0.037,0.23,0.103,0.102,0.106,0.028,0.01,0.003,0.106,0.049,0.085,0.034,0.011,0.002,0.03,0.116,0.031,0.114,0.011,0.218,0.001,0.093,0,0.018,0.127,0.219,0,0.012,0.002,0.002,0.002,0.031,0.002,0.016,0.002,0.012,0.001,0.012,0,0.116,0.017,0.015,0.012,0,0.015,0.003,0.002,0.252,0.014,0,0,0,0,0,0,0,0,0,0,0,0,0,0.01,0,0.026,0.19,0.19,0.19,0.19,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.19,0.091,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.19,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.19,0.19,0.19,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.091,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.19,0.091,0.19,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.19,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.091,0.19,0.091,0.19,0.091,0.091,0.19,0.19,0.19,0.19,0.19,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.19,0.19,0.19,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.19,0.091,0.091,0.091,0.091,0.19,0.19,0.19,0.19,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.091,0.19,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.091,0.19,0.091,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.19,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.19,0.19,0.19,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.19,0.19,0.091,0.19,0.19,0.091,0.091,0.19,0.091,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.091,0.091,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.19,0.091,0.19,0.091,0.19,0.19,0.091,0.19,0.19,0.091,0.091,0.19,0.19,0.19,0.19,0.19,0.091,0.091,0.091,0.091,0.19,0.19,0.091,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.091,0.091,0.19,0.091,0.091,0.19,0.19,0.091,0.19,0.091,0.19,0.091,0.091,0.19,0.19],\"q5\":[-775.801,0.001,0.002,0.002,0.005,0.002,0.004,0.002,0.477,0.057,0.102,0.101,0.089,0.046,0.03,0.077,0.132,0.05,0.053,0.093,0.081,0.029,0.022,0.053,0.77,0.802,0.818,0.819,0.745,0.691,0.799,0.82,0.757,0.757,0.793,0.802,0.682,0.688,0.765,0.801,0.223,0.694,0.074,0.81,-6.512,-6.001,-6.295,-5.294,-6.274,-5.478,-6.391,-0.741,-2.402,-0.222,-1.501,-0.365,-2.606,-0.211,-2.402,-0.222,-2.402,-0.222,-2.402,-0.222,-2.402,-0.222,-1.501,-1.501,-0.365,-0.365,-1.501,-1.501,-0.365,-0.365,-2.606,-2.606,-2.606,-2.606,-0.211,-0.211,-0.211,-0.211,0.091,0.223,0.074,0.091,0.223,0.074,0.091,0.223,0.074,0.091,0.223,0.074,0.091,0.223,0.074,0.801,0.223,0.074,0.801,0.223,0.074,0.801,0.223,0.074,0.801,0.223,0.074,0.801,0.223,0.074,0.091,0.694,0.074,0.091,0.694,0.074,0.091,0.694,0.074,0.091,0.694,0.074,0.091,0.694,0.074,0.801,0.694,0.074,0.801,0.694,0.074,0.801,0.694,0.074,0.801,0.694,0.074,0.801,0.694,0.074,0.091,0.223,0.81,0.091,0.223,0.81,0.091,0.223,0.81,0.091,0.223,0.81,0.091,0.223,0.81,0.801,0.223,0.81,0.801,0.223,0.81,0.801,0.223,0.81,0.801,0.223,0.81,0.801,0.223,0.81,0.091,0.694,0.81,0.091,0.694,0.81,0.091,0.694,0.81,0.091,0.694,0.81,0.091,0.694,0.81,0.801,0.694,0.81,0.801,0.694,0.81,0.801,0.694,0.81,0.801,0.694,0.81,0.801,0.694,0.81,0.569,-0.562,-0.639,-0.443,-0.461,-0.85,-1.221,-0.496,-0.562,-0.578,-0.581,-0.656,-0.477,-1.143,-0.841,-1.465,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0,0,0,0,0.004,0.019,0,0,0,0,0,0.001,0,0,0,0,0,0,0,0.001,0,0,0,0,0,0,0,0.006,0,0.011,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.036,0,0.006,0,0,0.005,0.028,0,0.001,0,0,0,0,0,0,0,0.001,0,0.001,0,0.007,0.001,0.001,0.001,0,0.001,0,0,0.046,0.001,0,0,0,0.003,0.002,0,0,0,0,0.001,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.002,0,0,0,0,0,0,0.006,0.005,0.005,0.002,0.001,0,0.002,0.001,0.002,0.02,0.007,0.006,0.007,0.001,0,0,0.006,0.001,0.004,0.001,0,0,0.001,0.005,0.001,0.001,0,0.001,0,0,0.002,0.002,0,0,0,0,0.002,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.002,0,0,0,0.09,0.944,0.574,0.46,0.459,0.367,0.082,0.784,0.863,0.651,0.79,0.764,0.258,0.547,0.541,0.534,0.783,0.895,0.906,0.535,0.489,0.57,0.75,0.862,0.967,0.772,0.473,0.797,0.106,0.856,0.405,0.972,0.951,0.706,0.716,0.991,0.976,0.97,0.996,0.777,0.996,0.972,0.926,0.993,0.995,0.956,0.995,0.975,0.996,0.953,0.993,0.963,0.965,0.996,0.997,0.996,0.721,0.996,0.979,0.996,0.791,0.967,0.905,0.503,0.521,0.493,0.279,0.791,0.907,0.747,0.792,0.772,0.267,0.548,0.554,0.54,0.812,0.907,0.967,0.539,0.754,0.594,0.786,0.907,0.968,0.807,0.523,0.802,0.523,0.907,0.101,0.99,0.6,0.998,0.909,0.532,0.107,0.999,0.928,0.989,0.989,0.99,0.843,0.986,0.916,0.988,0.929,0.99,0.928,0.998,0.554,0.91,0.916,0.926,0.999,0.918,0.983,0.989,0.118,0.922,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.856,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-1.465,-1.465,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-1.465,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-1.465,-1.465,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-0.564,-0.564,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-1.465,-0.564,-1.465,-0.564,-1.465,-1.465,-0.564,-1.465,-1.465,-0.564,-0.564,-1.465,-1.465,-1.465,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-1.465,-0.564,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-0.564,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465,-0.564,-1.465,-0.564,-1.465,-0.564,-0.564,-1.465,-1.465],\"q95\":[-760.056,0.081,0.126,0.091,0.188,0.096,0.245,0.097,0.837,0.246,0.325,0.325,0.307,0.204,0.175,0.292,0.369,0.226,0.239,0.307,0.293,0.176,0.135,0.232,0.95,0.959,0.963,0.963,0.947,0.925,0.957,0.965,0.944,0.946,0.958,0.96,0.923,0.936,0.951,0.909,0.523,0.839,0.305,0.921,-2.512,-2.073,-2.394,-1.671,-2.347,-1.406,-2.332,-0.178,-1.614,-0.095,-0.647,-0.175,-1.186,-0.083,-1.614,-0.095,-1.614,-0.095,-1.614,-0.095,-1.614,-0.095,-0.647,-0.647,-0.175,-0.175,-0.647,-0.647,-0.175,-0.175,-1.186,-1.186,-1.186,-1.186,-0.083,-0.083,-0.083,-0.083,0.199,0.523,0.305,0.199,0.523,0.305,0.199,0.523,0.305,0.199,0.523,0.305,0.199,0.523,0.305,0.909,0.523,0.305,0.909,0.523,0.305,0.909,0.523,0.305,0.909,0.523,0.305,0.909,0.523,0.305,0.199,0.839,0.305,0.199,0.839,0.305,0.199,0.839,0.305,0.199,0.839,0.305,0.199,0.839,0.305,0.909,0.839,0.305,0.909,0.839,0.305,0.909,0.839,0.305,0.909,0.839,0.305,0.909,0.839,0.305,0.199,0.523,0.921,0.199,0.523,0.921,0.199,0.523,0.921,0.199,0.523,0.921,0.199,0.523,0.921,0.909,0.523,0.921,0.909,0.523,0.921,0.909,0.523,0.921,0.909,0.523,0.921,0.909,0.523,0.921,0.199,0.839,0.921,0.199,0.839,0.921,0.199,0.839,0.921,0.199,0.839,0.921,0.199,0.839,0.921,0.909,0.839,0.921,0.909,0.839,0.921,0.909,0.839,0.921,0.909,0.839,0.921,0.909,0.839,0.921,0.769,-0.281,-0.315,-0.193,-0.221,-0.448,-0.686,-0.241,-0.276,-0.28,-0.293,-0.332,-0.216,-0.657,-0.423,-0.841,0.081,0,0.003,0.001,0.022,0.003,0.013,0,0,0.001,0,0,0.009,0,0,0,0,0,0,0,0.001,0.003,0.002,0,0,0,0.004,0,0.006,0,0.123,0,0.036,0.001,0.031,0.281,0.668,0,0.006,0.002,0.002,0.002,0.123,0.007,0.041,0.006,0.014,0.001,0.002,0.001,0.123,0.037,0.017,0.006,0,0.017,0.006,0.002,0.43,0.007,0.551,0.001,0.03,0,0.02,0.003,0.011,0,0,0.003,0,0,0.002,0,0,0,0.001,0,0.001,0,0.004,0.003,0.005,0.001,0,0,0.003,0,0.015,0.001,0.762,0.009,0.374,0.001,0.04,0.321,0.729,0.001,0.068,0.006,0.009,0.009,0.046,0.008,0.053,0.007,0.063,0.009,0.07,0.001,0.396,0.059,0.073,0.071,0.001,0.072,0.007,0.009,0.808,0.074,0.008,0.001,0.003,0.209,0.184,0.003,0.001,0.01,0,0.087,0.001,0.01,0.06,0.004,0.003,0.029,0.001,0.003,0,0.031,0.001,0.02,0.01,0,0,0.001,0.188,0.001,0.001,0.001,0.024,0.032,0.062,0.408,0.387,0.377,0.197,0.205,0.088,0.204,0.206,0.224,0.692,0.45,0.429,0.449,0.179,0.09,0.031,0.449,0.163,0.382,0.199,0.089,0.032,0.184,0.394,0.197,0.093,0.088,0.067,0.027,0.023,0.152,0.142,0.002,0.001,0.024,0.003,0.174,0.003,0.021,0.014,0.003,0.002,0.02,0.003,0.022,0.004,0.022,0.002,0.019,0.025,0.003,0.003,0.003,0.144,0.003,0.001,0.003,0.713,0.998,0.975,0.944,0.933,0.918,0.695,0.994,0.995,0.975,0.996,0.993,0.919,0.989,0.979,0.981,0.992,0.997,0.997,0.983,0.96,0.975,0.985,0.995,1,0.991,0.947,0.996,0.8,0.995,0.969,1,0.998,0.984,0.981,1,1,0.999,1,0.99,1,0.999,0.998,1,1,0.999,1,1,1,0.999,1,0.999,0.999,1,1,1,0.985,1,1,1,0.996,1,0.999,0.969,0.966,0.967,0.947,0.997,0.999,0.992,0.998,0.997,0.936,0.991,0.987,0.987,0.998,0.999,1,0.988,0.993,0.987,0.995,0.999,1,0.997,0.972,0.998,0.985,0.999,0.754,1,0.985,1,0.996,0.971,0.778,1,0.999,1,1,1,0.995,1,0.998,1,0.999,1,0.999,1,0.98,0.997,0.998,0.999,1,0.998,1,1,0.835,0.999,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0.088,0.003,0.995,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.841,-0.841,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.841,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.841,-0.841,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.263,-0.263,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.841,-0.263,-0.841,-0.263,-0.841,-0.841,-0.263,-0.841,-0.841,-0.263,-0.263,-0.841,-0.841,-0.841,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.841,-0.263,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.263,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841,-0.263,-0.841,-0.263,-0.841,-0.263,-0.263,-0.841,-0.841]},\"columns\":[{\"id\":\"variable\",\"name\":\"variable\",\"type\":\"character\"},{\"id\":\"mean\",\"name\":\"mean\",\"type\":\"numeric\"},{\"id\":\"median\",\"name\":\"median\",\"type\":\"numeric\"},{\"id\":\"sd\",\"name\":\"sd\",\"type\":\"numeric\"},{\"id\":\"mad\",\"name\":\"mad\",\"type\":\"numeric\"},{\"id\":\"q5\",\"name\":\"q5\",\"type\":\"numeric\"},{\"id\":\"q95\",\"name\":\"q95\",\"type\":\"numeric\"}],\"filterable\":true,\"searchable\":true,\"highlight\":true,\"dataKey\":\"12011ac74608297eb618cd3c22784251\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_rep <- fit$draws(\"x_rep\") |> as_draws_matrix()\nstu_resp_attr <- fit$draws(\"prob_resp_attr\") |> as_draws_matrix()\n```\n:::\n\n\nI decided to extract the replicated values for the items and the probabilities oof each student's mastery of each of the three latent attributes.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\ny |> react_table()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-e9c0dbafc85536900211\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e9c0dbafc85536900211\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"studentid\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y1\":[0,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,1],\"y2\":[0,1,1,1,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1],\"y3\":[1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y4\":[0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1],\"y5\":[1,1,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,1,1,0,1,0,1,1,1,0,0,1,0,1],\"y6\":[0,1,0,1,1,0,0,1,0,1,0,1,0,0,1,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0],\"y7\":[1,1,1,1,0,0,1,0,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1],\"y8\":[1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,0,1,1],\"y9\":[0,0,0,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,1],\"y10\":[1,1,1,0,0,1,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,0,1,1,0,1,0,1,0,1],\"y11\":[1,1,0,0,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,0,0,1,1,0,1,1,1,1],\"y12\":[0,1,1,1,0,0,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1],\"y13\":[0,0,0,0,0,1,1,0,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,1,1,1,0,0,1,0],\"y14\":[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,0,0],\"y15\":[0,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0]},\"columns\":[{\"id\":\"studentid\",\"name\":\"studentid\",\"type\":\"numeric\"},{\"id\":\"y1\",\"name\":\"y1\",\"type\":\"numeric\"},{\"id\":\"y2\",\"name\":\"y2\",\"type\":\"numeric\"},{\"id\":\"y3\",\"name\":\"y3\",\"type\":\"numeric\"},{\"id\":\"y4\",\"name\":\"y4\",\"type\":\"numeric\"},{\"id\":\"y5\",\"name\":\"y5\",\"type\":\"numeric\"},{\"id\":\"y6\",\"name\":\"y6\",\"type\":\"numeric\"},{\"id\":\"y7\",\"name\":\"y7\",\"type\":\"numeric\"},{\"id\":\"y8\",\"name\":\"y8\",\"type\":\"numeric\"},{\"id\":\"y9\",\"name\":\"y9\",\"type\":\"numeric\"},{\"id\":\"y10\",\"name\":\"y10\",\"type\":\"numeric\"},{\"id\":\"y11\",\"name\":\"y11\",\"type\":\"numeric\"},{\"id\":\"y12\",\"name\":\"y12\",\"type\":\"numeric\"},{\"id\":\"y13\",\"name\":\"y13\",\"type\":\"numeric\"},{\"id\":\"y14\",\"name\":\"y14\",\"type\":\"numeric\"},{\"id\":\"y15\",\"name\":\"y15\",\"type\":\"numeric\"}],\"filterable\":true,\"searchable\":true,\"highlight\":true,\"dataKey\":\"e4070ee024f9a9b445f3493d12f7fe26\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the originaly data, the original responses and the probabilities with a probability threshold of 0.5 match one another.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nppc_intervals(\n  y = y |> pull(y1) |> as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(loo)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is loo version 2.8.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Online documentation and vignettes at mc-stan.org/loo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Windows 10 users: loo may be very slow if 'mc.cores' is set in your .Rprofile file (see https://github.com/stan-dev/loo/issues/94).\n```\n\n\n:::\n\n```{.r .cell-code}\nloo(y_rep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 8000 by 450 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -303.2  7.6\np_loo         8.1  0.3\nlooic       606.4 15.2\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n```\n\n\n:::\n\n```{.r .cell-code}\nwaic(y_rep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 8000 by 450 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -303.2  7.6\np_waic         8.1  0.3\nwaic         606.4 15.2\n```\n\n\n:::\n\n```{.r .cell-code}\nbn_resid <- y[,-1] - exp(y_rep)\n\nbn_resid^2 |> \n  as_tibble() |>\n  rowid_to_column() |>\n  ggplot(\n    aes(\n      rowid,\n      y2\n    )\n  ) +\n  geom_point(\n    alpha = .7\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nactual_stu_resp_attr <- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |>\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x > .5, 1, 0)\n    )\n  )\n```\n:::\n\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstu_resp_attr_mean <- stu_resp_attr |>\n  as_tibble() |>\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class <- stu_resp_attr_mean |>\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x > .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class <- stu_resp_attr_class |>\n  pivot_longer(\n    everything()\n  ) |>\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |>\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |>\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n```\n:::\n\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$att1\n   .y\n.x   0  1\n  1 10 20\n\n$att2\n   .y\n.x   0  1\n  1 11 19\n\n$att3\n   .y\n.x   0  1\n  0  1  2\n  1 15 12\n```\n\n\n:::\n\n```{.r .cell-code}\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$att1\n   .y\n.x          0         1\n  1 0.3333333 0.6666667\n\n$att2\n   .y\n.x          0         1\n  1 0.3666667 0.6333333\n\n$att3\n   .y\n.x           0          1\n  0 0.03333333 0.06666667\n  1 0.50000000 0.40000000\n```\n\n\n:::\n:::\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstu_resp_attr_long <- stu_resp_attr_class |>\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long <- actual_stu_resp_attr |>\n  pivot_longer(-studentid)\n\naccuracy_att <- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5777778\n```\n\n\n:::\n:::\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 0.5777778. This is a good starting point, but this may indicate that the model needs better definied priors and may require the edges between the attributes to show latent relationships. ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../site_libs/react-18.2.0/react.min.js\"></script>\n<script src=\"../../site_libs/react-18.2.0/react-dom.min.js\"></script>\n<script src=\"../../site_libs/reactwidget-2.0.0/react-tools.umd.cjs\"></script>\n<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/reactable-0.4.4/reactable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/reactable-binding-0.4.4/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}