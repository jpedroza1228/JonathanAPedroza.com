{
  "hash": "cf5177487104adc4efe1a8de469088bc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes Net Pt. 2\" \nsubtitle: |\n  Estimation of a Three Latent Attribute Model\nimage: whole_spider_web.jpg\ncategories: [Bayesian, Inference, Bayesian Network, bayes net, R, stan, cmdstanr, posterior, bayesplot, ggplot2]\ndate: 2024-11-15\n# citation:\n  # url: \nexecute:\n    message: false\n    warning: false\nparams:\n  slug: Bayes-Net-part-2\n  date: 2024-11-15\n---\n\n\n![Photo by [Nan Zhou](https://unsplash.com/@zzzzzzn?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash) on [Unsplash](https://unsplash.com/photos/a-spider-web-hanging-from-a-tree-in-a-forest-cpmZQRQdk9o?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)](whole_spider_web.jpg){fig-alt=\"An image of a spider web.\" fig-align=\"left\" width=\"6in\" height=\"6in\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table <- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n```\n:::\n\n\nAs mentioned in the [previous post](https://log-of-jandp.com/posts/2024-07-09-bayes-net-introduction/), the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the `cmdstanr` package to call on Stan for the Bayesian analyses, I am using the `posterior` package to manipulate the chains, iterations, and draws from the analyses and the `bayesplot` package to visualize the convergence of each parameter included in the bayes net model. I'm also using the `reactable` package to showcase the parameters for the model.\n\n# Data Creation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nbern_dist <- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny <- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |>\n  rowid_to_column() |>\n  rename(\n    studentid = rowid\n  )\n\nq_matrix <- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills <- 3\nskill_combo <- rep(list(0:1), skills)\nalpha <- expand.grid(skill_combo)\n\nalpha <- alpha |>\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |>\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n```\n:::\n\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_file <- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n```\n:::\n\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The **J**, **I**, **K**, and **C** list values are all important for looping through:\n\n-   J = The number of rows of data; in this case there are 30 \"students\"\n\n-   I = The number of columns in the dataset; which is 15 excluding the first column\n\n-   K = The number of latent attributes/skills\n\n-   C = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from **y** for the actual data and then **X** in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nmod <- cmdstan_model(here::here(\"posts/2024-11-15-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n\nfit <- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n\n# fit$save_object(\"simple_bayes_net.RDS\")\n```\n:::\n\n\nThis next part will be different depending on whether or not you are using `RStan` or like in this case `cmdstanR`. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For `cmdstanR`, you call on your Stan file. Below is the Stan code or if you'd like to see it side-by-side, the Stan file can be found [here](https://raw.githubusercontent.com/jpedroza1228/log-of-jandp/main/posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan). I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"\ndata {\n  int<lower=1> J; // number of examinees\n  int<lower=1> I; // number of items\n  int<lower=1> K; // number of latent variables\n  int<lower=1> C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\nparameters {\n  simplex[C] nu; // class probabilities\n  vector<lower=0, upper=1>[I] false_pos;\n  vector<lower=0, upper=1>[I] true_pos;\n  real<lower=0, upper=1> lambda1;\n  real<lower=0, upper=1> lambda20;\n  real<lower=0, upper=1> lambda21;\n  real<lower=0, upper=1> lambda30;\n  real<lower=0, upper=1> lambda31;\n}\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] > 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] > 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] > 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n```\n:::\n\n\nLooking over the Stan code, there is a lot here. I'll break down each section, but will not be spending an extensive amount of time for each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"\ndata {\n  int<lower=1> J; // number of examinees\n  int<lower=1> I; // number of items\n  int<lower=1> K; // number of latent variables\n  int<lower=1> C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\n\"\n```\n:::\n\n\nThe data section of stan code is including what you called the components of the `stan_file`list object. If you deviate from what you named the components in your list, then your model will show an error. While not entirely necessary, you may want to put constraints on these values. For instance, I know that I have more than 1 student, item, latent variable, and class, so I will put a constraint that the lowest possible value is 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"\nparameters {\n  simplex[C] nu; // class probabilities\n  vector<lower=0, upper=1>[I] false_pos;\n  vector<lower=0, upper=1>[I] true_pos;\n  real<lower=0, upper=1> lambda1;\n  real<lower=0, upper=1> lambda20;\n  real<lower=0, upper=1> lambda21;\n  real<lower=0, upper=1> lambda30;\n  real<lower=0, upper=1> lambda31;\n}\n\"\n```\n:::\n\n\nThe parameters section includes any parameters that are being included in your model. For instance, if creating a Bayesian linear regression, you would include the alpha and beta parameters in this section. For these models, I have the class probabilities for each latent class (to read more about the simplex function see [here](https://mc-stan.org/docs/reference-manual/transforms.html#simplex-transform.section)). Then I will have the probabilities of a student being either a true or false positive mastery case for the latent classes. These are vectors due to there being a true and false positive parameter for each item. The last parameters are the lambda parameters, which are the probabilities for mastery of the three latent attributes. These often require expert domain knowledge to specify informative priors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] > 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] > 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] > 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\n\"\n```\n:::\n\n\nWhile this section is optional, I like to include it because I use this section to do many of my calculations. For instance, in this section I like to use the prior lambda values to get the log probabilities of `theta_log` values, which are the log probabilities based on the level of mastery from the `lambda` values. I looped through the latent classes so when a latent class' value is 1, then it takes the greater log probability, and when the value is 0, then it takes the lower log probability. I also did my `delta` calculations in this section. The `delta` calculation takes `theta` values based on the latent classes values and it uses the Q-matrix for each item. Then by multiplying the `theta` values raised to the power of the Q-matrix gets the probability of mastery for each item within each latent class. This value indicates whether a given student will have mastery over all of the latent attributes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\n\"\n```\n:::\n\n\nFor the model section, which is necessary, I always start with declaring any new variables, followed by priors for my `lambda` values and the true and false positive probabilities for each item. Lastly, this section is always where you will do your calculations for each item and for each latent class. Finally, the target calculation at the end is for the target log density.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\"\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); \n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n```\n:::\n\n\nThe last section, the generated quantities, is \"generate additional quantities of interest from a fitted model without re-running the sampler\" ([Stan](https://mc-stan.org/docs/cmdstan-guide/generate_quantities_config.html)). For this series, I am using this section to calculate posterior probabilities, such as the probability of a student being in a specific latent class and the probability that students have mastered the attributes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- read_rds(here::here(\"posts/2024-11-15-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9376389 0.9111992 0.8754096 0.9844359\n```\n\n\n:::\n\n```{.r .cell-code}\nbn_converge <- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure <- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |> arrange(desc(rhat)) |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 4\n  variable               rhat ess_bulk ess_tail\n  <chr>                 <dbl>    <dbl>    <dbl>\n1 prob_resp_class[12,8]  1.00    3869.    4877.\n2 nu[5]                  1.00    7371.    5381.\n3 log_nu[5]              1.00    7371.    5381.\n4 prob_resp_class[5,8]   1.00    5686.    5902.\n5 prob_resp_class[18,8]  1.00    3686.    4569.\n6 prob_resp_class[27,5]  1.00    5974.    5744.\n```\n\n\n:::\n\n```{.r .cell-code}\nbn_measure |> mutate(across(-variable, ~round(.x, 3))) |> react_table()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-d169c7963e3dff87e286\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-d169c7963e3dff87e286\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"variable\":[\"lp__\",\"nu[1]\",\"nu[2]\",\"nu[3]\",\"nu[4]\",\"nu[5]\",\"nu[6]\",\"nu[7]\",\"nu[8]\",\"false_pos[1]\",\"false_pos[2]\",\"false_pos[3]\",\"false_pos[4]\",\"false_pos[5]\",\"false_pos[6]\",\"false_pos[7]\",\"false_pos[8]\",\"false_pos[9]\",\"false_pos[10]\",\"false_pos[11]\",\"false_pos[12]\",\"false_pos[13]\",\"false_pos[14]\",\"false_pos[15]\",\"true_pos[1]\",\"true_pos[2]\",\"true_pos[3]\",\"true_pos[4]\",\"true_pos[5]\",\"true_pos[6]\",\"true_pos[7]\",\"true_pos[8]\",\"true_pos[9]\",\"true_pos[10]\",\"true_pos[11]\",\"true_pos[12]\",\"true_pos[13]\",\"true_pos[14]\",\"true_pos[15]\",\"lambda1\",\"lambda20\",\"lambda21\",\"lambda30\",\"lambda31\",\"log_nu[1]\",\"log_nu[2]\",\"log_nu[3]\",\"log_nu[4]\",\"log_nu[5]\",\"log_nu[6]\",\"log_nu[7]\",\"log_nu[8]\",\"theta_log1[1]\",\"theta_log1[2]\",\"theta_log2[1]\",\"theta_log2[2]\",\"theta_log3[1]\",\"theta_log3[2]\",\"theta1[1]\",\"theta1[2]\",\"theta1[3]\",\"theta1[4]\",\"theta1[5]\",\"theta1[6]\",\"theta1[7]\",\"theta1[8]\",\"theta2[1]\",\"theta2[2]\",\"theta2[3]\",\"theta2[4]\",\"theta2[5]\",\"theta2[6]\",\"theta2[7]\",\"theta2[8]\",\"theta3[1]\",\"theta3[2]\",\"theta3[3]\",\"theta3[4]\",\"theta3[5]\",\"theta3[6]\",\"theta3[7]\",\"theta3[8]\",\"delta[1,1]\",\"delta[2,1]\",\"delta[3,1]\",\"delta[4,1]\",\"delta[5,1]\",\"delta[6,1]\",\"delta[7,1]\",\"delta[8,1]\",\"delta[9,1]\",\"delta[10,1]\",\"delta[11,1]\",\"delta[12,1]\",\"delta[13,1]\",\"delta[14,1]\",\"delta[15,1]\",\"delta[1,2]\",\"delta[2,2]\",\"delta[3,2]\",\"delta[4,2]\",\"delta[5,2]\",\"delta[6,2]\",\"delta[7,2]\",\"delta[8,2]\",\"delta[9,2]\",\"delta[10,2]\",\"delta[11,2]\",\"delta[12,2]\",\"delta[13,2]\",\"delta[14,2]\",\"delta[15,2]\",\"delta[1,3]\",\"delta[2,3]\",\"delta[3,3]\",\"delta[4,3]\",\"delta[5,3]\",\"delta[6,3]\",\"delta[7,3]\",\"delta[8,3]\",\"delta[9,3]\",\"delta[10,3]\",\"delta[11,3]\",\"delta[12,3]\",\"delta[13,3]\",\"delta[14,3]\",\"delta[15,3]\",\"delta[1,4]\",\"delta[2,4]\",\"delta[3,4]\",\"delta[4,4]\",\"delta[5,4]\",\"delta[6,4]\",\"delta[7,4]\",\"delta[8,4]\",\"delta[9,4]\",\"delta[10,4]\",\"delta[11,4]\",\"delta[12,4]\",\"delta[13,4]\",\"delta[14,4]\",\"delta[15,4]\",\"delta[1,5]\",\"delta[2,5]\",\"delta[3,5]\",\"delta[4,5]\",\"delta[5,5]\",\"delta[6,5]\",\"delta[7,5]\",\"delta[8,5]\",\"delta[9,5]\",\"delta[10,5]\",\"delta[11,5]\",\"delta[12,5]\",\"delta[13,5]\",\"delta[14,5]\",\"delta[15,5]\",\"delta[1,6]\",\"delta[2,6]\",\"delta[3,6]\",\"delta[4,6]\",\"delta[5,6]\",\"delta[6,6]\",\"delta[7,6]\",\"delta[8,6]\",\"delta[9,6]\",\"delta[10,6]\",\"delta[11,6]\",\"delta[12,6]\",\"delta[13,6]\",\"delta[14,6]\",\"delta[15,6]\",\"delta[1,7]\",\"delta[2,7]\",\"delta[3,7]\",\"delta[4,7]\",\"delta[5,7]\",\"delta[6,7]\",\"delta[7,7]\",\"delta[8,7]\",\"delta[9,7]\",\"delta[10,7]\",\"delta[11,7]\",\"delta[12,7]\",\"delta[13,7]\",\"delta[14,7]\",\"delta[15,7]\",\"delta[1,8]\",\"delta[2,8]\",\"delta[3,8]\",\"delta[4,8]\",\"delta[5,8]\",\"delta[6,8]\",\"delta[7,8]\",\"delta[8,8]\",\"delta[9,8]\",\"delta[10,8]\",\"delta[11,8]\",\"delta[12,8]\",\"delta[13,8]\",\"delta[14,8]\",\"delta[15,8]\",\"pie\",\"log_item[1]\",\"log_item[2]\",\"log_item[3]\",\"log_item[4]\",\"log_item[5]\",\"log_item[6]\",\"log_item[7]\",\"log_item[8]\",\"log_item[9]\",\"log_item[10]\",\"log_item[11]\",\"log_item[12]\",\"log_item[13]\",\"log_item[14]\",\"log_item[15]\",\"prob_resp_class[1,1]\",\"prob_resp_class[2,1]\",\"prob_resp_class[3,1]\",\"prob_resp_class[4,1]\",\"prob_resp_class[5,1]\",\"prob_resp_class[6,1]\",\"prob_resp_class[7,1]\",\"prob_resp_class[8,1]\",\"prob_resp_class[9,1]\",\"prob_resp_class[10,1]\",\"prob_resp_class[11,1]\",\"prob_resp_class[12,1]\",\"prob_resp_class[13,1]\",\"prob_resp_class[14,1]\",\"prob_resp_class[15,1]\",\"prob_resp_class[16,1]\",\"prob_resp_class[17,1]\",\"prob_resp_class[18,1]\",\"prob_resp_class[19,1]\",\"prob_resp_class[20,1]\",\"prob_resp_class[21,1]\",\"prob_resp_class[22,1]\",\"prob_resp_class[23,1]\",\"prob_resp_class[24,1]\",\"prob_resp_class[25,1]\",\"prob_resp_class[26,1]\",\"prob_resp_class[27,1]\",\"prob_resp_class[28,1]\",\"prob_resp_class[29,1]\",\"prob_resp_class[30,1]\",\"prob_resp_class[1,2]\",\"prob_resp_class[2,2]\",\"prob_resp_class[3,2]\",\"prob_resp_class[4,2]\",\"prob_resp_class[5,2]\",\"prob_resp_class[6,2]\",\"prob_resp_class[7,2]\",\"prob_resp_class[8,2]\",\"prob_resp_class[9,2]\",\"prob_resp_class[10,2]\",\"prob_resp_class[11,2]\",\"prob_resp_class[12,2]\",\"prob_resp_class[13,2]\",\"prob_resp_class[14,2]\",\"prob_resp_class[15,2]\",\"prob_resp_class[16,2]\",\"prob_resp_class[17,2]\",\"prob_resp_class[18,2]\",\"prob_resp_class[19,2]\",\"prob_resp_class[20,2]\",\"prob_resp_class[21,2]\",\"prob_resp_class[22,2]\",\"prob_resp_class[23,2]\",\"prob_resp_class[24,2]\",\"prob_resp_class[25,2]\",\"prob_resp_class[26,2]\",\"prob_resp_class[27,2]\",\"prob_resp_class[28,2]\",\"prob_resp_class[29,2]\",\"prob_resp_class[30,2]\",\"prob_resp_class[1,3]\",\"prob_resp_class[2,3]\",\"prob_resp_class[3,3]\",\"prob_resp_class[4,3]\",\"prob_resp_class[5,3]\",\"prob_resp_class[6,3]\",\"prob_resp_class[7,3]\",\"prob_resp_class[8,3]\",\"prob_resp_class[9,3]\",\"prob_resp_class[10,3]\",\"prob_resp_class[11,3]\",\"prob_resp_class[12,3]\",\"prob_resp_class[13,3]\",\"prob_resp_class[14,3]\",\"prob_resp_class[15,3]\",\"prob_resp_class[16,3]\",\"prob_resp_class[17,3]\",\"prob_resp_class[18,3]\",\"prob_resp_class[19,3]\",\"prob_resp_class[20,3]\",\"prob_resp_class[21,3]\",\"prob_resp_class[22,3]\",\"prob_resp_class[23,3]\",\"prob_resp_class[24,3]\",\"prob_resp_class[25,3]\",\"prob_resp_class[26,3]\",\"prob_resp_class[27,3]\",\"prob_resp_class[28,3]\",\"prob_resp_class[29,3]\",\"prob_resp_class[30,3]\",\"prob_resp_class[1,4]\",\"prob_resp_class[2,4]\",\"prob_resp_class[3,4]\",\"prob_resp_class[4,4]\",\"prob_resp_class[5,4]\",\"prob_resp_class[6,4]\",\"prob_resp_class[7,4]\",\"prob_resp_class[8,4]\",\"prob_resp_class[9,4]\",\"prob_resp_class[10,4]\",\"prob_resp_class[11,4]\",\"prob_resp_class[12,4]\",\"prob_resp_class[13,4]\",\"prob_resp_class[14,4]\",\"prob_resp_class[15,4]\",\"prob_resp_class[16,4]\",\"prob_resp_class[17,4]\",\"prob_resp_class[18,4]\",\"prob_resp_class[19,4]\",\"prob_resp_class[20,4]\",\"prob_resp_class[21,4]\",\"prob_resp_class[22,4]\",\"prob_resp_class[23,4]\",\"prob_resp_class[24,4]\",\"prob_resp_class[25,4]\",\"prob_resp_class[26,4]\",\"prob_resp_class[27,4]\",\"prob_resp_class[28,4]\",\"prob_resp_class[29,4]\",\"prob_resp_class[30,4]\",\"prob_resp_class[1,5]\",\"prob_resp_class[2,5]\",\"prob_resp_class[3,5]\",\"prob_resp_class[4,5]\",\"prob_resp_class[5,5]\",\"prob_resp_class[6,5]\",\"prob_resp_class[7,5]\",\"prob_resp_class[8,5]\",\"prob_resp_class[9,5]\",\"prob_resp_class[10,5]\",\"prob_resp_class[11,5]\",\"prob_resp_class[12,5]\",\"prob_resp_class[13,5]\",\"prob_resp_class[14,5]\",\"prob_resp_class[15,5]\",\"prob_resp_class[16,5]\",\"prob_resp_class[17,5]\",\"prob_resp_class[18,5]\",\"prob_resp_class[19,5]\",\"prob_resp_class[20,5]\",\"prob_resp_class[21,5]\",\"prob_resp_class[22,5]\",\"prob_resp_class[23,5]\",\"prob_resp_class[24,5]\",\"prob_resp_class[25,5]\",\"prob_resp_class[26,5]\",\"prob_resp_class[27,5]\",\"prob_resp_class[28,5]\",\"prob_resp_class[29,5]\",\"prob_resp_class[30,5]\",\"prob_resp_class[1,6]\",\"prob_resp_class[2,6]\",\"prob_resp_class[3,6]\",\"prob_resp_class[4,6]\",\"prob_resp_class[5,6]\",\"prob_resp_class[6,6]\",\"prob_resp_class[7,6]\",\"prob_resp_class[8,6]\",\"prob_resp_class[9,6]\",\"prob_resp_class[10,6]\",\"prob_resp_class[11,6]\",\"prob_resp_class[12,6]\",\"prob_resp_class[13,6]\",\"prob_resp_class[14,6]\",\"prob_resp_class[15,6]\",\"prob_resp_class[16,6]\",\"prob_resp_class[17,6]\",\"prob_resp_class[18,6]\",\"prob_resp_class[19,6]\",\"prob_resp_class[20,6]\",\"prob_resp_class[21,6]\",\"prob_resp_class[22,6]\",\"prob_resp_class[23,6]\",\"prob_resp_class[24,6]\",\"prob_resp_class[25,6]\",\"prob_resp_class[26,6]\",\"prob_resp_class[27,6]\",\"prob_resp_class[28,6]\",\"prob_resp_class[29,6]\",\"prob_resp_class[30,6]\",\"prob_resp_class[1,7]\",\"prob_resp_class[2,7]\",\"prob_resp_class[3,7]\",\"prob_resp_class[4,7]\",\"prob_resp_class[5,7]\",\"prob_resp_class[6,7]\",\"prob_resp_class[7,7]\",\"prob_resp_class[8,7]\",\"prob_resp_class[9,7]\",\"prob_resp_class[10,7]\",\"prob_resp_class[11,7]\",\"prob_resp_class[12,7]\",\"prob_resp_class[13,7]\",\"prob_resp_class[14,7]\",\"prob_resp_class[15,7]\",\"prob_resp_class[16,7]\",\"prob_resp_class[17,7]\",\"prob_resp_class[18,7]\",\"prob_resp_class[19,7]\",\"prob_resp_class[20,7]\",\"prob_resp_class[21,7]\",\"prob_resp_class[22,7]\",\"prob_resp_class[23,7]\",\"prob_resp_class[24,7]\",\"prob_resp_class[25,7]\",\"prob_resp_class[26,7]\",\"prob_resp_class[27,7]\",\"prob_resp_class[28,7]\",\"prob_resp_class[29,7]\",\"prob_resp_class[30,7]\",\"prob_resp_class[1,8]\",\"prob_resp_class[2,8]\",\"prob_resp_class[3,8]\",\"prob_resp_class[4,8]\",\"prob_resp_class[5,8]\",\"prob_resp_class[6,8]\",\"prob_resp_class[7,8]\",\"prob_resp_class[8,8]\",\"prob_resp_class[9,8]\",\"prob_resp_class[10,8]\",\"prob_resp_class[11,8]\",\"prob_resp_class[12,8]\",\"prob_resp_class[13,8]\",\"prob_resp_class[14,8]\",\"prob_resp_class[15,8]\",\"prob_resp_class[16,8]\",\"prob_resp_class[17,8]\",\"prob_resp_class[18,8]\",\"prob_resp_class[19,8]\",\"prob_resp_class[20,8]\",\"prob_resp_class[21,8]\",\"prob_resp_class[22,8]\",\"prob_resp_class[23,8]\",\"prob_resp_class[24,8]\",\"prob_resp_class[25,8]\",\"prob_resp_class[26,8]\",\"prob_resp_class[27,8]\",\"prob_resp_class[28,8]\",\"prob_resp_class[29,8]\",\"prob_resp_class[30,8]\",\"prob_resp_attr[1,1]\",\"prob_resp_attr[2,1]\",\"prob_resp_attr[3,1]\",\"prob_resp_attr[4,1]\",\"prob_resp_attr[5,1]\",\"prob_resp_attr[6,1]\",\"prob_resp_attr[7,1]\",\"prob_resp_attr[8,1]\",\"prob_resp_attr[9,1]\",\"prob_resp_attr[10,1]\",\"prob_resp_attr[11,1]\",\"prob_resp_attr[12,1]\",\"prob_resp_attr[13,1]\",\"prob_resp_attr[14,1]\",\"prob_resp_attr[15,1]\",\"prob_resp_attr[16,1]\",\"prob_resp_attr[17,1]\",\"prob_resp_attr[18,1]\",\"prob_resp_attr[19,1]\",\"prob_resp_attr[20,1]\",\"prob_resp_attr[21,1]\",\"prob_resp_attr[22,1]\",\"prob_resp_attr[23,1]\",\"prob_resp_attr[24,1]\",\"prob_resp_attr[25,1]\",\"prob_resp_attr[26,1]\",\"prob_resp_attr[27,1]\",\"prob_resp_attr[28,1]\",\"prob_resp_attr[29,1]\",\"prob_resp_attr[30,1]\",\"prob_resp_attr[1,2]\",\"prob_resp_attr[2,2]\",\"prob_resp_attr[3,2]\",\"prob_resp_attr[4,2]\",\"prob_resp_attr[5,2]\",\"prob_resp_attr[6,2]\",\"prob_resp_attr[7,2]\",\"prob_resp_attr[8,2]\",\"prob_resp_attr[9,2]\",\"prob_resp_attr[10,2]\",\"prob_resp_attr[11,2]\",\"prob_resp_attr[12,2]\",\"prob_resp_attr[13,2]\",\"prob_resp_attr[14,2]\",\"prob_resp_attr[15,2]\",\"prob_resp_attr[16,2]\",\"prob_resp_attr[17,2]\",\"prob_resp_attr[18,2]\",\"prob_resp_attr[19,2]\",\"prob_resp_attr[20,2]\",\"prob_resp_attr[21,2]\",\"prob_resp_attr[22,2]\",\"prob_resp_attr[23,2]\",\"prob_resp_attr[24,2]\",\"prob_resp_attr[25,2]\",\"prob_resp_attr[26,2]\",\"prob_resp_attr[27,2]\",\"prob_resp_attr[28,2]\",\"prob_resp_attr[29,2]\",\"prob_resp_attr[30,2]\",\"prob_resp_attr[1,3]\",\"prob_resp_attr[2,3]\",\"prob_resp_attr[3,3]\",\"prob_resp_attr[4,3]\",\"prob_resp_attr[5,3]\",\"prob_resp_attr[6,3]\",\"prob_resp_attr[7,3]\",\"prob_resp_attr[8,3]\",\"prob_resp_attr[9,3]\",\"prob_resp_attr[10,3]\",\"prob_resp_attr[11,3]\",\"prob_resp_attr[12,3]\",\"prob_resp_attr[13,3]\",\"prob_resp_attr[14,3]\",\"prob_resp_attr[15,3]\",\"prob_resp_attr[16,3]\",\"prob_resp_attr[17,3]\",\"prob_resp_attr[18,3]\",\"prob_resp_attr[19,3]\",\"prob_resp_attr[20,3]\",\"prob_resp_attr[21,3]\",\"prob_resp_attr[22,3]\",\"prob_resp_attr[23,3]\",\"prob_resp_attr[24,3]\",\"prob_resp_attr[25,3]\",\"prob_resp_attr[26,3]\",\"prob_resp_attr[27,3]\",\"prob_resp_attr[28,3]\",\"prob_resp_attr[29,3]\",\"prob_resp_attr[30,3]\",\"prob_joint[1]\",\"prob_joint[2]\",\"prob_joint[3]\",\"prob_joint[4]\",\"prob_joint[5]\",\"prob_joint[6]\",\"prob_joint[7]\",\"prob_joint[8]\",\"prob_attr_class[1]\",\"prob_attr_class[2]\",\"prob_attr_class[3]\",\"prob_attr_class[4]\",\"prob_attr_class[5]\",\"prob_attr_class[6]\",\"prob_attr_class[7]\",\"prob_attr_class[8]\",\"x_rep[1,1]\",\"x_rep[2,1]\",\"x_rep[3,1]\",\"x_rep[4,1]\",\"x_rep[5,1]\",\"x_rep[6,1]\",\"x_rep[7,1]\",\"x_rep[8,1]\",\"x_rep[9,1]\",\"x_rep[10,1]\",\"x_rep[11,1]\",\"x_rep[12,1]\",\"x_rep[13,1]\",\"x_rep[14,1]\",\"x_rep[15,1]\",\"x_rep[16,1]\",\"x_rep[17,1]\",\"x_rep[18,1]\",\"x_rep[19,1]\",\"x_rep[20,1]\",\"x_rep[21,1]\",\"x_rep[22,1]\",\"x_rep[23,1]\",\"x_rep[24,1]\",\"x_rep[25,1]\",\"x_rep[26,1]\",\"x_rep[27,1]\",\"x_rep[28,1]\",\"x_rep[29,1]\",\"x_rep[30,1]\",\"x_rep[1,2]\",\"x_rep[2,2]\",\"x_rep[3,2]\",\"x_rep[4,2]\",\"x_rep[5,2]\",\"x_rep[6,2]\",\"x_rep[7,2]\",\"x_rep[8,2]\",\"x_rep[9,2]\",\"x_rep[10,2]\",\"x_rep[11,2]\",\"x_rep[12,2]\",\"x_rep[13,2]\",\"x_rep[14,2]\",\"x_rep[15,2]\",\"x_rep[16,2]\",\"x_rep[17,2]\",\"x_rep[18,2]\",\"x_rep[19,2]\",\"x_rep[20,2]\",\"x_rep[21,2]\",\"x_rep[22,2]\",\"x_rep[23,2]\",\"x_rep[24,2]\",\"x_rep[25,2]\",\"x_rep[26,2]\",\"x_rep[27,2]\",\"x_rep[28,2]\",\"x_rep[29,2]\",\"x_rep[30,2]\",\"x_rep[1,3]\",\"x_rep[2,3]\",\"x_rep[3,3]\",\"x_rep[4,3]\",\"x_rep[5,3]\",\"x_rep[6,3]\",\"x_rep[7,3]\",\"x_rep[8,3]\",\"x_rep[9,3]\",\"x_rep[10,3]\",\"x_rep[11,3]\",\"x_rep[12,3]\",\"x_rep[13,3]\",\"x_rep[14,3]\",\"x_rep[15,3]\",\"x_rep[16,3]\",\"x_rep[17,3]\",\"x_rep[18,3]\",\"x_rep[19,3]\",\"x_rep[20,3]\",\"x_rep[21,3]\",\"x_rep[22,3]\",\"x_rep[23,3]\",\"x_rep[24,3]\",\"x_rep[25,3]\",\"x_rep[26,3]\",\"x_rep[27,3]\",\"x_rep[28,3]\",\"x_rep[29,3]\",\"x_rep[30,3]\",\"x_rep[1,4]\",\"x_rep[2,4]\",\"x_rep[3,4]\",\"x_rep[4,4]\",\"x_rep[5,4]\",\"x_rep[6,4]\",\"x_rep[7,4]\",\"x_rep[8,4]\",\"x_rep[9,4]\",\"x_rep[10,4]\",\"x_rep[11,4]\",\"x_rep[12,4]\",\"x_rep[13,4]\",\"x_rep[14,4]\",\"x_rep[15,4]\",\"x_rep[16,4]\",\"x_rep[17,4]\",\"x_rep[18,4]\",\"x_rep[19,4]\",\"x_rep[20,4]\",\"x_rep[21,4]\",\"x_rep[22,4]\",\"x_rep[23,4]\",\"x_rep[24,4]\",\"x_rep[25,4]\",\"x_rep[26,4]\",\"x_rep[27,4]\",\"x_rep[28,4]\",\"x_rep[29,4]\",\"x_rep[30,4]\",\"x_rep[1,5]\",\"x_rep[2,5]\",\"x_rep[3,5]\",\"x_rep[4,5]\",\"x_rep[5,5]\",\"x_rep[6,5]\",\"x_rep[7,5]\",\"x_rep[8,5]\",\"x_rep[9,5]\",\"x_rep[10,5]\",\"x_rep[11,5]\",\"x_rep[12,5]\",\"x_rep[13,5]\",\"x_rep[14,5]\",\"x_rep[15,5]\",\"x_rep[16,5]\",\"x_rep[17,5]\",\"x_rep[18,5]\",\"x_rep[19,5]\",\"x_rep[20,5]\",\"x_rep[21,5]\",\"x_rep[22,5]\",\"x_rep[23,5]\",\"x_rep[24,5]\",\"x_rep[25,5]\",\"x_rep[26,5]\",\"x_rep[27,5]\",\"x_rep[28,5]\",\"x_rep[29,5]\",\"x_rep[30,5]\",\"x_rep[1,6]\",\"x_rep[2,6]\",\"x_rep[3,6]\",\"x_rep[4,6]\",\"x_rep[5,6]\",\"x_rep[6,6]\",\"x_rep[7,6]\",\"x_rep[8,6]\",\"x_rep[9,6]\",\"x_rep[10,6]\",\"x_rep[11,6]\",\"x_rep[12,6]\",\"x_rep[13,6]\",\"x_rep[14,6]\",\"x_rep[15,6]\",\"x_rep[16,6]\",\"x_rep[17,6]\",\"x_rep[18,6]\",\"x_rep[19,6]\",\"x_rep[20,6]\",\"x_rep[21,6]\",\"x_rep[22,6]\",\"x_rep[23,6]\",\"x_rep[24,6]\",\"x_rep[25,6]\",\"x_rep[26,6]\",\"x_rep[27,6]\",\"x_rep[28,6]\",\"x_rep[29,6]\",\"x_rep[30,6]\",\"x_rep[1,7]\",\"x_rep[2,7]\",\"x_rep[3,7]\",\"x_rep[4,7]\",\"x_rep[5,7]\",\"x_rep[6,7]\",\"x_rep[7,7]\",\"x_rep[8,7]\",\"x_rep[9,7]\",\"x_rep[10,7]\",\"x_rep[11,7]\",\"x_rep[12,7]\",\"x_rep[13,7]\",\"x_rep[14,7]\",\"x_rep[15,7]\",\"x_rep[16,7]\",\"x_rep[17,7]\",\"x_rep[18,7]\",\"x_rep[19,7]\",\"x_rep[20,7]\",\"x_rep[21,7]\",\"x_rep[22,7]\",\"x_rep[23,7]\",\"x_rep[24,7]\",\"x_rep[25,7]\",\"x_rep[26,7]\",\"x_rep[27,7]\",\"x_rep[28,7]\",\"x_rep[29,7]\",\"x_rep[30,7]\",\"x_rep[1,8]\",\"x_rep[2,8]\",\"x_rep[3,8]\",\"x_rep[4,8]\",\"x_rep[5,8]\",\"x_rep[6,8]\",\"x_rep[7,8]\",\"x_rep[8,8]\",\"x_rep[9,8]\",\"x_rep[10,8]\",\"x_rep[11,8]\",\"x_rep[12,8]\",\"x_rep[13,8]\",\"x_rep[14,8]\",\"x_rep[15,8]\",\"x_rep[16,8]\",\"x_rep[17,8]\",\"x_rep[18,8]\",\"x_rep[19,8]\",\"x_rep[20,8]\",\"x_rep[21,8]\",\"x_rep[22,8]\",\"x_rep[23,8]\",\"x_rep[24,8]\",\"x_rep[25,8]\",\"x_rep[26,8]\",\"x_rep[27,8]\",\"x_rep[28,8]\",\"x_rep[29,8]\",\"x_rep[30,8]\",\"x_rep[1,9]\",\"x_rep[2,9]\",\"x_rep[3,9]\",\"x_rep[4,9]\",\"x_rep[5,9]\",\"x_rep[6,9]\",\"x_rep[7,9]\",\"x_rep[8,9]\",\"x_rep[9,9]\",\"x_rep[10,9]\",\"x_rep[11,9]\",\"x_rep[12,9]\",\"x_rep[13,9]\",\"x_rep[14,9]\",\"x_rep[15,9]\",\"x_rep[16,9]\",\"x_rep[17,9]\",\"x_rep[18,9]\",\"x_rep[19,9]\",\"x_rep[20,9]\",\"x_rep[21,9]\",\"x_rep[22,9]\",\"x_rep[23,9]\",\"x_rep[24,9]\",\"x_rep[25,9]\",\"x_rep[26,9]\",\"x_rep[27,9]\",\"x_rep[28,9]\",\"x_rep[29,9]\",\"x_rep[30,9]\",\"x_rep[1,10]\",\"x_rep[2,10]\",\"x_rep[3,10]\",\"x_rep[4,10]\",\"x_rep[5,10]\",\"x_rep[6,10]\",\"x_rep[7,10]\",\"x_rep[8,10]\",\"x_rep[9,10]\",\"x_rep[10,10]\",\"x_rep[11,10]\",\"x_rep[12,10]\",\"x_rep[13,10]\",\"x_rep[14,10]\",\"x_rep[15,10]\",\"x_rep[16,10]\",\"x_rep[17,10]\",\"x_rep[18,10]\",\"x_rep[19,10]\",\"x_rep[20,10]\",\"x_rep[21,10]\",\"x_rep[22,10]\",\"x_rep[23,10]\",\"x_rep[24,10]\",\"x_rep[25,10]\",\"x_rep[26,10]\",\"x_rep[27,10]\",\"x_rep[28,10]\",\"x_rep[29,10]\",\"x_rep[30,10]\",\"x_rep[1,11]\",\"x_rep[2,11]\",\"x_rep[3,11]\",\"x_rep[4,11]\",\"x_rep[5,11]\",\"x_rep[6,11]\",\"x_rep[7,11]\",\"x_rep[8,11]\",\"x_rep[9,11]\",\"x_rep[10,11]\",\"x_rep[11,11]\",\"x_rep[12,11]\",\"x_rep[13,11]\",\"x_rep[14,11]\",\"x_rep[15,11]\",\"x_rep[16,11]\",\"x_rep[17,11]\",\"x_rep[18,11]\",\"x_rep[19,11]\",\"x_rep[20,11]\",\"x_rep[21,11]\",\"x_rep[22,11]\",\"x_rep[23,11]\",\"x_rep[24,11]\",\"x_rep[25,11]\",\"x_rep[26,11]\",\"x_rep[27,11]\",\"x_rep[28,11]\",\"x_rep[29,11]\",\"x_rep[30,11]\",\"x_rep[1,12]\",\"x_rep[2,12]\",\"x_rep[3,12]\",\"x_rep[4,12]\",\"x_rep[5,12]\",\"x_rep[6,12]\",\"x_rep[7,12]\",\"x_rep[8,12]\",\"x_rep[9,12]\",\"x_rep[10,12]\",\"x_rep[11,12]\",\"x_rep[12,12]\",\"x_rep[13,12]\",\"x_rep[14,12]\",\"x_rep[15,12]\",\"x_rep[16,12]\",\"x_rep[17,12]\",\"x_rep[18,12]\",\"x_rep[19,12]\",\"x_rep[20,12]\",\"x_rep[21,12]\",\"x_rep[22,12]\",\"x_rep[23,12]\",\"x_rep[24,12]\",\"x_rep[25,12]\",\"x_rep[26,12]\",\"x_rep[27,12]\",\"x_rep[28,12]\",\"x_rep[29,12]\",\"x_rep[30,12]\",\"x_rep[1,13]\",\"x_rep[2,13]\",\"x_rep[3,13]\",\"x_rep[4,13]\",\"x_rep[5,13]\",\"x_rep[6,13]\",\"x_rep[7,13]\",\"x_rep[8,13]\",\"x_rep[9,13]\",\"x_rep[10,13]\",\"x_rep[11,13]\",\"x_rep[12,13]\",\"x_rep[13,13]\",\"x_rep[14,13]\",\"x_rep[15,13]\",\"x_rep[16,13]\",\"x_rep[17,13]\",\"x_rep[18,13]\",\"x_rep[19,13]\",\"x_rep[20,13]\",\"x_rep[21,13]\",\"x_rep[22,13]\",\"x_rep[23,13]\",\"x_rep[24,13]\",\"x_rep[25,13]\",\"x_rep[26,13]\",\"x_rep[27,13]\",\"x_rep[28,13]\",\"x_rep[29,13]\",\"x_rep[30,13]\",\"x_rep[1,14]\",\"x_rep[2,14]\",\"x_rep[3,14]\",\"x_rep[4,14]\",\"x_rep[5,14]\",\"x_rep[6,14]\",\"x_rep[7,14]\",\"x_rep[8,14]\",\"x_rep[9,14]\",\"x_rep[10,14]\",\"x_rep[11,14]\",\"x_rep[12,14]\",\"x_rep[13,14]\",\"x_rep[14,14]\",\"x_rep[15,14]\",\"x_rep[16,14]\",\"x_rep[17,14]\",\"x_rep[18,14]\",\"x_rep[19,14]\",\"x_rep[20,14]\",\"x_rep[21,14]\",\"x_rep[22,14]\",\"x_rep[23,14]\",\"x_rep[24,14]\",\"x_rep[25,14]\",\"x_rep[26,14]\",\"x_rep[27,14]\",\"x_rep[28,14]\",\"x_rep[29,14]\",\"x_rep[30,14]\",\"x_rep[1,15]\",\"x_rep[2,15]\",\"x_rep[3,15]\",\"x_rep[4,15]\",\"x_rep[5,15]\",\"x_rep[6,15]\",\"x_rep[7,15]\",\"x_rep[8,15]\",\"x_rep[9,15]\",\"x_rep[10,15]\",\"x_rep[11,15]\",\"x_rep[12,15]\",\"x_rep[13,15]\",\"x_rep[14,15]\",\"x_rep[15,15]\",\"x_rep[16,15]\",\"x_rep[17,15]\",\"x_rep[18,15]\",\"x_rep[19,15]\",\"x_rep[20,15]\",\"x_rep[21,15]\",\"x_rep[22,15]\",\"x_rep[23,15]\",\"x_rep[24,15]\",\"x_rep[25,15]\",\"x_rep[26,15]\",\"x_rep[27,15]\",\"x_rep[28,15]\",\"x_rep[29,15]\",\"x_rep[30,15]\"],\"mean\":[-381.679,0.101,0.116,0.11,0.125,0.13,0.134,0.136,0.148,0.54,0.56,0.612,0.701,0.277,0.268,0.66,0.641,0.374,0.533,0.501,0.512,0.344,0.198,0.374,0.757,0.788,0.861,0.85,0.69,0.619,0.767,0.863,0.735,0.684,0.791,0.842,0.616,0.506,0.777,0.717,0.433,0.752,0.474,0.806,-2.858,-2.672,-2.733,-2.602,-2.54,-2.528,-2.505,-2.412,-1.857,-0.434,-1.144,-0.336,-1.016,-0.25,-1.857,-0.434,-1.857,-0.434,-1.857,-0.434,-1.857,-0.434,-1.144,-1.144,-0.336,-0.336,-1.144,-1.144,-0.336,-0.336,-1.016,-1.016,-1.016,-1.016,-0.25,-0.25,-0.25,-0.25,0.283,0.433,0.474,0.283,0.433,0.474,0.283,0.433,0.474,0.283,0.433,0.474,0.283,0.433,0.474,0.717,0.433,0.474,0.717,0.433,0.474,0.717,0.433,0.474,0.717,0.433,0.474,0.717,0.433,0.474,0.283,0.752,0.474,0.283,0.752,0.474,0.283,0.752,0.474,0.283,0.752,0.474,0.283,0.752,0.474,0.717,0.752,0.474,0.717,0.752,0.474,0.717,0.752,0.474,0.717,0.752,0.474,0.717,0.752,0.474,0.283,0.433,0.806,0.283,0.433,0.806,0.283,0.433,0.806,0.283,0.433,0.806,0.283,0.433,0.806,0.717,0.433,0.806,0.717,0.433,0.806,0.717,0.433,0.806,0.717,0.433,0.806,0.717,0.433,0.806,0.283,0.752,0.806,0.283,0.752,0.806,0.283,0.752,0.806,0.283,0.752,0.806,0.283,0.752,0.806,0.717,0.752,0.806,0.717,0.752,0.806,0.717,0.752,0.806,0.717,0.752,0.806,0.717,0.752,0.806,0.672,-0.42,-0.338,-0.218,-0.233,-0.651,-0.726,-0.358,-0.218,-0.474,-0.538,-0.358,-0.274,-0.754,-0.449,-1.188,0.128,0.053,0.096,0.089,0.125,0.14,0.152,0.057,0.074,0.091,0.068,0.082,0.146,0.088,0.108,0.1,0.092,0.064,0.062,0.082,0.105,0.117,0.098,0.075,0.037,0.1,0.11,0.071,0.141,0.077,0.103,0.052,0.094,0.073,0.133,0.213,0.226,0.067,0.097,0.076,0.084,0.112,0.149,0.108,0.167,0.099,0.113,0.089,0.077,0.078,0.159,0.158,0.096,0.099,0.056,0.135,0.095,0.088,0.21,0.096,0.239,0.128,0.173,0.064,0.09,0.096,0.117,0.075,0.126,0.091,0.091,0.077,0.088,0.071,0.076,0.075,0.101,0.1,0.144,0.061,0.133,0.093,0.126,0.131,0.081,0.107,0.081,0.088,0.134,0.133,0.196,0.121,0.164,0.054,0.092,0.144,0.17,0.087,0.161,0.078,0.114,0.101,0.089,0.09,0.116,0.073,0.124,0.133,0.173,0.059,0.195,0.122,0.121,0.163,0.12,0.137,0.07,0.109,0.195,0.161,0.066,0.108,0.094,0.239,0.168,0.1,0.08,0.154,0.097,0.192,0.134,0.144,0.174,0.173,0.129,0.201,0.132,0.107,0.08,0.224,0.073,0.126,0.137,0.094,0.093,0.116,0.209,0.14,0.068,0.096,0.051,0.095,0.082,0.183,0.159,0.141,0.111,0.157,0.109,0.145,0.143,0.176,0.16,0.185,0.18,0.179,0.144,0.132,0.089,0.194,0.104,0.155,0.118,0.109,0.129,0.141,0.164,0.152,0.096,0.106,0.118,0.23,0.154,0.165,0.119,0.067,0.06,0.191,0.153,0.181,0.169,0.135,0.097,0.132,0.092,0.139,0.138,0.164,0.175,0.157,0.094,0.1,0.157,0.149,0.201,0.117,0.15,0.164,0.064,0.152,0.099,0.213,0.144,0.133,0.115,0.099,0.085,0.212,0.184,0.146,0.196,0.173,0.096,0.153,0.133,0.132,0.157,0.211,0.201,0.144,0.136,0.128,0.147,0.18,0.284,0.148,0.121,0.189,0.092,0.178,0.45,0.482,0.483,0.443,0.499,0.598,0.592,0.523,0.55,0.445,0.538,0.562,0.494,0.536,0.595,0.484,0.538,0.565,0.54,0.476,0.594,0.564,0.481,0.551,0.589,0.561,0.45,0.538,0.593,0.541,0.652,0.692,0.634,0.417,0.416,0.406,0.431,0.565,0.624,0.496,0.571,0.486,0.37,0.446,0.417,0.42,0.52,0.608,0.693,0.422,0.558,0.443,0.551,0.624,0.686,0.509,0.422,0.55,0.485,0.624,0.335,0.646,0.474,0.72,0.56,0.407,0.335,0.714,0.543,0.664,0.642,0.628,0.527,0.643,0.534,0.652,0.571,0.615,0.544,0.72,0.407,0.509,0.558,0.532,0.707,0.522,0.644,0.644,0.321,0.533,0,0,0,0,0,0,0,0,0,0,0,0,0.096,0.106,0.152,0.178,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-1.188,-1.188,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-1.188,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-1.188,-1.188,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-0.415,-0.415,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-1.188,-0.415,-1.188,-0.415,-1.188,-1.188,-0.415,-1.188,-1.188,-0.415,-0.415,-1.188,-1.188,-1.188,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-1.188,-0.415,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-0.415,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188,-0.415,-1.188,-0.415,-1.188,-0.415,-0.415,-1.188,-1.188],\"median\":[-381.306,0.073,0.087,0.08,0.094,0.098,0.101,0.102,0.114,0.529,0.575,0.632,0.713,0.243,0.23,0.67,0.662,0.359,0.534,0.507,0.52,0.297,0.149,0.356,0.782,0.802,0.878,0.869,0.704,0.609,0.79,0.885,0.743,0.703,0.81,0.859,0.625,0.468,0.793,0.788,0.431,0.803,0.481,0.855,-2.612,-2.442,-2.524,-2.368,-2.323,-2.292,-2.28,-2.173,-1.551,-0.238,-0.842,-0.219,-0.732,-0.157,-1.551,-0.238,-1.551,-0.238,-1.551,-0.238,-1.551,-0.238,-0.842,-0.842,-0.219,-0.219,-0.842,-0.842,-0.219,-0.219,-0.732,-0.732,-0.732,-0.732,-0.157,-0.157,-0.157,-0.157,0.212,0.431,0.481,0.212,0.431,0.481,0.212,0.431,0.481,0.212,0.431,0.481,0.212,0.431,0.481,0.788,0.431,0.481,0.788,0.431,0.481,0.788,0.431,0.481,0.788,0.431,0.481,0.788,0.431,0.481,0.212,0.803,0.481,0.212,0.803,0.481,0.212,0.803,0.481,0.212,0.803,0.481,0.212,0.803,0.481,0.788,0.803,0.481,0.788,0.803,0.481,0.788,0.803,0.481,0.788,0.803,0.481,0.788,0.803,0.481,0.212,0.431,0.855,0.212,0.431,0.855,0.212,0.431,0.855,0.212,0.431,0.855,0.212,0.431,0.855,0.788,0.431,0.855,0.788,0.431,0.855,0.788,0.431,0.855,0.788,0.431,0.855,0.788,0.431,0.855,0.212,0.803,0.855,0.212,0.803,0.855,0.212,0.803,0.855,0.212,0.803,0.855,0.212,0.803,0.855,0.788,0.803,0.855,0.788,0.803,0.855,0.788,0.803,0.855,0.788,0.803,0.855,0.788,0.803,0.855,0.676,-0.398,-0.32,-0.202,-0.213,-0.633,-0.677,-0.318,-0.204,-0.45,-0.483,-0.341,-0.253,-0.642,-0.409,-1.128,0.052,0.014,0.041,0.034,0.061,0.066,0.073,0.019,0.031,0.04,0.026,0.026,0.083,0.039,0.045,0.046,0.041,0.018,0.02,0.025,0.045,0.044,0.049,0.032,0.007,0.05,0.055,0.028,0.065,0.034,0.037,0.013,0.038,0.027,0.068,0.133,0.142,0.024,0.046,0.032,0.036,0.046,0.084,0.053,0.092,0.046,0.057,0.033,0.028,0.024,0.089,0.076,0.044,0.049,0.014,0.077,0.046,0.039,0.13,0.047,0.153,0.052,0.095,0.021,0.041,0.04,0.052,0.028,0.065,0.04,0.039,0.026,0.037,0.028,0.029,0.029,0.046,0.038,0.073,0.016,0.067,0.034,0.063,0.069,0.025,0.053,0.036,0.037,0.065,0.071,0.102,0.047,0.087,0.016,0.04,0.078,0.097,0.034,0.093,0.03,0.053,0.042,0.037,0.038,0.055,0.027,0.063,0.063,0.1,0.015,0.125,0.054,0.059,0.097,0.048,0.079,0.029,0.05,0.12,0.094,0.02,0.037,0.037,0.161,0.099,0.044,0.031,0.089,0.043,0.119,0.066,0.067,0.104,0.098,0.063,0.123,0.067,0.042,0.027,0.135,0.03,0.055,0.069,0.042,0.034,0.061,0.14,0.071,0.024,0.042,0.012,0.031,0.029,0.106,0.091,0.075,0.051,0.092,0.054,0.08,0.078,0.103,0.09,0.115,0.108,0.105,0.079,0.063,0.032,0.107,0.05,0.083,0.058,0.054,0.056,0.081,0.097,0.084,0.039,0.051,0.048,0.137,0.077,0.093,0.06,0.026,0.021,0.122,0.082,0.11,0.092,0.064,0.042,0.066,0.038,0.07,0.069,0.085,0.094,0.075,0.042,0.042,0.086,0.081,0.116,0.062,0.086,0.087,0.022,0.082,0.032,0.119,0.067,0.066,0.058,0.047,0.037,0.142,0.114,0.081,0.119,0.103,0.041,0.086,0.073,0.065,0.089,0.134,0.121,0.065,0.075,0.063,0.074,0.111,0.207,0.087,0.064,0.114,0.039,0.105,0.433,0.475,0.481,0.429,0.507,0.638,0.636,0.525,0.568,0.432,0.545,0.597,0.494,0.544,0.634,0.477,0.547,0.6,0.551,0.463,0.638,0.604,0.474,0.567,0.627,0.572,0.441,0.548,0.636,0.553,0.698,0.77,0.672,0.398,0.401,0.389,0.421,0.578,0.651,0.491,0.586,0.481,0.336,0.439,0.4,0.406,0.522,0.633,0.769,0.404,0.568,0.43,0.555,0.652,0.757,0.506,0.409,0.554,0.481,0.653,0.277,0.689,0.466,0.804,0.572,0.385,0.278,0.791,0.547,0.712,0.68,0.662,0.531,0.683,0.542,0.696,0.584,0.644,0.552,0.805,0.383,0.51,0.565,0.534,0.784,0.524,0.684,0.682,0.255,0.538,0,0,0,0,0,0,0,0,0,0,0,0,0.042,0.051,0.082,0.105,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-1.128,-1.128,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-1.128,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-1.128,-1.128,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-0.391,-0.391,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-1.128,-0.391,-1.128,-0.391,-1.128,-1.128,-0.391,-1.128,-1.128,-0.391,-0.391,-1.128,-1.128,-1.128,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-1.128,-0.391,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-0.391,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128,-0.391,-1.128,-0.391,-1.128,-0.391,-0.391,-1.128,-1.128],\"sd\":[5.066,0.095,0.103,0.101,0.112,0.114,0.118,0.12,0.126,0.185,0.192,0.192,0.141,0.183,0.191,0.186,0.177,0.201,0.206,0.193,0.199,0.207,0.17,0.202,0.169,0.136,0.097,0.107,0.192,0.189,0.165,0.107,0.153,0.201,0.141,0.111,0.233,0.23,0.144,0.247,0.253,0.196,0.259,0.17,1.282,1.229,1.231,1.229,1.194,1.242,1.226,1.229,1.347,0.534,1.005,0.365,0.936,0.301,1.347,0.534,1.347,0.534,1.347,0.534,1.347,0.534,1.005,1.005,0.365,0.365,1.005,1.005,0.365,0.365,0.936,0.936,0.936,0.936,0.301,0.301,0.301,0.301,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.253,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.196,0.259,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.253,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.247,0.196,0.17,0.117,0.232,0.147,0.108,0.135,0.273,0.294,0.222,0.109,0.202,0.293,0.163,0.138,0.466,0.22,0.413,0.17,0.101,0.134,0.133,0.16,0.177,0.188,0.095,0.106,0.126,0.103,0.129,0.164,0.123,0.15,0.134,0.125,0.107,0.103,0.133,0.144,0.167,0.127,0.105,0.077,0.125,0.138,0.105,0.178,0.107,0.152,0.099,0.133,0.113,0.163,0.218,0.227,0.104,0.127,0.108,0.119,0.153,0.17,0.138,0.191,0.132,0.139,0.132,0.119,0.127,0.182,0.197,0.127,0.127,0.103,0.154,0.124,0.12,0.217,0.125,0.244,0.174,0.198,0.107,0.124,0.135,0.154,0.115,0.155,0.125,0.13,0.121,0.126,0.107,0.118,0.114,0.136,0.145,0.176,0.11,0.164,0.139,0.157,0.159,0.132,0.137,0.113,0.127,0.167,0.161,0.227,0.169,0.193,0.098,0.127,0.17,0.19,0.128,0.179,0.116,0.152,0.14,0.126,0.127,0.148,0.113,0.154,0.167,0.194,0.109,0.199,0.16,0.155,0.178,0.167,0.156,0.103,0.146,0.202,0.179,0.111,0.161,0.136,0.229,0.183,0.137,0.121,0.172,0.133,0.198,0.166,0.179,0.19,0.191,0.161,0.212,0.163,0.15,0.127,0.237,0.108,0.166,0.167,0.128,0.14,0.14,0.204,0.171,0.111,0.133,0.097,0.148,0.125,0.199,0.177,0.167,0.148,0.173,0.14,0.168,0.169,0.194,0.18,0.193,0.191,0.198,0.168,0.166,0.134,0.219,0.136,0.182,0.152,0.139,0.172,0.159,0.179,0.175,0.138,0.139,0.165,0.243,0.189,0.188,0.149,0.104,0.098,0.194,0.18,0.192,0.193,0.17,0.132,0.163,0.13,0.171,0.172,0.194,0.203,0.194,0.129,0.14,0.184,0.172,0.217,0.144,0.171,0.191,0.105,0.178,0.152,0.236,0.182,0.164,0.143,0.131,0.121,0.207,0.194,0.169,0.207,0.19,0.134,0.176,0.159,0.165,0.181,0.218,0.214,0.184,0.159,0.161,0.179,0.191,0.254,0.165,0.147,0.203,0.13,0.193,0.331,0.33,0.325,0.278,0.273,0.297,0.299,0.258,0.284,0.273,0.296,0.33,0.293,0.292,0.295,0.3,0.291,0.329,0.298,0.327,0.291,0.333,0.322,0.281,0.287,0.269,0.257,0.295,0.298,0.291,0.264,0.266,0.255,0.25,0.25,0.243,0.246,0.248,0.243,0.239,0.247,0.241,0.246,0.239,0.251,0.246,0.251,0.247,0.261,0.254,0.247,0.256,0.237,0.243,0.263,0.244,0.249,0.247,0.242,0.243,0.257,0.26,0.252,0.26,0.261,0.253,0.253,0.257,0.253,0.258,0.254,0.254,0.247,0.252,0.255,0.251,0.256,0.252,0.258,0.264,0.245,0.258,0.241,0.248,0.262,0.242,0.248,0.253,0.257,0.246,0,0,0,0,0,0,0,0,0,0,0,0,0.133,0.139,0.178,0.193,0.413,0.413,0.413,0.413,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.413,0.192,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.413,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.413,0.413,0.413,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.192,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.413,0.192,0.413,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.413,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.192,0.413,0.192,0.413,0.192,0.192,0.413,0.413,0.413,0.413,0.413,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.413,0.413,0.413,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.413,0.192,0.192,0.192,0.192,0.413,0.413,0.413,0.413,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.192,0.413,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.192,0.413,0.192,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.413,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.413,0.413,0.413,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.413,0.413,0.192,0.413,0.413,0.192,0.192,0.413,0.192,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.192,0.192,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.413,0.192,0.413,0.192,0.413,0.413,0.192,0.413,0.413,0.192,0.192,0.413,0.413,0.413,0.413,0.413,0.192,0.192,0.192,0.192,0.413,0.413,0.192,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.192,0.192,0.413,0.192,0.192,0.413,0.413,0.192,0.413,0.192,0.413,0.192,0.192,0.413,0.413],\"mad\":[5,0.075,0.087,0.08,0.094,0.098,0.102,0.101,0.111,0.194,0.201,0.198,0.141,0.18,0.186,0.204,0.179,0.219,0.235,0.205,0.214,0.194,0.133,0.217,0.196,0.146,0.096,0.108,0.224,0.208,0.181,0.102,0.169,0.231,0.152,0.114,0.286,0.251,0.159,0.243,0.306,0.173,0.316,0.132,1.131,1.076,1.093,1.102,1.075,1.089,1.076,1.057,1.276,0.289,0.703,0.209,0.636,0.151,1.276,0.289,1.276,0.289,1.276,0.289,1.276,0.289,0.703,0.703,0.209,0.209,0.703,0.703,0.209,0.209,0.636,0.636,0.636,0.636,0.151,0.151,0.151,0.151,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.306,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.173,0.316,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.306,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.243,0.173,0.132,0.116,0.207,0.131,0.099,0.111,0.247,0.234,0.163,0.1,0.176,0.222,0.15,0.12,0.296,0.171,0.357,0.072,0.019,0.055,0.046,0.079,0.087,0.096,0.026,0.041,0.053,0.035,0.037,0.106,0.052,0.06,0.061,0.054,0.026,0.027,0.036,0.059,0.062,0.064,0.042,0.01,0.065,0.07,0.038,0.086,0.045,0.052,0.019,0.051,0.037,0.089,0.161,0.173,0.033,0.059,0.043,0.047,0.064,0.105,0.069,0.116,0.061,0.073,0.046,0.038,0.034,0.113,0.103,0.059,0.062,0.02,0.094,0.06,0.051,0.158,0.059,0.196,0.072,0.126,0.029,0.053,0.053,0.068,0.038,0.083,0.053,0.052,0.036,0.049,0.037,0.039,0.039,0.06,0.053,0.094,0.023,0.086,0.047,0.083,0.088,0.035,0.068,0.046,0.049,0.084,0.09,0.143,0.066,0.117,0.022,0.053,0.097,0.119,0.046,0.116,0.041,0.069,0.058,0.049,0.05,0.071,0.037,0.081,0.085,0.126,0.022,0.148,0.074,0.08,0.12,0.066,0.097,0.038,0.066,0.144,0.115,0.028,0.052,0.05,0.19,0.12,0.058,0.041,0.111,0.056,0.142,0.086,0.091,0.128,0.124,0.081,0.154,0.086,0.058,0.037,0.177,0.04,0.075,0.091,0.055,0.046,0.077,0.161,0.092,0.033,0.055,0.017,0.043,0.041,0.134,0.113,0.094,0.066,0.112,0.068,0.102,0.098,0.133,0.115,0.138,0.132,0.133,0.099,0.085,0.044,0.142,0.064,0.109,0.078,0.069,0.075,0.099,0.117,0.106,0.051,0.065,0.066,0.181,0.103,0.117,0.076,0.035,0.028,0.146,0.104,0.132,0.118,0.087,0.056,0.086,0.051,0.091,0.09,0.115,0.121,0.103,0.055,0.057,0.112,0.103,0.149,0.078,0.105,0.112,0.03,0.105,0.047,0.161,0.092,0.087,0.075,0.061,0.048,0.165,0.136,0.102,0.147,0.131,0.055,0.108,0.091,0.087,0.11,0.17,0.149,0.091,0.092,0.084,0.1,0.133,0.245,0.104,0.081,0.138,0.051,0.128,0.448,0.451,0.437,0.344,0.33,0.375,0.38,0.312,0.362,0.336,0.381,0.447,0.366,0.373,0.37,0.379,0.374,0.444,0.389,0.443,0.366,0.453,0.427,0.358,0.362,0.332,0.309,0.378,0.379,0.374,0.309,0.264,0.304,0.299,0.301,0.287,0.292,0.295,0.283,0.275,0.296,0.283,0.285,0.277,0.298,0.293,0.297,0.291,0.261,0.307,0.288,0.304,0.277,0.285,0.268,0.282,0.298,0.292,0.283,0.284,0.279,0.3,0.298,0.238,0.314,0.301,0.273,0.245,0.301,0.295,0.29,0.302,0.291,0.288,0.304,0.291,0.303,0.295,0.313,0.239,0.289,0.309,0.281,0.293,0.255,0.282,0.288,0.292,0.273,0.29,0,0,0,0,0,0,0,0,0,0,0,0,0.055,0.065,0.105,0.128,0.357,0.357,0.357,0.357,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.357,0.171,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.357,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.357,0.357,0.357,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.171,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.357,0.171,0.357,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.357,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.171,0.357,0.171,0.357,0.171,0.171,0.357,0.357,0.357,0.357,0.357,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.357,0.357,0.357,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.357,0.171,0.171,0.171,0.171,0.357,0.357,0.357,0.357,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.171,0.357,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.171,0.357,0.171,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.357,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.357,0.357,0.357,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.357,0.357,0.171,0.357,0.357,0.171,0.171,0.357,0.171,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.171,0.171,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.357,0.171,0.357,0.171,0.357,0.357,0.171,0.357,0.357,0.171,0.171,0.357,0.357,0.357,0.357,0.357,0.171,0.171,0.171,0.171,0.357,0.357,0.171,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.171,0.171,0.357,0.171,0.171,0.357,0.357,0.171,0.357,0.171,0.357,0.171,0.171,0.357,0.357],\"q5\":[-390.606,0.005,0.007,0.006,0.007,0.008,0.008,0.008,0.009,0.253,0.22,0.254,0.458,0.038,0.03,0.333,0.318,0.074,0.194,0.177,0.167,0.078,0.019,0.078,0.453,0.54,0.678,0.646,0.359,0.323,0.461,0.659,0.472,0.328,0.536,0.636,0.229,0.184,0.52,0.214,0.048,0.347,0.054,0.453,-5.252,-5.019,-5.082,-4.926,-4.788,-4.864,-4.848,-4.767,-4.447,-1.542,-3.041,-1.058,-2.913,-0.792,-4.447,-1.542,-4.447,-1.542,-4.447,-1.542,-4.447,-1.542,-3.041,-3.041,-1.058,-1.058,-3.041,-3.041,-1.058,-1.058,-2.913,-2.913,-2.913,-2.913,-0.792,-0.792,-0.792,-0.792,0.012,0.048,0.054,0.012,0.048,0.054,0.012,0.048,0.054,0.012,0.048,0.054,0.012,0.048,0.054,0.214,0.048,0.054,0.214,0.048,0.054,0.214,0.048,0.054,0.214,0.048,0.054,0.214,0.048,0.054,0.012,0.347,0.054,0.012,0.347,0.054,0.012,0.347,0.054,0.012,0.347,0.054,0.012,0.347,0.054,0.214,0.347,0.054,0.214,0.347,0.054,0.214,0.347,0.054,0.214,0.347,0.054,0.214,0.347,0.054,0.012,0.048,0.453,0.012,0.048,0.453,0.012,0.048,0.453,0.012,0.048,0.453,0.012,0.048,0.453,0.214,0.048,0.453,0.214,0.048,0.453,0.214,0.048,0.453,0.214,0.048,0.453,0.214,0.048,0.453,0.012,0.347,0.453,0.012,0.347,0.453,0.012,0.347,0.453,0.012,0.347,0.453,0.012,0.347,0.453,0.214,0.347,0.453,0.214,0.347,0.453,0.214,0.347,0.453,0.214,0.347,0.453,0.214,0.347,0.453,0.48,-0.814,-0.605,-0.413,-0.462,-1.108,-1.261,-0.751,-0.421,-0.818,-1.099,-0.643,-0.52,-1.697,-0.852,-1.935,0.001,0,0.001,0.001,0.002,0.002,0.002,0,0.001,0.001,0,0,0.002,0.001,0.001,0.001,0.001,0,0,0,0.001,0.001,0.001,0.001,0,0.001,0.002,0,0.002,0.001,0,0,0.001,0,0.002,0.005,0.005,0,0.001,0.001,0.001,0.001,0.003,0.001,0.003,0.001,0.002,0,0.001,0,0.003,0.001,0.001,0.002,0,0.003,0.001,0.001,0.005,0.001,0.003,0.001,0.002,0,0.001,0.001,0.002,0,0.002,0.001,0.001,0,0.001,0.001,0.001,0,0.001,0.001,0.002,0,0.002,0,0.001,0.002,0,0.002,0.001,0.001,0.002,0.002,0.001,0.001,0.001,0,0.001,0.003,0.004,0.001,0.003,0.001,0.001,0.001,0.001,0.001,0.002,0,0.002,0.001,0.003,0,0.005,0.001,0.001,0.004,0.001,0.003,0.001,0.001,0.005,0.004,0,0,0.001,0.007,0.004,0.001,0.001,0.003,0.001,0.005,0.002,0.001,0.003,0.003,0.002,0.004,0.002,0.001,0.001,0.002,0.001,0.001,0.001,0.001,0.001,0.002,0.007,0.002,0.001,0.001,0,0,0,0.004,0.003,0.002,0.002,0.004,0.002,0.003,0.003,0.002,0.003,0.005,0.004,0.003,0.003,0.001,0.001,0.001,0.001,0.001,0.001,0.002,0.001,0.003,0.004,0.003,0.001,0.001,0.001,0.002,0.001,0.003,0.002,0,0,0.005,0.003,0.005,0.003,0.001,0.001,0.002,0.001,0.002,0.002,0.002,0.003,0.001,0.001,0.001,0.002,0.002,0.003,0.002,0.003,0.003,0,0.003,0,0.002,0.001,0.002,0.002,0.001,0.001,0.006,0.004,0.003,0.004,0.002,0.001,0.003,0.002,0.001,0.003,0.003,0.004,0.001,0.003,0.001,0.001,0.004,0.007,0.004,0.002,0.004,0.001,0.004,0.004,0.011,0.013,0.034,0.05,0.075,0.075,0.101,0.077,0.036,0.062,0.022,0.036,0.064,0.08,0.026,0.067,0.023,0.06,0.012,0.089,0.02,0.013,0.085,0.089,0.105,0.055,0.062,0.077,0.068,0.168,0.159,0.174,0.051,0.047,0.05,0.061,0.146,0.183,0.114,0.149,0.1,0.034,0.077,0.048,0.054,0.11,0.173,0.174,0.049,0.141,0.056,0.151,0.185,0.165,0.105,0.051,0.139,0.099,0.182,0.022,0.165,0.08,0.181,0.115,0.043,0.024,0.184,0.125,0.178,0.18,0.173,0.121,0.183,0.11,0.19,0.133,0.168,0.117,0.17,0.053,0.092,0.153,0.124,0.177,0.124,0.196,0.181,0.018,0.124,0,0,0,0,0,0,0,0,0,0,0,0,0.001,0.001,0.003,0.004,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-1.935,-1.935,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-1.935,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-1.935,-1.935,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-0.735,-0.735,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-1.935,-0.735,-1.935,-0.735,-1.935,-1.935,-0.735,-1.935,-1.935,-0.735,-0.735,-1.935,-1.935,-1.935,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-1.935,-0.735,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-0.735,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935,-0.735,-1.935,-0.735,-1.935,-0.735,-0.735,-1.935,-1.935],\"q95\":[-373.992,0.288,0.328,0.318,0.345,0.359,0.371,0.382,0.4,0.868,0.855,0.892,0.91,0.632,0.646,0.931,0.895,0.73,0.864,0.815,0.828,0.757,0.558,0.743,0.98,0.977,0.984,0.988,0.971,0.943,0.983,0.99,0.969,0.971,0.98,0.985,0.962,0.926,0.979,0.988,0.851,0.978,0.88,0.986,-1.244,-1.114,-1.147,-1.063,-1.023,-0.991,-0.962,-0.918,-0.241,-0.012,-0.161,-0.022,-0.128,-0.015,-0.241,-0.012,-0.241,-0.012,-0.241,-0.012,-0.241,-0.012,-0.161,-0.161,-0.022,-0.022,-0.161,-0.161,-0.022,-0.022,-0.128,-0.128,-0.128,-0.128,-0.015,-0.015,-0.015,-0.015,0.786,0.851,0.88,0.786,0.851,0.88,0.786,0.851,0.88,0.786,0.851,0.88,0.786,0.851,0.88,0.988,0.851,0.88,0.988,0.851,0.88,0.988,0.851,0.88,0.988,0.851,0.88,0.988,0.851,0.88,0.786,0.978,0.88,0.786,0.978,0.88,0.786,0.978,0.88,0.786,0.978,0.88,0.786,0.978,0.88,0.988,0.978,0.88,0.988,0.978,0.88,0.988,0.978,0.88,0.988,0.978,0.88,0.988,0.978,0.88,0.786,0.851,0.986,0.786,0.851,0.986,0.786,0.851,0.986,0.786,0.851,0.986,0.786,0.851,0.986,0.988,0.851,0.986,0.988,0.851,0.986,0.988,0.851,0.986,0.988,0.851,0.986,0.988,0.851,0.986,0.786,0.978,0.986,0.786,0.978,0.986,0.786,0.978,0.986,0.786,0.978,0.986,0.786,0.978,0.986,0.988,0.978,0.986,0.988,0.978,0.986,0.988,0.978,0.986,0.988,0.978,0.986,0.988,0.978,0.986,0.856,-0.099,-0.135,-0.075,-0.061,-0.244,-0.361,-0.094,-0.07,-0.197,-0.174,-0.133,-0.091,-0.243,-0.189,-0.653,0.517,0.242,0.398,0.381,0.491,0.537,0.586,0.253,0.299,0.364,0.281,0.365,0.505,0.346,0.44,0.389,0.357,0.287,0.271,0.368,0.43,0.499,0.369,0.302,0.181,0.365,0.411,0.291,0.548,0.305,0.444,0.244,0.384,0.319,0.49,0.687,0.722,0.286,0.373,0.307,0.339,0.445,0.519,0.408,0.584,0.383,0.417,0.371,0.335,0.355,0.562,0.605,0.375,0.371,0.261,0.469,0.362,0.352,0.687,0.362,0.765,0.512,0.611,0.283,0.358,0.387,0.46,0.319,0.469,0.366,0.377,0.336,0.361,0.29,0.323,0.316,0.406,0.419,0.546,0.287,0.499,0.393,0.472,0.484,0.367,0.399,0.323,0.363,0.504,0.493,0.699,0.51,0.598,0.246,0.364,0.519,0.591,0.367,0.552,0.324,0.45,0.409,0.361,0.364,0.441,0.31,0.462,0.507,0.608,0.279,0.621,0.481,0.461,0.548,0.5,0.471,0.289,0.427,0.637,0.554,0.299,0.471,0.384,0.727,0.564,0.403,0.341,0.533,0.385,0.619,0.505,0.547,0.602,0.592,0.492,0.663,0.491,0.437,0.35,0.743,0.306,0.492,0.507,0.38,0.4,0.412,0.65,0.515,0.301,0.383,0.244,0.428,0.351,0.62,0.547,0.515,0.434,0.54,0.41,0.512,0.509,0.598,0.552,0.6,0.598,0.61,0.518,0.501,0.385,0.679,0.392,0.558,0.447,0.415,0.513,0.482,0.555,0.536,0.396,0.407,0.501,0.756,0.587,0.587,0.448,0.28,0.257,0.602,0.557,0.603,0.603,0.509,0.377,0.493,0.375,0.518,0.521,0.591,0.637,0.594,0.367,0.41,0.568,0.533,0.68,0.431,0.517,0.592,0.271,0.552,0.442,0.741,0.562,0.489,0.421,0.381,0.331,0.652,0.601,0.507,0.647,0.584,0.373,0.535,0.476,0.504,0.56,0.686,0.67,0.551,0.485,0.491,0.548,0.596,0.799,0.51,0.449,0.636,0.37,0.596,0.979,0.983,0.982,0.913,0.927,0.98,0.978,0.932,0.967,0.906,0.973,0.988,0.958,0.97,0.978,0.961,0.97,0.988,0.974,0.983,0.974,0.99,0.981,0.967,0.973,0.961,0.885,0.971,0.978,0.971,0.983,0.988,0.973,0.851,0.85,0.831,0.856,0.941,0.962,0.885,0.944,0.883,0.822,0.848,0.851,0.842,0.921,0.96,0.987,0.86,0.938,0.88,0.922,0.961,0.986,0.905,0.852,0.939,0.882,0.963,0.825,0.982,0.897,0.993,0.955,0.851,0.82,0.993,0.94,0.986,0.978,0.976,0.921,0.977,0.934,0.979,0.957,0.969,0.946,0.994,0.84,0.922,0.931,0.927,0.993,0.912,0.976,0.978,0.824,0.923,0,0,0.001,0.001,0,0,0.001,0.001,0,0,0,0,0.383,0.407,0.552,0.596,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.653,-0.653,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.653,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.653,-0.653,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.156,-0.156,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.653,-0.156,-0.653,-0.156,-0.653,-0.653,-0.156,-0.653,-0.653,-0.156,-0.156,-0.653,-0.653,-0.653,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.653,-0.156,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.156,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653,-0.156,-0.653,-0.156,-0.653,-0.156,-0.156,-0.653,-0.653]},\"columns\":[{\"id\":\"variable\",\"name\":\"variable\",\"type\":\"character\"},{\"id\":\"mean\",\"name\":\"mean\",\"type\":\"numeric\"},{\"id\":\"median\",\"name\":\"median\",\"type\":\"numeric\"},{\"id\":\"sd\",\"name\":\"sd\",\"type\":\"numeric\"},{\"id\":\"mad\",\"name\":\"mad\",\"type\":\"numeric\"},{\"id\":\"q5\",\"name\":\"q5\",\"type\":\"numeric\"},{\"id\":\"q95\",\"name\":\"q95\",\"type\":\"numeric\"}],\"filterable\":true,\"searchable\":true,\"highlight\":true,\"dataKey\":\"bdef6e4f9c22242b942e78efaf5099b2\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbn_measure |> \n  mutate(across(-variable, ~round(.x, 3))) |> \n  filter(str_detect(variable, \"prob_resp_attr\")) |>\n  react_table()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-00a6c970eec1c85a40d3\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-00a6c970eec1c85a40d3\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"variable\":[\"prob_resp_attr[1,1]\",\"prob_resp_attr[2,1]\",\"prob_resp_attr[3,1]\",\"prob_resp_attr[4,1]\",\"prob_resp_attr[5,1]\",\"prob_resp_attr[6,1]\",\"prob_resp_attr[7,1]\",\"prob_resp_attr[8,1]\",\"prob_resp_attr[9,1]\",\"prob_resp_attr[10,1]\",\"prob_resp_attr[11,1]\",\"prob_resp_attr[12,1]\",\"prob_resp_attr[13,1]\",\"prob_resp_attr[14,1]\",\"prob_resp_attr[15,1]\",\"prob_resp_attr[16,1]\",\"prob_resp_attr[17,1]\",\"prob_resp_attr[18,1]\",\"prob_resp_attr[19,1]\",\"prob_resp_attr[20,1]\",\"prob_resp_attr[21,1]\",\"prob_resp_attr[22,1]\",\"prob_resp_attr[23,1]\",\"prob_resp_attr[24,1]\",\"prob_resp_attr[25,1]\",\"prob_resp_attr[26,1]\",\"prob_resp_attr[27,1]\",\"prob_resp_attr[28,1]\",\"prob_resp_attr[29,1]\",\"prob_resp_attr[30,1]\",\"prob_resp_attr[1,2]\",\"prob_resp_attr[2,2]\",\"prob_resp_attr[3,2]\",\"prob_resp_attr[4,2]\",\"prob_resp_attr[5,2]\",\"prob_resp_attr[6,2]\",\"prob_resp_attr[7,2]\",\"prob_resp_attr[8,2]\",\"prob_resp_attr[9,2]\",\"prob_resp_attr[10,2]\",\"prob_resp_attr[11,2]\",\"prob_resp_attr[12,2]\",\"prob_resp_attr[13,2]\",\"prob_resp_attr[14,2]\",\"prob_resp_attr[15,2]\",\"prob_resp_attr[16,2]\",\"prob_resp_attr[17,2]\",\"prob_resp_attr[18,2]\",\"prob_resp_attr[19,2]\",\"prob_resp_attr[20,2]\",\"prob_resp_attr[21,2]\",\"prob_resp_attr[22,2]\",\"prob_resp_attr[23,2]\",\"prob_resp_attr[24,2]\",\"prob_resp_attr[25,2]\",\"prob_resp_attr[26,2]\",\"prob_resp_attr[27,2]\",\"prob_resp_attr[28,2]\",\"prob_resp_attr[29,2]\",\"prob_resp_attr[30,2]\",\"prob_resp_attr[1,3]\",\"prob_resp_attr[2,3]\",\"prob_resp_attr[3,3]\",\"prob_resp_attr[4,3]\",\"prob_resp_attr[5,3]\",\"prob_resp_attr[6,3]\",\"prob_resp_attr[7,3]\",\"prob_resp_attr[8,3]\",\"prob_resp_attr[9,3]\",\"prob_resp_attr[10,3]\",\"prob_resp_attr[11,3]\",\"prob_resp_attr[12,3]\",\"prob_resp_attr[13,3]\",\"prob_resp_attr[14,3]\",\"prob_resp_attr[15,3]\",\"prob_resp_attr[16,3]\",\"prob_resp_attr[17,3]\",\"prob_resp_attr[18,3]\",\"prob_resp_attr[19,3]\",\"prob_resp_attr[20,3]\",\"prob_resp_attr[21,3]\",\"prob_resp_attr[22,3]\",\"prob_resp_attr[23,3]\",\"prob_resp_attr[24,3]\",\"prob_resp_attr[25,3]\",\"prob_resp_attr[26,3]\",\"prob_resp_attr[27,3]\",\"prob_resp_attr[28,3]\",\"prob_resp_attr[29,3]\",\"prob_resp_attr[30,3]\"],\"mean\":[0.45,0.482,0.483,0.443,0.499,0.598,0.592,0.523,0.55,0.445,0.538,0.562,0.494,0.536,0.595,0.484,0.538,0.565,0.54,0.476,0.594,0.564,0.481,0.551,0.589,0.561,0.45,0.538,0.593,0.541,0.652,0.692,0.634,0.417,0.416,0.406,0.431,0.565,0.624,0.496,0.571,0.486,0.37,0.446,0.417,0.42,0.52,0.608,0.693,0.422,0.558,0.443,0.551,0.624,0.686,0.509,0.422,0.55,0.485,0.624,0.335,0.646,0.474,0.72,0.56,0.407,0.335,0.714,0.543,0.664,0.642,0.628,0.527,0.643,0.534,0.652,0.571,0.615,0.544,0.72,0.407,0.509,0.558,0.532,0.707,0.522,0.644,0.644,0.321,0.533],\"median\":[0.433,0.475,0.481,0.429,0.507,0.638,0.636,0.525,0.568,0.432,0.545,0.597,0.494,0.544,0.634,0.477,0.547,0.6,0.551,0.463,0.638,0.604,0.474,0.567,0.627,0.572,0.441,0.548,0.636,0.553,0.698,0.77,0.672,0.398,0.401,0.389,0.421,0.578,0.651,0.491,0.586,0.481,0.336,0.439,0.4,0.406,0.522,0.633,0.769,0.404,0.568,0.43,0.555,0.652,0.757,0.506,0.409,0.554,0.481,0.653,0.277,0.689,0.466,0.804,0.572,0.385,0.278,0.791,0.547,0.712,0.68,0.662,0.531,0.683,0.542,0.696,0.584,0.644,0.552,0.805,0.383,0.51,0.565,0.534,0.784,0.524,0.684,0.682,0.255,0.538],\"sd\":[0.331,0.33,0.325,0.278,0.273,0.297,0.299,0.258,0.284,0.273,0.296,0.33,0.293,0.292,0.295,0.3,0.291,0.329,0.298,0.327,0.291,0.333,0.322,0.281,0.287,0.269,0.257,0.295,0.298,0.291,0.264,0.266,0.255,0.25,0.25,0.243,0.246,0.248,0.243,0.239,0.247,0.241,0.246,0.239,0.251,0.246,0.251,0.247,0.261,0.254,0.247,0.256,0.237,0.243,0.263,0.244,0.249,0.247,0.242,0.243,0.257,0.26,0.252,0.26,0.261,0.253,0.253,0.257,0.253,0.258,0.254,0.254,0.247,0.252,0.255,0.251,0.256,0.252,0.258,0.264,0.245,0.258,0.241,0.248,0.262,0.242,0.248,0.253,0.257,0.246],\"mad\":[0.448,0.451,0.437,0.344,0.33,0.375,0.38,0.312,0.362,0.336,0.381,0.447,0.366,0.373,0.37,0.379,0.374,0.444,0.389,0.443,0.366,0.453,0.427,0.358,0.362,0.332,0.309,0.378,0.379,0.374,0.309,0.264,0.304,0.299,0.301,0.287,0.292,0.295,0.283,0.275,0.296,0.283,0.285,0.277,0.298,0.293,0.297,0.291,0.261,0.307,0.288,0.304,0.277,0.285,0.268,0.282,0.298,0.292,0.283,0.284,0.279,0.3,0.298,0.238,0.314,0.301,0.273,0.245,0.301,0.295,0.29,0.302,0.291,0.288,0.304,0.291,0.303,0.295,0.313,0.239,0.289,0.309,0.281,0.293,0.255,0.282,0.288,0.292,0.273,0.29],\"q5\":[0.004,0.011,0.013,0.034,0.05,0.075,0.075,0.101,0.077,0.036,0.062,0.022,0.036,0.064,0.08,0.026,0.067,0.023,0.06,0.012,0.089,0.02,0.013,0.085,0.089,0.105,0.055,0.062,0.077,0.068,0.168,0.159,0.174,0.051,0.047,0.05,0.061,0.146,0.183,0.114,0.149,0.1,0.034,0.077,0.048,0.054,0.11,0.173,0.174,0.049,0.141,0.056,0.151,0.185,0.165,0.105,0.051,0.139,0.099,0.182,0.022,0.165,0.08,0.181,0.115,0.043,0.024,0.184,0.125,0.178,0.18,0.173,0.121,0.183,0.11,0.19,0.133,0.168,0.117,0.17,0.053,0.092,0.153,0.124,0.177,0.124,0.196,0.181,0.018,0.124],\"q95\":[0.979,0.983,0.982,0.913,0.927,0.98,0.978,0.932,0.967,0.906,0.973,0.988,0.958,0.97,0.978,0.961,0.97,0.988,0.974,0.983,0.974,0.99,0.981,0.967,0.973,0.961,0.885,0.971,0.978,0.971,0.983,0.988,0.973,0.851,0.85,0.831,0.856,0.941,0.962,0.885,0.944,0.883,0.822,0.848,0.851,0.842,0.921,0.96,0.987,0.86,0.938,0.88,0.922,0.961,0.986,0.905,0.852,0.939,0.882,0.963,0.825,0.982,0.897,0.993,0.955,0.851,0.82,0.993,0.94,0.986,0.978,0.976,0.921,0.977,0.934,0.979,0.957,0.969,0.946,0.994,0.84,0.922,0.931,0.927,0.993,0.912,0.976,0.978,0.824,0.923]},\"columns\":[{\"id\":\"variable\",\"name\":\"variable\",\"type\":\"character\"},{\"id\":\"mean\",\"name\":\"mean\",\"type\":\"numeric\"},{\"id\":\"median\",\"name\":\"median\",\"type\":\"numeric\"},{\"id\":\"sd\",\"name\":\"sd\",\"type\":\"numeric\"},{\"id\":\"mad\",\"name\":\"mad\",\"type\":\"numeric\"},{\"id\":\"q5\",\"name\":\"q5\",\"type\":\"numeric\"},{\"id\":\"q95\",\"name\":\"q95\",\"type\":\"numeric\"}],\"filterable\":true,\"searchable\":true,\"highlight\":true,\"dataKey\":\"4c7124ef3935e33d345ea2f867e37fdc\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nI decided to filter in on the probabilities for students to have mastery over the attributes. The first index in the square brackets indicates the student and then the second index value indicates the three attributes. Obviously for something more thought out this would line up for meaningful attributes, but for this example, the values align with arbitrary values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_rep <- fit$draws(\"x_rep\") |> as_draws_matrix()\nstu_resp_attr <- fit$draws(\"prob_resp_attr\") |> as_draws_matrix()\n```\n:::\n\n\nI decided to extract the replicated values for the items and the probabilities of each student's mastery of each of the three latent attributes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n\n```{.r .cell-code}\ny |> react_table()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-e7c0a0f8b10b21911ad9\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e7c0a0f8b10b21911ad9\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"studentid\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\"y1\":[0,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,1],\"y2\":[0,1,1,1,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1],\"y3\":[1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y4\":[0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1],\"y5\":[1,1,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,1,1,0,1,0,1,1,1,0,0,1,0,1],\"y6\":[0,1,0,1,1,0,0,1,0,1,0,1,0,0,1,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0],\"y7\":[1,1,1,1,0,0,1,0,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1],\"y8\":[1,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,0,1,1],\"y9\":[0,0,0,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,1],\"y10\":[1,1,1,0,0,1,0,1,1,0,1,0,1,1,1,1,1,0,1,1,0,0,1,1,0,1,0,1,0,1],\"y11\":[1,1,0,0,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,0,0,1,1,0,1,1,1,1],\"y12\":[0,1,1,1,0,0,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1],\"y13\":[0,0,0,0,0,1,1,0,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,1,1,1,0,0,1,0],\"y14\":[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,0,0],\"y15\":[0,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0]},\"columns\":[{\"id\":\"studentid\",\"name\":\"studentid\",\"type\":\"numeric\"},{\"id\":\"y1\",\"name\":\"y1\",\"type\":\"numeric\"},{\"id\":\"y2\",\"name\":\"y2\",\"type\":\"numeric\"},{\"id\":\"y3\",\"name\":\"y3\",\"type\":\"numeric\"},{\"id\":\"y4\",\"name\":\"y4\",\"type\":\"numeric\"},{\"id\":\"y5\",\"name\":\"y5\",\"type\":\"numeric\"},{\"id\":\"y6\",\"name\":\"y6\",\"type\":\"numeric\"},{\"id\":\"y7\",\"name\":\"y7\",\"type\":\"numeric\"},{\"id\":\"y8\",\"name\":\"y8\",\"type\":\"numeric\"},{\"id\":\"y9\",\"name\":\"y9\",\"type\":\"numeric\"},{\"id\":\"y10\",\"name\":\"y10\",\"type\":\"numeric\"},{\"id\":\"y11\",\"name\":\"y11\",\"type\":\"numeric\"},{\"id\":\"y12\",\"name\":\"y12\",\"type\":\"numeric\"},{\"id\":\"y13\",\"name\":\"y13\",\"type\":\"numeric\"},{\"id\":\"y14\",\"name\":\"y14\",\"type\":\"numeric\"},{\"id\":\"y15\",\"name\":\"y15\",\"type\":\"numeric\"}],\"filterable\":true,\"searchable\":true,\"highlight\":true,\"dataKey\":\"e4070ee024f9a9b445f3493d12f7fe26\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the original data, the original responses and the probabilities with a probability threshold of 0.5 match one another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n\n```{.r .cell-code}\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-2.png){width=768}\n:::\n\n```{.r .cell-code}\nppc_intervals(\n  y = y |> pull(y1) |> as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-3.png){width=768}\n:::\n:::\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nactual_stu_resp_attr <- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |>\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x > .5, 1, 0)\n    )\n  )\n```\n:::\n\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstu_resp_attr_mean <- stu_resp_attr |>\n  as_tibble() |>\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class <- stu_resp_attr_mean |>\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x > .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class <- stu_resp_attr_class |>\n  pivot_longer(\n    everything()\n  ) |>\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |>\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |>\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n```\n:::\n\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$att1\n   .y\n.x   0  1\n  0  6  5\n  1  7 12\n\n$att2\n   .y\n.x   0  1\n  0  6  8\n  1  5 11\n\n$att3\n   .y\n.x   0  1\n  0  4  2\n  1 11 13\n```\n\n\n:::\n\n```{.r .cell-code}\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$att1\n   .y\n.x          0         1\n  0 0.2000000 0.1666667\n  1 0.2333333 0.4000000\n\n$att2\n   .y\n.x          0         1\n  0 0.2000000 0.2666667\n  1 0.1666667 0.3666667\n\n$att3\n   .y\n.x           0          1\n  0 0.13333333 0.06666667\n  1 0.36666667 0.43333333\n```\n\n\n:::\n:::\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstu_resp_attr_long <- stu_resp_attr_class |>\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long <- actual_stu_resp_attr |>\n  pivot_longer(-studentid)\n\naccuracy_att <- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5777778\n```\n\n\n:::\n:::\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 58%. This is a good starting point, but this may indicate that the model needs better defined priors and may require the edges between the attributes to show latent relationships. The low accuracy value may also be indicative of the importance of domain knowledge in building a latent bayes net.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../site_libs/react-18.2.0/react.min.js\"></script>\n<script src=\"../../site_libs/react-18.2.0/react-dom.min.js\"></script>\n<script src=\"../../site_libs/reactwidget-2.0.0/react-tools.js\"></script>\n<link href=\"../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/reactable-0.4.4/reactable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/reactable-binding-0.4.4/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}