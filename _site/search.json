[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! I am Jonathan Andrew Pedroza PhD, but I have gotten used to everyone just referring to me as JP. I am currently on the job market looking for any data scientist, data analyst, or psychometrician openings. I received my PhD in Prevention Science from the University of Oregon in 2021. My training as a Prevention Scientist included training in program evaluation, implementation science, machine learning, inferential statistics, and survey design. I am also a Posit (Previously RStudio) Academy Mentor to cohorts around the world where I guide learners to find answers to their R or Python code. I also provide tips and tricks that I have learned while working in R and Python while also showcasing additional packages and best coding practices.\nSome of my research interests include: the examination of physical activity and sedentary behaviors, environmental factors that contribute to inequities of access and engagement in health behaviors, and educational interventions. I am open to learning more about a new field, especially while working with various datasets with my Posit Academy cohorts.\nI am proficient in using base R and the Tidyverse, along with Python and SQL for databases. I also have working knowledge using Stan for Bayesian statistics and CSS/HTML for making changes in Shiny applications, interactive documents, and my website. When I am away from my computer, I enjoy EVERYTHING about coffee, hiking, fishing, cooking, and playing with my cats. I’m currently on BlueSky for social media and infrequently check on Twitter/X/whatever it is called now and Mastodon. You can also email me  at jonpedroza1228@gmail.com."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Posts",
    "section": "",
    "text": "My submission to Posit’s Closeread Competition\n\n\n\ncloseread\n\n\nggplot2\n\n\nHTML\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Network for US Coffee Tasting Data Using R & Stan\n\n\n\nBayesian\n\n\nBayesian Network\n\n\nbayes net\n\n\nR\n\n\nrstan\n\n\ncmdstanr\n\n\nposterior\n\n\nbayesplot\n\n\ndag\n\n\n\n\n\n\n\n\n\n\nNov 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Network for US Coffee Tasting Data\n\n\n\nBayesian\n\n\nBayesian Network\n\n\nbayes net\n\n\nR\n\n\nbnlearn\n\n\ndag\n\n\n\n\n\n\n\n\n\n\nNov 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Net Pt. 2\n\n\n\nBayesian\n\n\nBayesian Network\n\n\nbayes net\n\n\nR\n\n\nstan\n\n\ncmdstanr\n\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Nets\n\n\n\nbayesian\n\n\nbayesian network\n\n\nbayes net\n\n\nR\n\n\nstan\n\n\ncmdstanr\n\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation in R & Python\n\n\n\ndata manipulation\n\n\ndplyr\n\n\npandas\n\n\nnumpy\n\n\npython\n\n\nR\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProphet Model\n\n\n\nVisualizations\n\n\nAnalysis\n\n\nForecast\n\n\nTidyModels\n\n\nModeltime\n\n\n\n\n\n\n\n\n\n\nJun 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student Satisfaction Exit Surveys\n\n\n\nVisualizations\n\n\nShiny\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday Coffee Ratings\n\n\n\nVisualizations\n\n\nAnalysis\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter Conference Presentation\n\n\n\nVisualizations\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html",
    "href": "posts/2022-06-02-prophet-model/index.html",
    "title": "Prophet Model",
    "section": "",
    "text": "As I start looking for non-academic positions, I wanted to practice forecasting as I didn’t really have much experience with these types of models. NOTE: This is for practicing forecasting skills and you should not trust this model with your own stocks. After plenty of reading,\nI finally have some understanding of how to utilize these models. This post started because even after a BA, 2 masters degrees, and a doctorate, my brother still has no clue what I do. He, along with most of my family think I am a Clinical Psychologist.\nSo for me to try and make my brother understand what I do, I thought I would show him with something that he has become interested with recently; stocks. So for this post, I’ll\nBelow are all the sites for the packages I used.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(prophet)\nlibrary(lubridate)\nlibrary(modeltime)\nlibrary(timetk)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "href": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "title": "Prophet Model",
    "section": "Loading Data",
    "text": "Loading Data\nTo load the Google Finance data, I decided to pick a stock that my brother had, which in this case was JetBlue. A cool feature about Google Finance and Google Sheets is that you can use the following formula in a Google Sheet on the first cell of the first column =GOOGLEFINANCE(\"JBLU\", \"price\", DATE(2000,1,1), DATE(2025, 1, 1), \"DAILY\") and it will give you the date and stock closing values for whatever period you’d like. The example above provides Google financial data for JBLU or the abbreviation for JetBlue stock. It also provides the price of the stock from the first day that there is data on JetBlue stocks, which in this case is April 12th 2002. You can also choose the period of time for the stock prices. I decided to look at daily data.\nJetBlue Sheet\nHere I have a copy of my Google Sheet for JetBlue that I will use to train and test my Prophet model. Instead of having a .csv file on my local machine, I decided to keep this on Google Drive so that it constantly updates with the Google Finance function. This meant that I had to use the googlesheets4 package to load the data from a Google Sheet. I also changed the name and class of the date variable to make it a date variable instead of a date and time variable.\n\ngooglesheets4::gs4_deauth()\n\ntheme_set(theme_light())\n\njet &lt;- \n  googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1SpRXsC3kXDaQLUfC6cPIOvsqxDF6updhgHRJeT8PTog/edit#gid=0\", sheet = 1) %&gt;% \n  janitor::clean_names() %&gt;%\n  mutate(ds = as_date(date))\n\n\nCleaning Up the Data\nBased on some visualizations below, I also decided to create some additional variables from the date variable. Specifically, I used lubridate's wday() function to create a new variable that gives you the actual day from the corresponding cell’s date. I also used the ts_clean_vec function from time_tk to clean for outliers in the stock price values. There are additional arguments for the function, like applying a Box-Cox transformation but that is for a multiplicative trend, which this model does not appear to fit since the variation in the outcome does not grow exponentially. I’ll also include 2002 as the reference year for the year variable and make sure that my data is arranged by date.\n\njetblue &lt;- jet %&gt;% \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) %&gt;% \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %&gt;% \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002')) %&gt;% \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') %&gt;%\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) %&gt;% \n  select(-drop) %&gt;% \n  arrange(ds)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "href": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "title": "Prophet Model",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nStarting with some quick visualizations, we can see that the only area that there is a difference in the variation of the stock prices is in the beginning of 2020. I wonder what that could have been .\n\njetblue %&gt;% \n  group_by(year_num, month_num) %&gt;% \n  summarize(var_value = sd(close)^2) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(month_num, var_value)) + \n  geom_point() + \n  facet_wrap(vars(year_num))\n\n\n\n\n\n\n\n\nNext, we can look at the histograms for the outcome of interest. If we look at the histograms, we can see that there are potential outliers in the original stock prices data. We can also see that cleaning the variable removed the potential outliers.\n\nonly_numeric &lt;- jetblue %&gt;% \n  select(close, clean)\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~ggplot(data = only_numeric,\n             aes(.x)) + \n       geom_histogram(color = 'white',\n                      fill = 'dodgerblue') +\n       geom_vline(xintercept = mean(.x) +\n                    sd(.x) +\n                    sd(.x) +\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       geom_vline(xintercept = mean(.x) -\n                    sd(.x) -\n                    sd(.x) -\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       labs(title = .y))\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nThere will also be a lot of use of the purrr package and the map functions, which are part of the tidyverse. We can also see that in the plot series visualization using modeltime's plot_time_series function, that the cleaned stock prices remove the outliers. So from here on out, I’ll be using the cleaned stock prices.\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~only_numeric %&gt;% \n       plot_time_series(jetblue$ds,\n                        .x,\n                        .interactive = FALSE) + \n       labs(title = .y))\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nWe can also look for anomalies, or points that deviate from the trend. Using the plot_anomaly_diagnostics function from the modeltime package, I can see all the anomalies in the data. I also used ggplot to create my own visualization using the same data. Lastly, we’ll deal with those anomalies by removing them from the dataset. This is not too much of a problem because the Prophet model should be able to handle this fairly easy.\n\njetblue %&gt;% \n  plot_anomaly_diagnostics(ds,\n                           clean,\n                           .facet_ncol = 1,\n                           .interactive = FALSE)\n\n\n\n\n\n\n\njetblue %&gt;% \n  tk_anomaly_diagnostics(ds,\n                         clean) %&gt;% \n  ggplot(aes(ds, observed)) + \n  geom_line() + \n  geom_point(aes(color = anomaly)) +\n  viridis::scale_color_viridis(option = 'D',\n                               discrete = TRUE,\n                               begin = .5,\n                               end = 0)\n\n\n\n\n\n\n\nanomaly &lt;- jetblue %&gt;%\n  tk_anomaly_diagnostics(ds,\n                         clean)\n\njetblue &lt;- left_join(jetblue, anomaly) %&gt;%\n  filter(anomaly != 'Yes')\n\nWe can also look into additional regressors to include in the model by looking into seasonality. We can see some fluctuation in stock prices across the years. We’ll include the year variable as another regressor on the stock prices.\n\njetblue %&gt;% \n  plot_seasonal_diagnostics(ds,\n                            clean,\n                            .interactive = FALSE)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "href": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "title": "Prophet Model",
    "section": "Training the Prophet Model",
    "text": "Training the Prophet Model\nBefore we begin, I’m going to designate 10 cores to process any models run.\n\nset.seed(05262022)\n\nparallel::detectCores()\n\n[1] 12\n\nparallel_start(10,\n               .method = 'parallel')\n\nFirst, instead of the normal initial_split used for training and testing splits, we’ll use the initial_time_split function from tidymodels to separate the first 80% of the data into training set and the other 20% into the testing set.\n\nset.seed(05262022)\njet_split &lt;- initial_time_split(jetblue)\n\n\nProphet Model Function\nI decided to create my own Prophet function to be able to use for both training the model and testing it. In this function, I’ve also included parameters that can be changed to see if the model performs better or worse. Lastly, the train = TRUE allows us to practice with the training dataset and then when we’re happy with the model, we can use it to test our model. For our model, we’ll be predicting stock prices with date and comparing each year to the reference year (2002).\n\nprophet_mod &lt;- function(splits,\n                        changepoints = .05,\n                        seasonality = .01,\n                        holiday = .01,\n                        season_type = 'additive',\n                        day_season = 'auto',\n                        week_season = 'auto',\n                        year_season = 'auto',\n                        train = TRUE){\n  library(tidyverse)\n  library(tidymodels)\n  library(modeltime)\n  library(prophet)\n  \n  analy_data &lt;- analysis(splits)\n  assess_data &lt;- assessment(splits)\n  \n  model &lt;- prophet_reg() %&gt;% \n    set_engine(engine = 'prophet',\n               verbose = TRUE) %&gt;% \n    set_args(prior_scale_changepoints = changepoints,\n             prior_scale_seasonality = seasonality,\n             prior_scale_holidays = holiday,\n             season = season_type,\n             seasonality_daily = day_season,\n             seasonality_weekly = week_season,\n             seasonality_yearly = year_season) %&gt;% \n    fit(clean ~ ds + year_num, \n        data = analy_data)\n  \n  if(train == TRUE){\n    train_cali &lt;- model %&gt;% \n      modeltime_calibrate(new_data = analy_data)\n    \n    train_acc &lt;- train_cali %&gt;% \n      modeltime_accuracy()\n    \n    return(list(train_cali, train_acc))\n  }\n  \n  else{\n    test_cali &lt;- model %&gt;% \n      modeltime_calibrate(new_data = assess_data)\n    \n    test_acc &lt;- test_cali %&gt;% \n      modeltime_accuracy()\n    \n    return(list(test_cali, test_acc))\n  }\n}\n\nIt is worth noting that I’m using the modeltime package to run the prophet model because I believe it is easier to use (especially for later steps) than from Prophet but both can be implemented in this function. Let’s try running this model with the some random parameters I chose from the Prophet website until realizing that the modeltime parameters are log transformed.\n\nset.seed(05262022)\nbaseline &lt;- prophet_mod(jet_split,\n                 train = TRUE) %&gt;% \n  pluck(2)\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nConverting to Modeltime Table.\n\nbaseline\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted 0.895  8.26  4.14  8.09  1.20 0.962\n\n\nSo with the model, we can see that the Mean Absolute Scaled Error (MASE) is 4.1398965 and the Root Mean Square Error (RMSE) is 1.1991467. Not bad for an initial run. Let’s look at how the model fits the training data.\n\nprophet_mod(jet_split,\n                 train = TRUE) %&gt;%  \n  pluck(1) %&gt;% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Prophet Baseline Model')\n\n\n\n\n\n\n\n\nSo the model appears to follow the trend line. We’ll try to tune some of these parameters to see if we can make the model better.\n\n\nTuning the Model\nNow, I’ll tune the prior scale values for the model. I’ll use the grid_latin_hypercube from the dials package in tidymodels to choose 5 sets of parameter values to run. I’m also using the rolling_origin from the rsample package in tidymodels because we are working with time series data. This does not create random samples but instead has samples with data points with consecutive values.\n\nset.seed(05262022)\n\nproph_model &lt;- prophet_reg() %&gt;%\n  set_engine(engine = 'prophet',\n             verbose = TRUE) %&gt;%\n  set_args(prior_scale_changepoints = tune(),\n           prior_scale_seasonality = tune(),\n           prior_scale_holidays = tune(),\n           season = 'additive',\n           seasonality_daily = 'auto',\n           seasonality_weekly = 'auto',\n           seasonality_yearly = 'auto')\n\nproph_rec &lt;-\n  recipe(clean ~ ds + year_num,\n         data = training(jet_split))\n\n\nset.seed(05262022)\ntrain_fold &lt;-\n  rolling_origin(training(jet_split),\n                 initial = 270,  \n                 assess = 90, \n                 skip = 30,\n                 cumulative = TRUE)\n\nset.seed(05262022)\ngrid_values &lt;-\n  grid_latin_hypercube(prior_scale_changepoints(),\n                       prior_scale_seasonality(),\n                       prior_scale_holidays(),\n                       size = 5)\n\nset.seed(05262022)\nproph_fit &lt;- tune_grid(object = proph_model,\n                       preprocessor = proph_rec,\n                       resamples = train_fold,\n                       grid = grid_values,\n                       control = control_grid(verbose = TRUE,\n                                              save_pred = TRUE,\n                                              allow_par = TRUE))\n\n\ntuned_metrics &lt;- collect_metrics(proph_fit)\ntuned_metrics %&gt;%\n  filter(.metric == 'rmse') %&gt;% \n  arrange(mean)\n\n# saveRDS(tuned_metrics,\n#         file = 'tuned_metrics.rds')\n\n\nmetrics &lt;-\n  readr::read_rds(here::here('posts/2022-06-02-prophet-model/tuned_metrics.rds'))\n\nmetrics %&gt;% \n  filter(.metric == 'rmse') %&gt;% \n  arrange(mean)\n\n# A tibble: 5 × 9\n  prior_scale_changepoints prior_scale_seasonality prior_scale_holidays .metric\n                     &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;  \n1                  3.53                    0.0170               1.12    rmse   \n2                  0.884                  36.4                  0.0131  rmse   \n3                  0.00139                 0.00166              0.00172 rmse   \n4                  0.0549                  0.261                0.231   rmse   \n5                 43.0                     3.80                12.2     rmse   \n# ℹ 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;,\n#   .config &lt;chr&gt;\n\n\nFor the sake of not waiting for this to render, I decided to make a RDS file of the metrics gathered from the tuned Prophet model. We can see that the RMSE value was 2.4252669 and the prior scale changepoint value was 3.5347457, the prior scale seasonality value was 0.0170306, and the prior scale holiday value was 1.1198542.\n\n\nFinal Training Model\nI then decided to run the prophet model on the training dataset with the new parameter values.\n\nfinal_train &lt;- prophet_mod(jet_split,\n                 changepoints = 3.53,\n                 seasonality = .017,\n                 holiday = 1.12,\n                 train = TRUE) %&gt;%  \n  pluck(2)\n\nfinal_train\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted 0.800  7.28  3.70  7.16  1.10 0.968\n\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = TRUE) %&gt;%  \n  pluck(1) %&gt;% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Training Model')\n\n\n\n\n\n\n\n\nWe can see that when using the whole training set, we have a RMSE of 1.1013642 and a MASE of 3.7008292 so both metrics reduced slightly."
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "href": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "title": "Prophet Model",
    "section": "Testing the Model",
    "text": "Testing the Model\nFinally, let’s test our Prophet model to see how well the model fits.\n\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %&gt;%\n  pluck(1) %&gt;% \n  modeltime_forecast(new_data = testing(jet_split),\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Testing Model')\n\n\n\n\n\n\n\ntest_model &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %&gt;%\n  pluck(2)\n\ntest_model\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Test   2.17  20.4  8.89  19.1  2.83 0.681\n\n\nWell, that doesn’t look very good and we can see that with the metrics. The MASE has gotten much worse (8.8923832) and so has the RMSE (2.8295592)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "href": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "title": "Prophet Model",
    "section": "Forecasting Ahead a Year",
    "text": "Forecasting Ahead a Year\nWell our model did not fit well to the testing data, but let’s see how it model looks when refit to the full data and forecasted forward a year. So in a year, it seems that JetBlue stock will remain roughly around the same value. It is important to note that the confidence intervals are large and with 95% confidence that values could be between 52.49 and -28.39 (not possible), there is not much confidence that JetBlue stock prices will remain where they are now in a year.\n\nfuture &lt;- jetblue %&gt;% \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture &lt;-\n  future %&gt;%\n  select(-year_num, -month_num, -day_num) %&gt;%\n  mutate(date2 = ds) %&gt;%\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %&gt;%\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num)) %&gt;% \n  arrange(ds)\n\nglimpse(future)\n\nRows: 5,922\nColumns: 17\n$ close         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ ds            &lt;date&gt; 2002-04-12, 2002-04-15, 2002-04-16, 2002-04-17, 2002-04…\n$ actual_day    &lt;ord&gt; Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, M…\n$ clean         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ observed      &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ season        &lt;dbl&gt; -0.0040803648, -0.0002458832, -0.0051024208, 0.000334500…\n$ trend         &lt;dbl&gt; 13.40485, 13.41437, 13.42388, 13.43339, 13.44290, 13.452…\n$ remainder     &lt;dbl&gt; -0.0707726153, -0.0141198535, 0.1512239275, -0.073725750…\n$ seasadj       &lt;dbl&gt; 13.33408, 13.40025, 13.57510, 13.35967, 13.09091, 12.934…\n$ remainder_l1  &lt;dbl&gt; -2.189152, -2.189152, -2.189152, -2.189152, -2.189152, -…\n$ remainder_l2  &lt;dbl&gt; 2.205557, 2.205557, 2.205557, 2.205557, 2.205557, 2.2055…\n$ anomaly       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ recomposed_l1 &lt;dbl&gt; 11.21162, 11.22497, 11.22962, 11.24457, 11.26285, 11.259…\n$ recomposed_l2 &lt;dbl&gt; 15.60633, 15.61968, 15.62433, 15.63928, 15.65756, 15.653…\n$ year_num      &lt;fct&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 20…\n$ month_num     &lt;fct&gt; 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 05, …\n$ day_num       &lt;dbl&gt; 12, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 29, 30, 1, 2…\n\ntest_model1 &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %&gt;%\n  pluck(1)\n\ntest_model1 %&gt;% \n  modeltime_refit(data = future) %&gt;% \n  modeltime_forecast(new_data = future,\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Forecasted JetBlue Stock Prices')"
  },
  {
    "objectID": "posts/2024-03-09-bayesian-networks-pt-1/index.html",
    "href": "posts/2024-03-09-bayesian-networks-pt-1/index.html",
    "title": "Bayes Nets Pt. 1",
    "section": "",
    "text": "Under Development\nAs I am continuing to grow in understanding and conducting bayesian networks, this page and series may change in the future. -JP\nOkay, I will be the first to state that I am not an expert in the field of conducint bayeaian networks, bayesian analyses, statistics (the list goes on), but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses the programming language Stan. There are several tutorials on how to download Stan using either R or Python, so I will not be covering that. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using Python and Stan. Additionally, I will be creating data that will represent educational assessment data, with latent variables representing proficiency in certain skills (e.g., math, English/language arts, and science) for students. While most of my experience of using bayes nets is to represent measurement models, bayes net can be used outside of this field. Bayes net is similar to path analysis and structural equation modeling; however, EXPLAIN DIFFERENCE BETWEEN THE TWO METHODS. I will also start referring to everything in this series in a bayesian network framework. For instance, instead of using variables, whether they are observed or unobserved (latent), I will be referring to them as nodes and latent nodes, respectively. When it comes to showing the “paths” between nodes, I wwill now be referring to them as edges. Lastly, any image that shows all of the nodes and the edges connecting to one another will be referred to as a directed acyclic graph or DAG.\nOkay, now on to this post. For this post I will simply discuss creating the data in R to be used in Stan, as well as creating the object of data that will be used in the Stan calculations. One last comment before diving in, I will be using cmdstanr instead of rstan for my Stan computations."
  },
  {
    "objectID": "posts/2024-03-09-bayesian-networks-pt-1/index.html#getting-the-data-set-up",
    "href": "posts/2024-03-09-bayesian-networks-pt-1/index.html#getting-the-data-set-up",
    "title": "Bayes Nets Pt. 1",
    "section": "Getting the Data Set Up",
    "text": "Getting the Data Set Up\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(bayestestR)\nlibrary(bayesplot)\nlibrary(posterior)\n\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 1000, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .8),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nThe first thing I am going to do is load in all the necessary libraries that you need. Then I decided to create a function that would create a binomial distribution with a single trial, so essentially a bernoulii distribution. I decided on some random numbers for the probabilities of correct responses to the 15 different items and decided to create some fake studentids for each row.\n\n\n\n\n\n\n\n\n\nI decided to create a simple table that shows all of the students and their responses for the 15 items in this assessment. I’m not sure why I have all the data in the table, but I used some pagination so there is not a laundry list of rows with 0s and 1s clogging up this post…hopefully.\n\n# map(y |&gt; select(-studentid), table)\n# map(y |&gt; select(-studentid), ~round(prop.table(table(.x)), 2))\n\nmap(y |&gt; select(-studentid), table)[[1]]\n\n\n  0   1 \n199 801 \n\n\nAfter seeing that the data looks correct, I am also neurotic and need to make sure that my created data is how I imagined it would be. So I looped through each of my items to make sure the proportions are correct. More importantly, I like to see the counts of the data and get an understanding of how many are answering each item correctly. I commented out the loop and am only going to show the counts for the first item. So seeing at how my function had approximately 80% of the students answering the item correctly, I can now see that 801 answered item 1 correctly.\n\nQ Matrix\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  gt::gt() |&gt;\n  gt::opt_interactive(\n    use_filters = TRUE,\n    use_resizers = TRUE,\n    use_highlight = TRUE,\n    use_compact_mode = TRUE\n  )\n\n\n\n\n\n\n\n\nOkay, now on to the Q-matrix. This is the only other piece of information we may need for our model in Stan. WARNING I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The gt table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column. So now I believe we have everything we need to get started on a bayes net using Stan and Markov chain Monte Carlo (MCMC) sampling.\n\n\nStan Data\n\nstan_data &lt;- list(\n  J = nrow(y[, -1]), # Number of students/rows\n  I = ncol(y[, -1]), # Number of items\n  K = ncol(q_matrix[, -1]), #Number of latent attributes/skills\n  y = y[,-1], # Student responses on all items\n  Q = q_matrix[,-1] # Items that measure each attribute\n)\n\nprint(stan_data)\nglimpse(stan_data)"
  },
  {
    "objectID": "posts/2024-03-16-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-03-16-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Under Development - Not Complete\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ndata { int&lt;lower=1&gt; J; // number of examinees int&lt;lower=1&gt; I; // number of items int&lt;lower=1&gt; K; // number of latent variables int&lt;lower=1&gt; C; // number of classes matrix[J, I] X; // response matrix x matrix[I, K] Q; // Q matrix Q matrix[C, K] alpha; // attribute profile matrix } parameters { simplex[C] nu; // class probabilities vector&lt;lower=0, upper=1&gt;[I] false_pos; vector&lt;lower=0, upper=1&gt;[I] true_pos; real&lt;lower=0, upper=1&gt; lambda1; real&lt;lower=0, upper=1&gt; lambda20; real&lt;lower=0, upper=1&gt; lambda21; real&lt;lower=0, upper=1&gt; lambda30; real&lt;lower=0, upper=1&gt; lambda31; real&lt;lower=0, upper=1&gt; lambda40; real&lt;lower=0, upper=1&gt; lambda41; real&lt;lower=0, upper=1&gt; lambda50; real&lt;lower=0, upper=1&gt; lambda51; } transformed parameters { vector[C] log_nu; log_nu = log(nu); } model { vector[2] theta_log1; vector[2] theta_log2; vector[2] theta_log3; vector[2] theta_log4; vector[2] theta_log5; vector[C] theta1; vector[C] theta2; vector[C] theta3; vector[C] theta4; vector[C] theta5; matrix[I, C] delta; real pie; vector[I] log_item; vector[C] log_lik;\n// Priors lambda1 ~ beta(25, 5); lambda20 ~ beta(10, 20); lambda21 ~ beta(20, 10); lambda30 ~ beta(5, 25); lambda31 ~ beta(25, 5); lambda40 ~ beta(5, 25); lambda41 ~ beta(25, 5); lambda50 ~ beta(12, 18); lambda51 ~ beta(18, 12);\nfor (i in 1 : I) { false_pos[i] ~ beta(4, 26); true_pos[i] ~ beta(26, 4); }\ntheta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1); theta_log1[2] = bernoulli_lpmf(1 | lambda1);\ntheta_log2[1] = bernoulli_lpmf(1 | lambda20); theta_log2[2] = bernoulli_lpmf(1 | lambda21);\ntheta_log3[1] = bernoulli_lpmf(1 | lambda30); theta_log3[2] = bernoulli_lpmf(1 | lambda31);\ntheta_log4[1] = bernoulli_lpmf(1 | lambda40); theta_log4[2] = bernoulli_lpmf(1 | lambda41);\ntheta_log5[1] = bernoulli_lpmf(1 | lambda50); theta_log5[2] = bernoulli_lpmf(1 | lambda51);\nfor (c in 1 : C) { if (alpha[c, 1] &gt; 0) { theta1[c] = theta_log1[2]; } else { theta1[c] = theta_log1[1]; } if (alpha[c, 2] &gt; 0) { theta2[c] = theta_log2[2]; } else { theta2[c] = theta_log2[1]; } if (alpha[c, 3] &gt; 0) { theta3[c] = theta_log3[2]; } else { theta3[c] = theta_log3[1]; } if (alpha[c, 4] &gt; 0) { theta4[c] = theta_log4[2]; } else { theta4[c] = theta_log4[1]; } if (alpha[c, 5] &gt; 0) { theta5[c] = theta_log5[2]; } else { theta5[c] = theta_log5[1]; } }\n//Likelihood for (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2]) * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4]) * pow(exp(theta5[c]), Q[i, 5]);\n    pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n    log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n  }\n  log_lik[c] = log_nu[c] + sum(log_item);\n}\ntarget += log_sum_exp(log_lik);\n} } generated quantities { vector[2] theta_log1; vector[2] theta_log2; vector[2] theta_log3; vector[2] theta_log4; vector[2] theta_log5; vector[C] theta1; vector[C] theta2; vector[C] theta3; vector[C] theta4; vector[C] theta5; matrix[I, C] delta; real pie; vector[I] log_item;\nmatrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k row_vector[C] prob_joint; vector[C] prob_attr_class;\nmatrix[J, I] x_rep;\ntheta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1); theta_log1[2] = bernoulli_lpmf(1 | lambda1);\ntheta_log2[1] = bernoulli_lpmf(1 | lambda20); theta_log2[2] = bernoulli_lpmf(1 | lambda21);\ntheta_log3[1] = bernoulli_lpmf(1 | lambda30); theta_log3[2] = bernoulli_lpmf(1 | lambda31);\ntheta_log4[1] = bernoulli_lpmf(1 | lambda40); theta_log4[2] = bernoulli_lpmf(1 | lambda41);\ntheta_log5[1] = bernoulli_lpmf(1 | lambda50); theta_log5[2] = bernoulli_lpmf(1 | lambda51);\nfor (c in 1 : C) { if (alpha[c, 1] &gt; 0) { theta1[c] = theta_log1[2]; } else { theta1[c] = theta_log1[1]; } if (alpha[c, 2] &gt; 0) { theta2[c] = theta_log2[2]; } else { theta2[c] = theta_log2[1]; } if (alpha[c, 3] &gt; 0) { theta3[c] = theta_log3[2]; } else { theta3[c] = theta_log3[1]; } if (alpha[c, 4] &gt; 0) { theta4[c] = theta_log4[2]; } else { theta4[c] = theta_log4[1]; } if (alpha[c, 5] &gt; 0) { theta5[c] = theta_log5[2]; } else { theta5[c] = theta_log5[1]; } }\nfor (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2]) * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4]) * pow(exp(theta5[c]), Q[i, 5]);\n    pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n    log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n    }\n  prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n}\nprob_resp_class[j] = prob_joint / sum(prob_joint);\n}\nfor (j in 1 : J) { for (k in 1 : K) { for (c in 1 : C) { // Calculate the probability of mastering attribute k given class c prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k]; } // Sum the probabilities to get the posterior probability of mastering attribute k prob_resp_attr[j, k] = sum(prob_attr_class); } }\nfor (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie); } } } }"
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html",
    "title": "Using Typst To Create Documents",
    "section": "",
    "text": "Under Development - Not Complete\nI have been using Typst, an awesome app for working on pdf files at the same time as colleagues. You could think of this as something similar to Google Docs or GitHub for code. This also got me thinking about creating a small series of blog posts about using Typst and then creating Typst templates for documents using Quarto. The latter topics would be using Quarto extensions and if following along, you would need Quarto version 1.4 at least to be able to use Typst code chunks on a Quarto document. So first, I will show the Typst file I will be using because let’s face it I’m on the job market and free publicity is always good.\n.\nHere is the link for the Typst resume to view. If you want, you can just copy and paste that into Typst and change the information. I will walk through each section of the document with Typst code in the post, as well as a cover letter post, and then end the series with a Quarto extension to create a Typst template so you can just write your resume and/or cover letter in Quarto."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#typst-documentation",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#typst-documentation",
    "title": "Using Typst To Create Documents",
    "section": "Typst Documentation",
    "text": "Typst Documentation\nI will be the first to state that Typst documentation is a little difficult to follow at first. Hopefully with this tutorial you will get a better understanding of the basics of Typst code. As someone who tried to learn LaTeX to edit the previous resume I had found a template for, I wish Typst existed earlier."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "title": "Using Typst To Create Documents",
    "section": "Creating a Typst Document",
    "text": "Creating a Typst Document\nWorking in the Typst app is pretty straightforward with you creating a username, followed by your dashboard with nothing there. This will be the location of all of your documents as you get started with Typst. While there are Typst templates already for resumes I really wanted to create something similar to the resume I had in LaTeX. Working in Quarto, you will have to learn how to create Typst code chunks. They are slightly different from other languages’ code chunks but you can still use all of the Quarto code chunk arguments.\nFrom what I have seen online, there does not seem to be much difference in the ordering of some of the beginning Typst documentation. I have decided to start my Typst document with any variables I will be including, followed by any Typst packages I will need, and then setting up the general parameters for the document. These general parameters are for the document overall. Things like setting the font to a specific font, size and maybe weight would be a good parameter to set at the top of your document."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "title": "Using Typst To Create Documents",
    "section": "Breakdown of Variables, Packages, and # Set function for parameters",
    "text": "Breakdown of Variables, Packages, and # Set function for parameters\n\n//Variables\n#let name = [Jonathan A. Pedroza Ph.D]\n\n//Packages\n#import \"@preview/tablex:0.0.8\": tablex\n#import \"lib-gen.typ\": *\n#import \"lib-impl.typ\": *\n#import \"lib.typ\": *\n\n#set page(\n  margin: (\n    top: 0cm,\n    bottom: 0cm,\n    left: .5cm,\n    right: 0cm\n  )\n)\n\n#set block(spacing: 0.5em)\n\n#set rect(\n  width: 37%,\n  height: 100%\n)\n\nAbove is the beginning Typst code for the resume I created. I’m going to walk through the code a little, but for more detailed information, check out the help documentation. The // syntax can be included wherever to include comments. Since I have been showing others how to use Typst to create quick pretty PDFs, I have been including a lot of comments for things like variables. To create variables in Typst, you will need to use the #let function followed by your variable name, an equal sign, and the information you want to include. So I created the variable name, which would be used as #name in Typst and the document will spell out my full name. After that, just as the comment states, I included the tablex package, which I have found to be useful for creating tables and grids. If you’d like you can use the #table or #grid functions from Typst. Additionally, I also included the files for using the FontAwesome Typst package. You can find all the icons and other information about FontAwesome at the FontAwesome website. I was just being lazy as I wanted to create my resume quick so I could get it out into the world ASAP. Next I set the margins to maximize the amount of space I would have for my resume and I created a block after the titles Education and Professional Experience since I did not want the default amount of space before my education and experience entries. The #set function creates rules for the document as a whole. So For the whole document I have the same margins throughout, a block to create more space between headers and text underneath and the rectangle to separate the sections of the document. Lastly, I set a rectangle for 37% of the document’s width and 100% of the height. There are other metrics that can be used to create the rectangle but I personally was enjoying using percentages for this document. This rectangle is for the right side of the document that includes the contact information."
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html",
    "title": "Data Manipulation in R & Python",
    "section": "",
    "text": "One of my favorite posts is the comparison between data.table and the tidyverse’s dplyr packages. Here is the link to that post. I have used that when trying to build my competence in using data.table. Now I’m going to try and expand on that by creating this post that compares cases of using dplyr, data.table, and now pandas. Hopefully this can be as useful as the comparison between dplyr an data.table post was for me. This is not an extensive way of comparing them but just to get started for anyone that wants to use python more."
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-integers",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-integers",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Integers)",
    "text": "Filtering (Integers)\n\nr_data |&gt;\n  filter(\n    x &gt; 1\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 3\n      x     x2     y\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.09 -0.457     1\n2  1.71  2.05      0\n3  1.15 -3.56      1\n4  1.37  3.76      1\n5  1.30  3.83      1\n6  3.01  2.09      0\n\n\n\nhead(\n  r_table[x &gt; 1]\n)\n\n          x         x2     y\n      &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1: 1.144979 -5.2480202     1\n2: 2.284941  5.9637749     0\n3: 1.196674 -0.6543321     1\n4: 1.890398 -1.8352037     0\n5: 1.850590 -1.2532883     1\n6: 2.428723  0.2854589     0\n\n\n\npy_data[py_data[\"x\"] &gt; 1].head()\n\n           x        x2  y\n0   1.625495 -2.513493  1\n1   1.375477  1.419490  1\n3   1.279369  3.155540  0\n5   1.212755  2.003532  0\n11  1.412002 -2.758251  1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-categorical",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-categorical",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Categorical)",
    "text": "Filtering (Categorical)\n\nr_data |&gt;\n  filter(\n    y == 1\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 3\n       x     x2     y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.09  -0.457     1\n2 -0.363 -1.66      1\n3  1.15  -3.56      1\n4  0.674  0.632     1\n5  1.37   3.76      1\n6 -1.82  -0.946     1\n\n\n\nhead(\n  r_table[y == 1]\n)\n\n            x         x2     y\n        &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1:  0.3012969 -0.1892336     1\n2:  1.1449794 -5.2480202     1\n3:  0.6039375  3.4301031     1\n4: -2.8776286 -5.6320773     1\n5:  1.1966740 -0.6543321     1\n6:  1.8505904 -1.2532883     1\n\n\n\npy_data[py_data[\"y\"] == 1].head()\n\n          x        x2  y\n0  1.625495 -2.513493  1\n1  1.375477  1.419490  1\n4 -0.992507  2.701708  1\n8 -0.284489 -0.317201  1\n9 -0.110960 -0.468932  1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering Multiple Columns",
    "text": "Filtering Multiple Columns\n\nr_data |&gt;\n  filter(\n    y == 1 &\n    x2 &lt; 0\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 3\n       x     x2     y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.09  -0.457     1\n2 -0.363 -1.66      1\n3  1.15  -3.56      1\n4 -1.82  -0.946     1\n5  0.446 -1.72      1\n6 -0.369 -6.86      1\n\n\n\nhead(\n  r_table[\n    y == 1 &\n    x2 &gt; 0\n  ]\n)\n\n            x        x2     y\n        &lt;num&gt;     &lt;num&gt; &lt;int&gt;\n1:  0.6039375 3.4301031     1\n2: -0.6132135 2.1888315     1\n3: -2.4182992 1.5879010     1\n4: -2.0788204 0.6814116     1\n5:  0.2546547 0.1272056     1\n6:  2.0922497 4.6317957     1\n\n\n\npy_data[\n  (py_data[\"y\"] == 1) & \n  (py_data[\"x2\"] &gt; 0)\n    ].head()\n\n           x        x2  y\n1   1.375477  1.419490  1\n4  -0.992507  2.701708  1\n10 -0.404824  0.780289  1\n14  0.237784  2.241245  1\n23  0.490488  0.160291  1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#sorting-rows",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#sorting-rows",
    "title": "Data Manipulation in R & Python",
    "section": "Sorting Rows",
    "text": "Sorting Rows\n\nr_data |&gt; \n  arrange(y) |&gt;\n  head()\n\n# A tibble: 6 × 3\n       x     x2     y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.71   2.05      0\n2 -0.651  0.850     0\n3 -1.61   2.95      0\n4  0.196 -4.13      0\n5  3.01   2.09      0\n6  0.228  0.598     0\n\n\n\nhead(\n  r_table[order(y)]\n)\n\n            x         x2     y\n        &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1:  0.6129881  0.5847694     0\n2:  2.2849412  5.9637749     0\n3: -1.0921220  7.5136606     0\n4:  0.7583684 -1.8330110     0\n5:  1.8903981 -1.8352037     0\n6: -0.9942675  0.1554529     0\n\n\n\npy_data.sort_values(by = \"y\").head()\n\n            x        x2  y\n499  0.938749  0.135750  0\n546 -0.424556 -2.931917  0\n544  0.147953 -2.487124  0\n542 -1.651471  0.683126  0\n541 -0.263890  1.406574  0"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-specific-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-specific-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Specific Columns",
    "text": "Selecting Specific Columns\n\nr_data |&gt;\n  select(\n    y\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 1\n      y\n  &lt;int&gt;\n1     1\n2     0\n3     1\n4     1\n5     1\n6     0\n\n\n\nhead(\n  r_table[,\"y\"]\n)\n\n       y\n   &lt;int&gt;\n1:     1\n2:     1\n3:     0\n4:     1\n5:     1\n6:     0\n\n\n\npy_data[\"y\"].head()\n\n0    1\n1    1\n2    0\n3    0\n4    1\nName: y, dtype: int32\n\n\n# py_data.filter(items = \"y\").head()"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Multiple Columns",
    "text": "Selecting Multiple Columns\n\nr_data |&gt; \n  select(x, x2) |&gt; \n  head()\n\n# A tibble: 6 × 2\n       x     x2\n   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.09  -0.457\n2  1.71   2.05 \n3 -0.363 -1.66 \n4  1.15  -3.56 \n5  0.674  0.632\n6 -0.651  0.850\n\n\n\nhead(\n  r_table[,list(x, x2)]\n)\n\n            x         x2\n        &lt;num&gt;      &lt;num&gt;\n1:  0.3012969 -0.1892336\n2:  1.1449794 -5.2480202\n3:  0.6129881  0.5847694\n4:  0.6039375  3.4301031\n5: -2.8776286 -5.6320773\n6:  2.2849412  5.9637749\n\n\n\n# py_data[{\"x\", \"x2\"}].head()\n\npy_data.filter(items = [\"x\", \"x2\"]).head()\n\n          x        x2\n0  1.625495 -2.513493\n1  1.375477  1.419490\n2  0.896304  3.582105\n3  1.279369  3.155540\n4 -0.992507  2.701708"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-using-regex",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-using-regex",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Using Regex",
    "text": "Selecting Using Regex\n\nr_data |&gt;\n  select(\n    matches(\"x\")\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 2\n       x     x2\n   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.09  -0.457\n2  1.71   2.05 \n3 -0.363 -1.66 \n4  1.15  -3.56 \n5  0.674  0.632\n6 -0.651  0.850\n\n\n\ncols &lt;- grep(\"^x\", names(r_table))\n\nhead(\n  r_table[, ..cols]\n)\n\n            x         x2\n        &lt;num&gt;      &lt;num&gt;\n1:  0.3012969 -0.1892336\n2:  1.1449794 -5.2480202\n3:  0.6129881  0.5847694\n4:  0.6039375  3.4301031\n5: -2.8776286 -5.6320773\n6:  2.2849412  5.9637749\n\n\n\npy_data.filter(regex = \"x\").head()\n\n          x        x2\n0  1.625495 -2.513493\n1  1.375477  1.419490\n2  0.896304  3.582105\n3  1.279369  3.155540\n4 -0.992507  2.701708"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#summarize-data",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#summarize-data",
    "title": "Data Manipulation in R & Python",
    "section": "Summarize Data",
    "text": "Summarize Data\n\nr_data |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n# A tibble: 1 × 1\n     avg\n   &lt;dbl&gt;\n1 0.0279\n\n  r_data |&gt;\n  summarize(\n    total = sum(x)\n  )\n\n# A tibble: 1 × 1\n  total\n  &lt;dbl&gt;\n1  27.9\n\n\n\nr_table[, .(avg = mean(x))]\n\n           avg\n         &lt;num&gt;\n1: -0.01425028\n\nr_table[, .(total = sum(x))]\n\n       total\n       &lt;num&gt;\n1: -14.25028\n\n\n\npy_data[\"x\"].mean()\n\n0.002775922653748875\n\npy_data[\"x\"].sum()\n\n2.775922653748875"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Adding/Updating/Deleting Columns",
    "text": "Adding/Updating/Deleting Columns\n\nr_data &lt;- r_data |&gt;\n  mutate(\n    x_mult = x*x2\n  )\nhead(r_data)\n\n# A tibble: 6 × 4\n       x     x2     y x_mult\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1  1.09  -0.457     1 -0.500\n2  1.71   2.05      0  3.49 \n3 -0.363 -1.66      1  0.604\n4  1.15  -3.56      1 -4.10 \n5  0.674  0.632     1  0.426\n6 -0.651  0.850     0 -0.554\n\n\n\nr_table[, x_mult := x*x2]\nhead(r_table[, \"x_mult\"])\n\n        x_mult\n         &lt;num&gt;\n1: -0.05701551\n2: -6.00887504\n3:  0.35845670\n4:  2.07156775\n5: 16.20702631\n6: 13.62687485\n\n\n\npy_data[\"x_mult\"] = py_data[\"x\"] * py_data[\"x2\"]\npy_data[\"x_mult\"].head()\n\n0   -4.085671\n1    1.952477\n2    3.210656\n3    4.037102\n4   -2.681463\nName: x_mult, dtype: float64"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#counting",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#counting",
    "title": "Data Manipulation in R & Python",
    "section": "Counting",
    "text": "Counting\n\nr_data |&gt; count(y)\n\n# A tibble: 2 × 2\n      y     n\n  &lt;int&gt; &lt;int&gt;\n1     0   383\n2     1   617\n\n\n\nr_table[, .N, by = (y)]\n\n       y     N\n   &lt;int&gt; &lt;int&gt;\n1:     1   619\n2:     0   381\n\n\n\npy_data[\"y\"].value_counts()\n\ny\n1    590\n0    410\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#group-by",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#group-by",
    "title": "Data Manipulation in R & Python",
    "section": "Group By",
    "text": "Group By\n\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n# A tibble: 2 × 2\n      y     avg\n  &lt;int&gt;   &lt;dbl&gt;\n1     0 0.0599 \n2     1 0.00810\n\n\n\nr_table[, .(avg = mean(x)), by = \"y\"]\n\n       y          avg\n   &lt;int&gt;        &lt;num&gt;\n1:     1 -0.009975402\n2:     0 -0.021195561\n\n\n\npy_data.groupby(\"y\")[[\"x\"]].mean()\n\n          x\ny          \n0  0.041513\n1 -0.024143"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#chain-expressions",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#chain-expressions",
    "title": "Data Manipulation in R & Python",
    "section": "Chain Expressions",
    "text": "Chain Expressions\n\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  ) |&gt;\n  filter(\n    y == 1\n  )\n\n# A tibble: 1 × 2\n      y     avg\n  &lt;int&gt;   &lt;dbl&gt;\n1     1 0.00810\n\n\n\nr_table[, \n  by = y,\n  .(avg = mean(x))\n  ][\n    y == 1\n  ]\n\n       y          avg\n   &lt;int&gt;        &lt;num&gt;\n1:     1 -0.009975402\n\n\n\npy_group = py_data.groupby(\"y\")[[\"x\"]].mean()\n\npy_group.loc[1]\n\nx   -0.024143\nName: 1, dtype: float64"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#pivot-data",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#pivot-data",
    "title": "Data Manipulation in R & Python",
    "section": "Pivot Data",
    "text": "Pivot Data\n\nr_data |&gt;\n  pivot_longer(\n    -y\n  )\n\n# A tibble: 3,000 × 3\n       y name    value\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1     1 x       1.09 \n 2     1 x2     -0.457\n 3     1 x_mult -0.500\n 4     0 x       1.71 \n 5     0 x2      2.05 \n 6     0 x_mult  3.49 \n 7     1 x      -0.363\n 8     1 x2     -1.66 \n 9     1 x_mult  0.604\n10     1 x       1.15 \n# ℹ 2,990 more rows\n\n\n\nmelt(r_table, id.vars = \"y\")\n\n          y variable      value\n      &lt;int&gt;   &lt;fctr&gt;      &lt;num&gt;\n   1:     1        x  0.3012969\n   2:     1        x  1.1449794\n   3:     0        x  0.6129881\n   4:     1        x  0.6039375\n   5:     1        x -2.8776286\n  ---                          \n2996:     0   x_mult -2.3109308\n2997:     0   x_mult -4.0893226\n2998:     0   x_mult  0.3624624\n2999:     0   x_mult  2.1338842\n3000:     0   x_mult -4.9910036\n\n\n\npy_data[\"id\"] = py_data.index\n\npy_data.head()\n\n          x        x2  y    x_mult  id\n0  1.625495 -2.513493  1 -4.085671   0\n1  1.375477  1.419490  1  1.952477   1\n2  0.896304  3.582105  0  3.210656   2\n3  1.279369  3.155540  0  4.037102   3\n4 -0.992507  2.701708  1 -2.681463   4\n\npy_pivot = py_data.pivot(index='id', columns='y', values=['x', 'x2', 'x_mult'])\n\nprint(py_pivot.head())\n\n           x                  x2              x_mult          \ny          0         1         0         1         0         1\nid                                                            \n0        NaN  1.625495       NaN -2.513493       NaN -4.085671\n1        NaN  1.375477       NaN  1.419490       NaN  1.952477\n2   0.896304       NaN  3.582105       NaN  3.210656       NaN\n3   1.279369       NaN  3.155540       NaN  4.037102       NaN\n4        NaN -0.992507       NaN  2.701708       NaN -2.681463"
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#grid-of-entries",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#grid-of-entries",
    "title": "Using Typst To Create Documents",
    "section": "Grid of Entries",
    "text": "Grid of Entries\n\n#grid(\n  columns: (70%, 82%),\n  [\n    #linebreak()\n\n    #set text(\n      font: \"Source Sans Pro\",\n      size: 10pt\n    )\n    #set align(center)\n    \n    = Education #fa-graduation-cap()\n    #line(\n      length: 94%,\n      stroke: black\n    )\n\nI decided to separate some of these functions because it might be easier to talk about. So the grid here is actually separating the main text and the contact information text. As I am reading this, I can see that I probably should have set the font to be 10pt throughout the document rather than within the grid."
  },
  {
    "objectID": "posts/2024-07-09-optimal-threshold/index.html",
    "href": "posts/2024-07-09-optimal-threshold/index.html",
    "title": "Finding Optimal Thresholds",
    "section": "",
    "text": "Under Development - Not Complete\nNow that I’m playing catch up with some posts I have wanted to write, I thought now would be an excellent time to write about this method I have been trying out to figure out the closest optimal threshold. While I found other ways to find the optimal threshold at a much faster rate, I still thought this was an interesting use of machine learning to try and figure out the optimal threshold. Particularly, this is a method to try and find an optimal threshold when the truth is not known. From what I could find there was not much literature on trying to find an optimal threshold when the truth was not known. So I’ll first show this method when the truth is known followed by trying this method out when there is no truth."
  },
  {
    "objectID": "posts/2024-07-09-optimal-threshold/index.html#fabricating-some-data",
    "href": "posts/2024-07-09-optimal-threshold/index.html#fabricating-some-data",
    "title": "Finding Optimal Thresholds",
    "section": "Fabricating Some Data",
    "text": "Fabricating Some Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071)\ncat_map &lt;- purrr::map\n\nn &lt;- 500\nseed &lt;- 12345\ntruth &lt;- rbinom(n = n, size = 1, prob = .6)\nestimates &lt;- bayestestR::distribution_beta(n = n, shape1 = 12, shape2 = 18)\n\ndata &lt;- tibble(\n  estimates = estimates,\n  truth = truth\n)\n\ndata |&gt; head()\n\n# A tibble: 6 × 2\n  estimates truth\n      &lt;dbl&gt; &lt;int&gt;\n1     0.160     0\n2     0.181     0\n3     0.192     0\n4     0.200     1\n5     0.206     1\n6     0.212     1\n\ndata_train &lt;- data |&gt; slice_sample(prop = .75)\ndata_test &lt;- anti_join(data, data_train)\n\nJoining with `by = join_by(estimates, truth)`\n\n\n\nset.seed(seed)\nclass_weights &lt;- data_train |&gt; count(truth) |&gt; arrange(n)\ncls_weights &lt;- class_weights[2, 2]/class_weights[1, 2]\n\nsvm_mod &lt;- svm(\n  truth ~ estimates,\n  data = data_train,\n  type = \"C-classification\",\n  kernel = \"radial\",\n  cost = 10,\n  class.weights = c(\"0\" = as.numeric(cls_weights), \"1\" = 1),\n  probability = TRUE\n)\nsummary(svm_mod)\n\n\nCall:\nsvm(formula = truth ~ estimates, data = data_train, type = \"C-classification\", \n    kernel = \"radial\", cost = 10, class.weights = c(`0` = as.numeric(cls_weights), \n        `1` = 1), probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  10 \n\nNumber of Support Vectors:  354\n\n ( 198 156 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\nsvm_pred &lt;- predict(svm_mod, data_test)\nsvm_pred |&gt; as_tibble() |&gt; count(value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;fct&gt; &lt;int&gt;\n1 0        69\n2 1        56\n\n\n\nmap_dbl(\n  data$estimates,\n  ~rbinom(n = 1, size = 1, prob = .x)\n)\n\n  [1] 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n [38] 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1\n [75] 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n[112] 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1\n[149] 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0\n[186] 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n[223] 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n[260] 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1\n[297] 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n[334] 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n[371] 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n[408] 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n[445] 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1\n[482] 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0\n\n\n\nprobably_threshold &lt;- function(\n  r6,\n  task,\n  threshold,\n  output = TRUE\n){\n  thresholds &lt;- c(threhold, 1 - threshold)\n  names(thresholds) &lt;- {{task}}$class_names\n\n  if(output == TRUE){\n    confuse &lt;- {{r6}}$clone(deep = TRUE)$set_threshold(threhsolds)$confusion |&gt; t()\n    score &lt;- {{r6}}$clone(deep = TRUE)$set_threshold$score(msrs(c(\"classif.mcc\", \"classif.acc\", \"classif.auc\", \"classif.ce\", \"classif.sensitivity\", \"classif.specificity\")))\n\n    list(confuse, score)\n  }\n  else{\n    preds &lt;- {{r6}}$clone(deep = TRUE)$set_threshold(thresholds)\n\n    preds\n  }\n}"
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html",
    "title": "Bayes Nets",
    "section": "",
    "text": "As I am continuing to grow in understanding and conducting bayesian networks, this page and series may change in the future. -JP\nOkay, I will be the first to state that I am not an expert in the field of conducting psychometric models, Bayesian networks, Bayesian analyses, but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses the programming language Stan. The purpose of this post is to walk through Stan and some bayes net terminology to get a basic understanding of some psychometric models conducting using Bayesian inference.\nTo get started, make sure you follow the detailed instructions on installing RStan. I know if using Mac, make sure to also download Xcode so that Stan will work correctly. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using PyStan or Cmdstanpy but there are just more readily available tools using R so I will be using R instead. Additionally, I will be creating dichotomous data that will represent an education assessment where a 1 indicates that a student has answered the item correctly and a 0 indicates they did not answer the item correctly. The model will also include three latent attributes/skills/variables where a 1 would indicate that the student has mastered the skill and a 0 would indicate that they do not have mastery of the skill.\nWhile I will be discussing bayes net through an educational measurement lens, bayes net can be used outside of education to show that individuals have skills that are not directly measured. Instead of items on an assessment, tasks that capture each skill can be assessed. Before walking through some bayes net terminology, it is important to note that this model is simply for educational purposes. Components of the psychometric models I will be writing about (e.g., Diagnostic Classification Model (DCMs) and bayes net) require expert opinion. For example, DCMs and bayes net models require expert opinions on the assignment of items to skills. Additionally, bayes net models require expert opinion on the priors for the lambda (\\(\\lambda\\)) parameters.\nSince there is different opinions on using different terms, I am going to stick to the following terms.\nFor this introductory post into bayes net, I thought it would be best to create some artificial data and show visually the models I will be planning on creating using R and Stan. I will be using cmdstanr instead of rstan for my Stan computations. The main difference between the two packages is that rstan avoids using R6 classes, while cmdstanr uses R6 classes. If you’d like more information on trade-offs of different object-oriented programming classes, you can read more here. Finally, I will state that while this is introductory to a bayes net model, this post assumes that you have a decent understanding of Bayesian inference."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#q-matrix",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#q-matrix",
    "title": "Bayes Nets",
    "section": "Q Matrix",
    "text": "Q Matrix\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  react_table()\n\n\n\n\n\nOkay, now on to the Q-matrix. As previously stated, I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#attribute-profile-matrix",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#attribute-profile-matrix",
    "title": "Bayes Nets",
    "section": "Attribute Profile Matrix",
    "text": "Attribute Profile Matrix\nIf we only wanted to examine how the posterior distributions compare to each student and their responses, then I would only need to have my student data and the Q-matrix. However, I also want to put students into latent classes. Because I want to put students into latent classes, I also have to create an attribute profile matrix. I am going to create this matrix by creating every possible combination of skills, which will create every potential latent class. Then I will just add each row as a numbered class. Below is the final matrix created for 3 skills.\n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\nalpha |&gt; react_table()\n\n\n\n\n\nNote: Latent classes are different from our latent nodes/attributes/skills. The matrix created above (alpha) is a matrix where each row is a different latent class and each column corresponds to each of the skills.\nSo now we have everything to build our bayes net model. Before we get to that, I do want to visually show the three models I will be creating in this series."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#naive-bayes",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#naive-bayes",
    "title": "Bayes Nets",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nnaive_dag &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - lambda1\" [latent,pos=\"0.175,0.076\"]\n\"Q-matrix\" [pos=\"0.874,0.402\"]\natt1 [latent,pos=\"0.220,0.209\"]\natt2 [latent,pos=\"0.488,0.182\"]\natt3 [latent,pos=\"0.709,0.169\"]\ndelta [latent,pos=\"0.481,0.421\"]\nfalse_positive [latent,pos=\"0.572,0.888\"]\nlambda1 [latent,pos=\"0.252,0.082\"]\nlambda20 [latent,pos=\"0.450,0.076\"]\nlambda21 [latent,pos=\"0.522,0.081\"]\nlambda30 [latent,pos=\"0.679,0.068\"]\nlambda31 [latent,pos=\"0.741,0.069\"]\ntrue_positive [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - lambda1\" -&gt; att1\n\"Q-matrix\" -&gt; delta\natt1 -&gt; delta\natt2 -&gt; delta\natt3 -&gt; delta\ndelta -&gt; y1\ndelta -&gt; y10\ndelta -&gt; y11\ndelta -&gt; y12\ndelta -&gt; y13\ndelta -&gt; y14\ndelta -&gt; y15\ndelta -&gt; y2\ndelta -&gt; y3\ndelta -&gt; y4\ndelta -&gt; y5\ndelta -&gt; y6\ndelta -&gt; y7\ndelta -&gt; y8\ndelta -&gt; y9\nfalse_positive -&gt; y1\nfalse_positive -&gt; y10\nfalse_positive -&gt; y11\nfalse_positive -&gt; y12\nfalse_positive -&gt; y13\nfalse_positive -&gt; y14\nfalse_positive -&gt; y15\nfalse_positive -&gt; y2\nfalse_positive -&gt; y3\nfalse_positive -&gt; y4\nfalse_positive -&gt; y5\nfalse_positive -&gt; y6\nfalse_positive -&gt; y7\nfalse_positive -&gt; y8\nfalse_positive -&gt; y9\nlambda1 -&gt; att1\nlambda20 -&gt; att2\nlambda21 -&gt; att2\nlambda30 -&gt; att3\nlambda31 -&gt; att3\ntrue_positive -&gt; y1\ntrue_positive -&gt; y10\ntrue_positive -&gt; y11\ntrue_positive -&gt; y12\ntrue_positive -&gt; y13\ntrue_positive -&gt; y14\ntrue_positive -&gt; y15\ntrue_positive -&gt; y2\ntrue_positive -&gt; y3\ntrue_positive -&gt; y4\ntrue_positive -&gt; y5\ntrue_positive -&gt; y6\ntrue_positive -&gt; y7\ntrue_positive -&gt; y8\ntrue_positive -&gt; y9\n}\n')\n\nggdag(naive_dag) + theme_dag()\n\n\n\n\n\n\n\n\nThe first model I will go over is essentially a naive bayes model; however, naive bayes models do not correct for what I have labeled as true positive and false positive probabilities. These are priors that will be discussed in the next post. This model mimics a deterministic inputs, noisy “and” gate (DINA) model. Essentially, the model assumes that each student has mastered all skills in order to correctly respond to an assessment item. See here for an excellent post about the DINA model and what the bayes netmodel will compare to in this series"
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#dcm",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#dcm",
    "title": "Bayes Nets",
    "section": "DCM",
    "text": "DCM\nThe DCM is somewhat difficult to visualize. Based on the model in the blog post above, the skills are not determined by priors and instead of a delta parameter, the parameter is already created in the data. I will talk shortly about that when conducting the DCM. Additionally, while the other two models have priors for true positives and false positives, the DCM includes similar parameters with prior distributions; the slip (essentially the student slipped up and made a mistake even though they have the skills) parameter and the guess (got the answer correctly but do not have the necessary skills) parameter. Those priors are different from the ones created for the bayes net models so we’ll talk about those next post."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#bayes-net",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#bayes-net",
    "title": "Bayes Nets",
    "section": "Bayes Net",
    "text": "Bayes Net\n\nbayes_net &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - lambda1\" [latent,pos=\"0.175,0.076\"]\n\"Q-matrix\" [pos=\"0.874,0.402\"]\natt1 [latent,pos=\"0.220,0.209\"]\natt2 [latent,pos=\"0.488,0.182\"]\natt3 [latent,pos=\"0.709,0.169\"]\ndelta [latent,pos=\"0.481,0.421\"]\nfalse_positive [latent,pos=\"0.572,0.888\"]\nlambda1 [latent,pos=\"0.252,0.082\"]\nlambda20 [latent,pos=\"0.450,0.076\"]\nlambda21 [latent,pos=\"0.522,0.081\"]\nlambda30 [latent,pos=\"0.679,0.068\"]\nlambda31 [latent,pos=\"0.741,0.069\"]\ntrue_positive [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - lambda1\" -&gt; att1\n\"Q-matrix\" -&gt; delta\natt1 -&gt; att2\natt1 -&gt; delta\natt2 -&gt; att3\natt2 -&gt; delta\natt3 -&gt; delta\ndelta -&gt; y1\ndelta -&gt; y10\ndelta -&gt; y11\ndelta -&gt; y12\ndelta -&gt; y13\ndelta -&gt; y14\ndelta -&gt; y15\ndelta -&gt; y2\ndelta -&gt; y3\ndelta -&gt; y4\ndelta -&gt; y5\ndelta -&gt; y6\ndelta -&gt; y7\ndelta -&gt; y8\ndelta -&gt; y9\nfalse_positive -&gt; y1\nfalse_positive -&gt; y10\nfalse_positive -&gt; y11\nfalse_positive -&gt; y12\nfalse_positive -&gt; y13\nfalse_positive -&gt; y14\nfalse_positive -&gt; y15\nfalse_positive -&gt; y2\nfalse_positive -&gt; y3\nfalse_positive -&gt; y4\nfalse_positive -&gt; y5\nfalse_positive -&gt; y6\nfalse_positive -&gt; y7\nfalse_positive -&gt; y8\nfalse_positive -&gt; y9\nlambda1 -&gt; att1\nlambda20 -&gt; att2\nlambda21 -&gt; att2\nlambda30 -&gt; att3\nlambda31 -&gt; att3\ntrue_positive -&gt; y1\ntrue_positive -&gt; y10\ntrue_positive -&gt; y11\ntrue_positive -&gt; y12\ntrue_positive -&gt; y13\ntrue_positive -&gt; y14\ntrue_positive -&gt; y15\ntrue_positive -&gt; y2\ntrue_positive -&gt; y3\ntrue_positive -&gt; y4\ntrue_positive -&gt; y5\ntrue_positive -&gt; y6\ntrue_positive -&gt; y7\ntrue_positive -&gt; y8\ntrue_positive -&gt; y9\n}\n')\n\nggdag(bayes_net) + theme_dag()\n\n\n\n\n\n\n\n\nLastly, the bayes net model is similar to the first model; however, now there are edges between the 3 skills. Other than that, nothing else has changed. In the next post I will be estimating the first bayes net model and doing some posterior checks to see how the model works."
  },
  {
    "objectID": "posts/2024-07-10-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-07-10-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Under Development - Not Complete\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table &lt;- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n\nAs mentioned in the previous post, the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the cmdstanr package to call on Stan for the Bayesian analyses, I am using the posterior package to manipulate the chains, iterations, and draws from the analyses and the bayesplot package to visualize the convergence of each parameter included in the bayes net model. I also love to use whatever table producing package I am interested at the time and create a function with html functionality. Specifically, I always include a feature to filter and highlight specific rows. This time I decided to use the reactable package.\n\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\nstan_file &lt;- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The J, I, K, and C list values are all important for looping through:\n\nJ = The number of rows of data; in this case there are 30 “students”\nI = The number of columns in the dataset; which is 15 excluding the first column\nK = The number of latent attributes/skills\nC = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from y for the actual data and then X in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n\nWarning in readLines(stan_file): incomplete final line found on\n'C:/Users/Jonathan/Documents/GitHubRepos/log-of-jandp/posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan'\n\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp:159: warning: 'stan::math::var stan::model::model_base_crtp&lt;M&gt;::log_prob(std::vector&lt;stan::math::var_value&lt;double&gt;, std::allocator&lt;stan::math::var_value&lt;double&gt; &gt; &gt;&, std::vector&lt;int&gt;&, std::ostream*) const [with M = simple_bayes_net_model_namespace::simple_bayes_net_model; stan::math::var = stan::math::var_value&lt;double&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  159 |   inline math::var log_prob(std::vector&lt;math::var&gt;& theta,\n\n\nC:/Users/Jonathan/AppData/Local/Temp/Rtmp8cZ17t/model-3aac173c6e5e.hpp:1635: note:   by 'simple_bayes_net_model_namespace::simple_bayes_net_model::log_prob'\n 1635 |   log_prob(std::vector&lt;T_&gt;& params_r, std::vector&lt;int&gt;& params_i,\nstan/src/stan/model/model_base_crtp.hpp:154: warning: 'double stan::model::model_base_crtp&lt;M&gt;::log_prob(std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::ostream*) const [with M = simple_bayes_net_model_namespace::simple_bayes_net_model; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  154 |   inline double log_prob(std::vector&lt;double&gt;& theta, std::vector&lt;int&gt;& theta_i,\nC:/Users/Jonathan/AppData/Local/Temp/Rtmp8cZ17t/model-3aac173c6e5e.hpp:1635: note:   by 'simple_bayes_net_model_namespace::simple_bayes_net_model::log_prob'\n 1635 |   log_prob(std::vector&lt;T_&gt;& params_r, std::vector&lt;int&gt;& params_i,\n\n\nstan/src/stan/model/model_base_crtp.hpp:96: warning: 'stan::math::var stan::model::model_base_crtp&lt;M&gt;::log_prob(Eigen::Matrix&lt;stan::math::var_value&lt;double&gt;, -1, 1&gt;&, std::ostream*) const [with M = simple_bayes_net_model_namespace::simple_bayes_net_model; stan::math::var = stan::math::var_value&lt;double&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n   96 |   inline math::var log_prob(Eigen::Matrix&lt;math::var, -1, 1&gt;& theta,\n\n\nC:/Users/Jonathan/AppData/Local/Temp/Rtmp8cZ17t/model-3aac173c6e5e.hpp:1635: note:   by 'simple_bayes_net_model_namespace::simple_bayes_net_model::log_prob'\n 1635 |   log_prob(std::vector&lt;T_&gt;& params_r, std::vector&lt;int&gt;& params_i,\n\n\nstan/src/stan/model/model_base_crtp.hpp:91: warning: 'double stan::model::model_base_crtp&lt;M&gt;::log_prob(Eigen::VectorXd&, std::ostream*) const [with M = simple_bayes_net_model_namespace::simple_bayes_net_model; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n   91 |   inline double log_prob(Eigen::VectorXd& theta,\n\n\nC:/Users/Jonathan/AppData/Local/Temp/Rtmp8cZ17t/model-3aac173c6e5e.hpp:1635: note:   by 'simple_bayes_net_model_namespace::simple_bayes_net_model::log_prob'\n 1635 |   log_prob(std::vector&lt;T_&gt;& params_r, std::vector&lt;int&gt;& params_i,\n\n\nstan/src/stan/model/model_base_crtp.hpp:205: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(stan::rng_t&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = simple_bayes_net_model_namespace::simple_bayes_net_model; stan::rng_t = boost::random::mixmax_engine&lt;17, 36, 0&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  205 |   void write_array(stan::rng_t& rng, std::vector&lt;double&gt;& theta,\n\n\nC:/Users/Jonathan/AppData/Local/Temp/Rtmp8cZ17t/model-3aac173c6e5e.hpp:1612: note:   by 'simple_bayes_net_model_namespace::simple_bayes_net_model::write_array'\n 1612 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(stan::rng_t&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = simple_bayes_net_model_namespace::simple_bayes_net_model; stan::rng_t = boost::random::mixmax_engine&lt;17, 36, 0&gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(stan::rng_t& rng, Eigen::VectorXd& theta,\nC:/Users/Jonathan/AppData/Local/Temp/Rtmp8cZ17t/model-3aac173c6e5e.hpp:1612: note:   by 'simple_bayes_net_model_namespace::simple_bayes_net_model::write_array'\n 1612 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n\nfit &lt;- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n\nRunning MCMC with 4 chains, at most 12 in parallel...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 37.6 seconds.\nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 41.5 seconds.\nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 42.1 seconds.\nChain 2 finished in 42.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 40.8 seconds.\nTotal execution time: 42.4 seconds.\n\n# fit$save_object(\"simple_bayes_net.RDS\")\n\nSo this next part will be different depending on whether or not you are using RStan or like in this case cmdstanR. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For cmdstanR, you call on your Stan file. Below is the Stan code or if you’d like to see it side-by-side, the Stan file can be found here. I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations.\n\n\n# fit &lt;- read_rds(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9748978 0.9855169 0.9468038 0.9679686\n\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |&gt; arrange(desc(rhat)) |&gt; head()\n\n# A tibble: 6 × 4\n  variable      rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__          1.00    3019.    4683.\n2 pie           1.00   11711.    6570.\n3 log_item[15]  1.00   11711.    6570.\n4 x_rep[1,1]    1.00   11711.    6570.\n5 x_rep[2,1]    1.00   11711.    6570.\n6 x_rep[3,1]    1.00   11711.    6570.\n\nbn_measure |&gt; mutate(across(-variable, ~round(.x, 3))) |&gt; react_table()\n\n\n\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\ny_rep &lt;- fit$draws(\"x_rep\") |&gt; as_draws_matrix()\nstu_resp_attr &lt;- fit$draws(\"prob_resp_attr\") |&gt; as_draws_matrix()\n\nI decided to extract the replicated values for the items and the probabilities oof each student’s mastery of each of the three latent attributes.\n\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n\n\n\n\n\n\n\ny |&gt; react_table()\n\n\n\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the originaly data, the original responses and the probabilities with a probability threshold of 0.5 match one another.\n\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\nppc_intervals(\n  y = y |&gt; pull(y1) |&gt; as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n\n\n\n\n\n\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\nlibrary(loo)\n\nThis is loo version 2.8.0\n\n\n- Online documentation and vignettes at mc-stan.org/loo\n\n\n- As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. \n\n\n- Windows 10 users: loo may be very slow if 'mc.cores' is set in your .Rprofile file (see https://github.com/stan-dev/loo/issues/94).\n\nloo(y_rep)\n\n\nComputed from 8000 by 450 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -303.2  7.6\np_loo         8.1  0.3\nlooic       606.4 15.2\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nwaic(y_rep)\n\n\nComputed from 8000 by 450 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -303.2  7.6\np_waic         8.1  0.3\nwaic         606.4 15.2\n\nbn_resid &lt;- y[,-1] - exp(y_rep)\n\nbn_resid^2 |&gt; \n  as_tibble() |&gt;\n  rowid_to_column() |&gt;\n  ggplot(\n    aes(\n      rowid,\n      y2\n    )\n  ) +\n  geom_point(\n    alpha = .7\n  )\n\n\n\n\n\n\n\n\n\nactual_stu_resp_attr &lt;- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |&gt;\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.\n\nstu_resp_attr_mean &lt;- stu_resp_attr |&gt;\n  as_tibble() |&gt;\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_mean |&gt;\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(\n    everything()\n  ) |&gt;\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |&gt;\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |&gt;\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute.\n\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n\n$att1\n   .y\n.x   0  1\n  1 15 15\n\n$att2\n   .y\n.x   0  1\n  1 14 16\n\n$att3\n   .y\n.x   0  1\n  0  1  2\n  1 11 16\n\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n\n$att1\n   .y\n.x    0   1\n  1 0.5 0.5\n\n$att2\n   .y\n.x          0         1\n  1 0.4666667 0.5333333\n\n$att3\n   .y\n.x           0          1\n  0 0.03333333 0.06666667\n  1 0.36666667 0.53333333\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model.\n\nstu_resp_attr_long &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long &lt;- actual_stu_resp_attr |&gt;\n  pivot_longer(-studentid)\n\naccuracy_att &lt;- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n\n[1] 0.5333333\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 0.5333333. This is a good starting point, but this may indicate that the model needs better definied priors and may require the edges between the attributes to show latent relationships."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html",
    "title": "Using Typst To Create Documents",
    "section": "",
    "text": "Under Development - Not Complete\nI have been using Typst, an awesome app for working on pdf files at the same time as colleagues. You could think of this as something similar to Google Docs or GitHub for code. This also got me thinking about creating a small series of blog posts about using Typst and then creating Typst templates for documents using Quarto. The latter topics would be using Quarto extensions and if following along, you would need Quarto version 1.4 at least to be able to use Typst code chunks on a Quarto document. So first, I will show the Typst file I will be using because let’s face it I’m on the job market and free publicity is always good.\n.\nHere is the link for the Typst resume to view. If you want, you can just copy and paste that into Typst and change the information. I will walk through each section of the document with Typst code in the post, as well as a cover letter post, and then end the series with a Quarto extension to create a Typst template so you can just write your resume and/or cover letter in Quarto."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#typst-documentation",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#typst-documentation",
    "title": "Using Typst To Create Documents",
    "section": "Typst Documentation",
    "text": "Typst Documentation\nI will be the first to state that Typst documentation is a little difficult to follow at first. Hopefully with this tutorial you will get a better understanding of the basics of Typst code. As someone who tried to learn LaTeX to edit the previous resume I had found a template for, I wish Typst existed earlier."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "title": "Using Typst To Create Documents",
    "section": "Creating a Typst Document",
    "text": "Creating a Typst Document\nWorking in the Typst app is pretty straightforward with you creating a username, followed by your dashboard with nothing there. This will be the location of all of your documents as you get started with Typst. While there are Typst templates already for resumes I really wanted to create something similar to the resume I had in LaTeX. Working in Quarto, you will have to learn how to create Typst code chunks. They are slightly different from other languages’ code chunks but you can still use all of the Quarto code chunk arguments.\nFrom what I have seen online, there does not seem to be much difference in the ordering of some of the beginning Typst documentation. I have decided to start my Typst document with any variables I will be including, followed by any Typst packages I will need, and then setting up the general parameters for the document. These general parameters are for the document overall. Things like setting the font to a specific font, size and maybe weight would be a good parameter to set at the top of your document."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "title": "Using Typst To Create Documents",
    "section": "Breakdown of Variables, Packages, and # Set function for parameters",
    "text": "Breakdown of Variables, Packages, and # Set function for parameters\n\n//Variables\n#let name = [Jonathan A. Pedroza Ph.D]\n\n//Packages\n#import \"@preview/tablex:0.0.8\": tablex\n#import \"lib-gen.typ\": *\n#import \"lib-impl.typ\": *\n#import \"lib.typ\": *\n\n#set page(\n  margin: (\n    top: 0cm,\n    bottom: 0cm,\n    left: .5cm,\n    right: 0cm\n  )\n)\n\n#set block(spacing: 0.5em)\n\n#set rect(\n  width: 37%,\n  height: 100%\n)\n\nAbove is the beginning Typst code for the resume I created. I’m going to walk through the code a little, but for more detailed information, check out the help documentation. The // syntax can be included wherever to include comments. Since I have been showing others how to use Typst to create quick pretty PDFs, I have been including a lot of comments for things like variables. To create variables in Typst, you will need to use the #let function followed by your variable name, an equal sign, and the information you want to include. So I created the variable name, which would be used as #name in Typst and the document will spell out my full name. After that, just as the comment states, I included the tablex package, which I have found to be useful for creating tables and grids. If you’d like you can use the #table or #grid functions from Typst. Additionally, I also included the files for using the FontAwesome Typst package. You can find all the icons and other information about FontAwesome at the FontAwesome website. I was just being lazy as I wanted to create my resume quick so I could get it out into the world ASAP. Next I set the margins to maximize the amount of space I would have for my resume and I created a block after the titles Education and Professional Experience since I did not want the default amount of space before my education and experience entries. The #set function creates rules for the document as a whole. So For the whole document I have the same margins throughout, a block to create more space between headers and text underneath and the rectangle to separate the sections of the document. Lastly, I set a rectangle for 37% of the document’s width and 100% of the height. There are other metrics that can be used to create the rectangle but I personally was enjoying using percentages for this document. This rectangle is for the right side of the document that includes the contact information."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#grid-of-entries",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#grid-of-entries",
    "title": "Using Typst To Create Documents",
    "section": "Grid of Entries",
    "text": "Grid of Entries\n\n#grid(\n  columns: (70%, 82%),\n  [\n    #linebreak()\n\n    #set text(\n      font: \"Source Sans Pro\",\n      size: 10pt\n    )\n    #set align(center)\n    \n    = Education #fa-graduation-cap()\n    #line(\n      length: 94%,\n      stroke: black\n    )\n\nI decided to separate some of these functions because it might be easier to talk about. So the grid here is actually separating the main text and the contact information text. As I am reading this, I can see that I probably should have set the font to be 10pt throughout the document rather than within the grid."
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html",
    "href": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(inspectdf)\nlibrary(bnlearn)\nlibrary(Rgraphviz)\n\nLoading required package: graph\nLoading required package: BiocGenerics\n\nAttaching package: 'BiocGenerics'\n\nThe following object is masked from 'package:bnlearn':\n\n    score\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\nAttaching package: 'graph'\n\nThe following objects are masked from 'package:bnlearn':\n\n    degree, nodes, nodes&lt;-\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\nLoading required package: grid\n\nlibrary(reactable)\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(dagitty)\n\n\nAttaching package: 'dagitty'\n\nThe following object is masked from 'package:Rgraphviz':\n\n    graphLayout\n\nThe following object is masked from 'package:graph':\n\n    edges\n\nThe following objects are masked from 'package:bnlearn':\n\n    ancestors, children, descendants, parents, spouses\n\nbn_score &lt;- bnlearn::score\n\ntheme_set(theme_light())\n\ncoffee &lt;- read_csv(here::here(\"posts/2024-11-03-bayes-net-us-coffee-tasting\", \"hoffmann_america_taste_data.csv\")) |&gt;\n  janitor::clean_names()\n\nRows: 4042 Columns: 113\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): Submission ID, What is your age?, How many cups of coffee do you t...\ndbl (13): Lastly, how would you rate your own coffee expertise?, Coffee A - ...\nlgl (56): Where do you typically drink coffee? (At home), Where do you typic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncoffee |&gt;\n  inspect_na() |&gt;\n  show_plot()\ncoffee_drop &lt;- coffee[, which(colMeans(!is.na(coffee)) &gt; 0.5)]\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -c(\n      where_do_you_typically_drink_coffee,\n      # how_do_you_brew_coffee_at_home,\n      do_you_usually_add_anything_to_your_coffee,\n      why_do_you_drink_coffee\n    )\n  ) |&gt;\n  rename(\n    age = what_is_your_age,\n    cup_per_day = how_many_cups_of_coffee_do_you_typically_drink_per_day,\n    drink_at_home = where_do_you_typically_drink_coffee_at_home,\n    drink_at_office = where_do_you_typically_drink_coffee_at_the_office,\n    drink_on_go = where_do_you_typically_drink_coffee_on_the_go,\n    drink_at_cafe = where_do_you_typically_drink_coffee_at_a_cafe,\n    drink_none_of_these = where_do_you_typically_drink_coffee_none_of_these,\n    home_brew_pour_over = how_do_you_brew_coffee_at_home_pour_over,\n    home_brew_french_press = how_do_you_brew_coffee_at_home_french_press,\n    home_brew_espresso = how_do_you_brew_coffee_at_home_espresso,\n    home_brew_mr_coffee = how_do_you_brew_coffee_at_home_coffee_brewing_machine_e_g_mr_coffee,\n    home_brew_pods = how_do_you_brew_coffee_at_home_pod_capsule_machine_e_g_keurig_nespresso,\n    home_brew_instant = how_do_you_brew_coffee_at_home_instant_coffee,\n    home_brew_bean2cup = how_do_you_brew_coffee_at_home_bean_to_cup_machine,\n    home_brew_cold_brew = how_do_you_brew_coffee_at_home_cold_brew,\n    home_brew_cometeer = how_do_you_brew_coffee_at_home_coffee_extract_e_g_cometeer,\n    home_brew_other = how_do_you_brew_coffee_at_home_other,\n    favorite_coffee_drink = what_is_your_favorite_coffee_drink,\n    coffee_black = do_you_usually_add_anything_to_your_coffee_no_just_black,\n    coffee_milk_alt_creamer = do_you_usually_add_anything_to_your_coffee_milk_dairy_alternative_or_coffee_creamer,\n    coffee_sugar = do_you_usually_add_anything_to_your_coffee_sugar_or_sweetener,\n    coffee_syrup = do_you_usually_add_anything_to_your_coffee_flavor_syrup,\n    coffee_other = do_you_usually_add_anything_to_your_coffee_other,\n    coffee_characteristic_preference = before_todays_tasting_which_of_the_following_best_described_what_kind_of_coffee_you_like,\n    coffee_strength = how_strong_do_you_like_your_coffee,\n    roast_preference = what_roast_level_of_coffee_do_you_prefer,\n    caffeine_preference = how_much_caffeine_do_you_like_in_your_coffee,\n    expertise = lastly_how_would_you_rate_your_own_coffee_expertise,\n    preference_a_to_b = between_coffee_a_coffee_b_and_coffee_c_which_did_you_prefer,\n    preference_a_to_d = between_coffee_a_and_coffee_d_which_did_you_prefer,\n    favorite_abcd = lastly_what_was_your_favorite_overall_coffee,\n    remote_work = do_you_work_from_home_or_in_person,\n    money_spend_a_month = in_total_much_money_do_you_typically_spend_on_coffee_in_a_month,\n    why_drink_taste_good = why_do_you_drink_coffee_it_tastes_good,\n    why_drink_caffeine = why_do_you_drink_coffee_i_need_the_caffeine,\n    why_drink_ritual = why_do_you_drink_coffee_i_need_the_ritual,\n    why_drink_makes_bathroom = why_do_you_drink_coffee_it_makes_me_go_to_the_bathroom,\n    why_drink_other = why_do_you_drink_coffee_other,\n    like_taste = do_you_like_the_taste_of_coffee,\n    know_where_coffee_comes_from = do_you_know_where_your_coffee_comes_from,\n    most_spent_on_cup_coffee = what_is_the_most_youve_ever_paid_for_a_cup_of_coffee,\n    willing_to_spend_cup_coffee = what_is_the_most_youd_ever_be_willing_to_pay_for_a_cup_of_coffee,\n    good_value_cafe = do_you_feel_like_you_re_getting_good_value_for_your_money_when_you_buy_coffee_at_a_cafe,\n    equipment_spent_5years = approximately_how_much_have_you_spent_on_coffee_equipment_in_the_past_5_years,\n    good_value_equipment = do_you_feel_like_you_re_getting_good_value_for_your_money_with_regards_to_your_coffee_equipment\n  )\ncoffee_logical &lt;- coffee_drop |&gt;\n  select_if(is.logical)\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  drop_na(\n    colnames(coffee_logical)\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  mutate(\n    across(\n      where(\n        is.logical\n      ),\n      ~case_when(\n        .x == TRUE ~ 1,\n        .x == FALSE ~ 0\n      )\n    ),\n    across(\n      where(\n        is.character\n      ),\n      ~as.factor(.x)\n    )\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -matches(\n      \"_notes\"\n    )\n  )\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()\ncoffee_drop |&gt;\n  drop_na(favorite_abcd) |&gt;\n  ggplot(\n    aes(\n      favorite_abcd\n    )\n  ) +\n  geom_bar(\n    aes(\n      fill = favorite_abcd\n    )\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  labs(\n    x = \"Coffee Choices\",\n    y = \"Counts\",\n    title = \"Counts of Each Coffee\"\n  ) +\n  theme(\n    legend.position = \"none\"\n  )\ncoffee_drop |&gt;\n  select(\n    age,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -age\n  ) |&gt;\n  group_by(\n    age,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    age = as.factor(age),\n    age = fct_relevel(\n      age,\n      \"&lt;18 years old\",\n      \"18-24 years old\",\n      \"25-34 years old\",\n      \"35-44 years old\",\n      \"45-54 years old\",\n      \"55-64 years old\",\n      \"&gt;65 years old\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  ggplot(\n    aes(\n      age,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    ),\n    axis.text.x = element_text(\n      angle = 45,\n      vjust = 0.5\n      )\n    ) +\n  NULL\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    gender = as.factor(gender),\n    name = as.factor(name),\n    gender = fct_relevel(\n      gender,\n      \"Male\",\n      \"Female\",\n      \"Non-binary\",\n      \"Other (please specify)\",\n      \"Prefer not to say\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  ggplot(\n    aes(\n      gender,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    )\n    ) +\n  NULL\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    gender = as.factor(gender),\n    name = as.factor(name),\n    gender = fct_relevel(\n      gender,\n      \"Male\",\n      \"Female\",\n      \"Non-binary\",\n      \"Other (please specify)\",\n      \"Prefer not to say\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  filter(\n    !gender %in% c(\"Male\", \"Female\")\n  ) |&gt;\n  ggplot(\n    aes(\n      gender,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    )\n    ) +\n  NULL\ncoffee_drop |&gt;\n  group_by(\n    favorite_abcd,\n    expertise\n  ) |&gt;\n  count() |&gt; \n  ungroup(\n    favorite_abcd\n    ) |&gt;\n  mutate(\n    percent = n/sum(n),\n    percent = percent*100\n  ) |&gt;\n  drop_na() |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise),\n      percent\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = as.factor(favorite_abcd)\n    )\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  labs(\n    title = \"Favorite Coffees By Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Percentage\",\n    fill = \"\"\n  ) +\n  NULL"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#using-bnlearn",
    "href": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#using-bnlearn",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Using Bnlearn",
    "text": "Using Bnlearn\n\n# Building Bayesian Network\ndag &lt;- empty.graph(nodes = colnames(no_fact))\n\narcs &lt;- matrix(\n  c(\"gender\", \"cup_per_day\",\n    \"gender\", \"favorite_coffee_drink\",\n    \"gender\", \"home_brew_pour_over\",\n    \"gender\", \"home_brew_french_press\",\n    \"gender\", \"home_brew_espresso\",\n    \"gender\", \"home_brew_mr_coffee\",\n    \"gender\", \"home_brew_pods\",\n    \"gender\", \"home_brew_instant\",\n    \"gender\", \"home_brew_bean2cup\",\n    \"gender\", \"home_brew_cold_brew\",\n    \"gender\", \"home_brew_cometeer\",\n    \"gender\", \"home_brew_other\",\n    \"age\", \"cup_per_day\",\n    \"age\", \"favorite_coffee_drink\",\n    \"age\", \"home_brew_pour_over\",\n    \"age\", \"home_brew_french_press\",\n    \"age\", \"home_brew_espresso\",\n    \"age\", \"home_brew_mr_coffee\",\n    \"age\", \"home_brew_pods\",\n    \"age\", \"home_brew_instant\",\n    \"age\", \"home_brew_bean2cup\",\n    \"age\", \"home_brew_cold_brew\",\n    \"age\", \"home_brew_cometeer\",\n    \"age\", \"home_brew_other\",\n\n    \"cup_per_day\", \"roast_preference\",\n    \"favorite_coffee_drink\", \"roast_preference\",\n    \"home_brew_pour_over\", \"roast_preference\",\n    \"home_brew_french_press\", \"roast_preference\",\n    \"home_brew_espresso\", \"roast_preference\",\n    \"home_brew_mr_coffee\", \"roast_preference\",\n    \"home_brew_pods\", \"roast_preference\",\n    \"home_brew_instant\", \"roast_preference\",\n    \"home_brew_bean2cup\", \"roast_preference\",\n    \"home_brew_cold_brew\", \"roast_preference\",\n    \"home_brew_cometeer\", \"roast_preference\",\n    \"home_brew_other\", \"roast_preference\",\n\n    \"roast_preference\", \"expertise\",\n    \"roast_preference\", \"favorite_abcd\",\n    \"expertise\", \"favorite_abcd\"),\n  byrow = TRUE,\n  ncol = 2,\n  dimnames = list(NULL, c(\"from\", \"to\"))\n)\n\narcs(dag) &lt;- arcs\n\ngraphviz.plot(dag)\n\n\n\n\n\n\n\n\n\nset.seed(12345)\ndag_fit &lt;- bn.fit(dag, data = no_fact, method = \"bayes\", iss = 5000)\n\n\nGender - Conditional Probability Table (CPT)\n\ntibble(\n  Gender = attributes(dag_fit$gender$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$gender$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\nAge - CPT\n\ntibble(\n  Age = attributes(dag_fit$age$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$age$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\nCups of Coffee Per Day - CPT\n\ndag_fit$cup_per_day$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = cup_per_day, y = n, fill = gender, facet = age) +\n  labs(\n    title = \"Probability of Cups of Coffee Per Day From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Pourover\n\ndag_fit$home_brew_pour_over$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pour_over, facet = age) +\n  labs(\n    title = \"Probability of Making Pourover at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - French Press\n\ndag_fit$home_brew_french_press$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_french_press, facet = age) +\n  labs(\n    title = \"Probability of Making French Press at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Espresso\n\ndag_fit$home_brew_espresso$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_espresso, facet = age) +\n  labs(\n    title = \"Probability of Making Espresso at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - coffee Brewing Machine\n\ndag_fit$home_brew_mr_coffee$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_mr_coffee, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Coffee Brewing Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Pods\n\ndag_fit$home_brew_pods$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pods, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using Pods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Instant Coffee\n\ndag_fit$home_brew_instant$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_instant, facet = age) +\n  labs(\n    title = \"Probability of Making Instant Coffee at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Bean 2 Cup\n\ndag_fit$home_brew_bean2cup$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_bean2cup, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Bean 2 Cup Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Cold Brew\n\ndag_fit$home_brew_cold_brew$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cold_brew, facet = age) +\n  labs(\n    title = \"Probability of Making Cold Brew at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Cometeer\n\ndag_fit$home_brew_cometeer$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cometeer, facet = age) +\n  labs(\n    title = \"Probability of Making Cometeer Coffees at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Other\n\ndag_fit$home_brew_other$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_other, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee From Other Methods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nFavorite Coffee Drink - CPT\n\ndag_fit$favorite_coffee_drink$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(\n    x = favorite_coffee_drink,\n    y = n,\n    fill = gender,\n    facet = age\n  ) +\n  labs(\n    title = \"Probability of Favorite Coffee Drinks From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nRoast Preference - CPT\n\nroast_pref_func &lt;- function(\n  dag_table,\n  x,\n  y = n,\n  fill,\n  facet_x,\n  facet_y\n){\n  {{dag_table}} |&gt;\n  as_tibble() |&gt;\n  mutate(\n    fill = str_to_title({{fill}}),\n    n = round(n, 2),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    x = fct_relevel(\n      {{x}},\n      \"three_or_more\",\n      \"2\",\n      \"one_or_less\"\n    ),\n    fill = fct_relevel(\n      fill,\n      \"Dark\",\n      \"Medium\",\n      \"Light\"\n    )\n  ) |&gt;\n  ggplot(\n    aes(\n      x,\n      {{y}}\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = fill\n      )\n  ) +\n  coord_flip() +\n  facet_grid(\n    vars({{facet_x}}),\n    vars({{facet_y}})\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  guides(\n    fill = guide_legend(\n      reverse = TRUE\n      )\n    )\n}\n\nThe CPT here only include the probabilities for whether participants used one home brewer or not and considered all of the other home brewers as not using those at home.\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pour_over\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Pourover\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_french_press\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a French Press\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_espresso\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Espresso at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_mr_coffee\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Coffee Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pods\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Pods\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_instant\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Instant Coffee\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_bean2cup\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Bean 2 Cup Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cold_brew\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Cold Brew at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cometeer\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Cometeer\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_other\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using an Other Method\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\nExpertise Level - CPT\n\nexpertise_tbl &lt;- dag_fit$expertise$prob |&gt; as_tibble() |&gt;\n  mutate(\n    roast_preference = str_to_title(roast_preference),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"Light\",\n      \"Medium\",\n      \"Dark\"\n    )\n  ) \n\nexpertise_tbl |&gt;\n  ggplot(\n    aes(\n      roast_preference,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = expertise\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = expertise_tbl |&gt; filter(roast_preference == \"Light\"),\n    aes(\n      label = expertise,\n      group = expertise,\n      color = expertise\n    ),\n    position = position_dodge(width = .9),\n    vjust = -.5\n  ) +\n   labs(\n    title = \"Probability of One's Roast Preference From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level\",\n    x = \"\",\n    y = \"Probability\",\n    caption = \"Note: Probabilities range from 0 to 1. The scale is reduced to visually compare groups.\"\n  ) +\n  viridis::scale_color_viridis(\n    discrete = TRUE\n    ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  scale_x_discrete(\n    expand = c(0, .5)\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text = element_text(\n      color = \"black\"\n    ),\n    axis.title = element_text(\n      color = \"black\"\n    ),\n    plot.title = element_text(\n      color = \"black\"\n    ),\n    plot.subtitle = element_text(\n      color = \"black\"\n    ),\n    plot.caption = element_text(\n      color = \"black\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\nFavorite Coffee (A, B, C, D) - CPT\n\nfavorite_abcd_prob &lt;- dag_fit$favorite_abcd$prob |&gt; as_tibble() |&gt;\n  mutate(\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"light\",\n      \"medium\",\n      \"dark\"\n    ),\n    favorite_abcd = fct_relevel(\n      favorite_abcd,\n      \"Coffee A\",\n      \"Coffee B\",\n      \"Coffee C\",\n      \"Coffee D\"\n    )\n  )\n\n# scales::show_col(viridis::viridis_pal(option = \"E\")(3))\n\nfavorite_abcd_prob |&gt;\n  ggplot(\n    aes(\n      favorite_abcd,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = roast_preference\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee D\" &\n      roast_preference == \"light\"\n    ),\n    label = \"Light\\nRoast\",\n    nudge_y = .03,\n    color = \"#00204DFF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee C\" &\n      roast_preference == \"medium\"\n    ),\n    label = \"Medium\\nRoast\",\n    nudge_y = .03,\n    color = \"#7C7B78FF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee B\" &\n      roast_preference == \"dark\"\n    ),\n    label = \"Dark\\nRoast\",\n    nudge_y = .03,\n    color = \"#FFEA46FF\"\n  ) +\n  facet_wrap(\n    ~expertise,\n    ncol = 5\n  ) +\n  scale_y_continuous(\n    breaks = seq(.1, .6, .1)\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE,\n    option = \"cividis\"\n  ) +\n  labs(\n    title = \"Probability of One's Favorite Coffees From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level & Roast Level Preference\",\n    x = \"\",\n    y = \"Probability\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    strip.background = element_rect(\n      fill = \"#7C7B78FF\"\n    ),\n    axis.text = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    axis.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.subtitle = element_text(\n      color = \"#7C7B78FF\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nscore(\n  dag,\n  data = no_fact, \n  type = \"bde\",\n  iss = 5000\n)\n\n[1] -49810.62\n\n\n\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\"),\n  evidence = (gender == \"Male\")\n)\n\n[1] 0.2386035\n\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\"),\n  evidence = (gender == \"Female\")\n)\n\n[1] 0.228005\n\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Male\")\n)\n\n[1] 0.302377\n\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Female\")\n)\n\n[1] 0.2890071\n\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Male\")\n)\n\n[1] 0.5395322\n\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Female\")\n)\n\n[1] 0.5265312"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "href": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Probabilities of Coffee A or Coffee D Based on Expertise Level",
    "text": "Probabilities of Coffee A or Coffee D Based on Expertise Level\n\nexpert1 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"1\")\n)\nexpert2 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"2\")\n)\nexpert3 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"3\")\n)\nexpert4 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"4\")\n)\nexpert5 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"5\")\n)\nexpert6 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"6\")\n)\nexpert7 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"7\")\n)\nexpert8 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"8\")\n)\nexpert9 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"9\")\n)\nexpert10 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"10\")\n)\n\ntibble(\n  expertise_level = seq(1, 10, 1),\n  probability_of_a_or_d = c(expert1, expert2, expert3, expert4, expert5, expert6, expert7, expert8, expert9, expert10)\n) |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise_level),\n      probability_of_a_or_d\n    )\n  ) +\n  geom_col(\n    aes(fill = as.factor(expertise_level)),\n    position = position_dodge()\n  ) +\n  geom_text(\n    aes(\n      label = round(probability_of_a_or_d, 2)\n    ),\n    color = \"black\",\n    vjust = -.3\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, .1)\n  ) +\n  labs(\n    title = \"Probability of Choosing Coffee A or D as Their Favorite Coffee\",\n    subtitle = \"By Level of Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Probability\"\n  ) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n  theme(\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html",
    "href": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html",
    "title": "Bayesian Network for US Coffee Tasting Data Using R & Stan",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(inspectdf)\nlibrary(bnlearn)\nlibrary(Rgraphviz)\n\nLoading required package: graph\nLoading required package: BiocGenerics\n\nAttaching package: 'BiocGenerics'\n\nThe following object is masked from 'package:bnlearn':\n\n    score\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\nAttaching package: 'graph'\n\nThe following objects are masked from 'package:bnlearn':\n\n    degree, nodes, nodes&lt;-\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\nLoading required package: grid\n\nlibrary(reactable)\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(dagitty)\n\n\nAttaching package: 'dagitty'\n\nThe following object is masked from 'package:Rgraphviz':\n\n    graphLayout\n\nThe following object is masked from 'package:graph':\n\n    edges\n\nThe following objects are masked from 'package:bnlearn':\n\n    ancestors, children, descendants, parents, spouses\n\nbn_score &lt;- bnlearn::score\n\ntheme_set(theme_light())\n\ncoffee &lt;- read_csv(here::here(\"posts/2024-11-03-bayes-net-us-coffee-tasting\", \"hoffmann_america_taste_data.csv\")) |&gt;\n  janitor::clean_names()\n\nRows: 4042 Columns: 113\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): Submission ID, What is your age?, How many cups of coffee do you t...\ndbl (13): Lastly, how would you rate your own coffee expertise?, Coffee A - ...\nlgl (56): Where do you typically drink coffee? (At home), Where do you typic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ncoffee_drop &lt;- coffee[, which(colMeans(!is.na(coffee)) &gt; 0.5)]\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -c(\n      where_do_you_typically_drink_coffee,\n      # how_do_you_brew_coffee_at_home,\n      do_you_usually_add_anything_to_your_coffee,\n      why_do_you_drink_coffee\n    )\n  ) |&gt;\n  rename(\n    age = what_is_your_age,\n    cup_per_day = how_many_cups_of_coffee_do_you_typically_drink_per_day,\n    drink_at_home = where_do_you_typically_drink_coffee_at_home,\n    drink_at_office = where_do_you_typically_drink_coffee_at_the_office,\n    drink_on_go = where_do_you_typically_drink_coffee_on_the_go,\n    drink_at_cafe = where_do_you_typically_drink_coffee_at_a_cafe,\n    drink_none_of_these = where_do_you_typically_drink_coffee_none_of_these,\n    home_brew_pour_over = how_do_you_brew_coffee_at_home_pour_over,\n    home_brew_french_press = how_do_you_brew_coffee_at_home_french_press,\n    home_brew_espresso = how_do_you_brew_coffee_at_home_espresso,\n    home_brew_mr_coffee = how_do_you_brew_coffee_at_home_coffee_brewing_machine_e_g_mr_coffee,\n    home_brew_pods = how_do_you_brew_coffee_at_home_pod_capsule_machine_e_g_keurig_nespresso,\n    home_brew_instant = how_do_you_brew_coffee_at_home_instant_coffee,\n    home_brew_bean2cup = how_do_you_brew_coffee_at_home_bean_to_cup_machine,\n    home_brew_cold_brew = how_do_you_brew_coffee_at_home_cold_brew,\n    home_brew_cometeer = how_do_you_brew_coffee_at_home_coffee_extract_e_g_cometeer,\n    home_brew_other = how_do_you_brew_coffee_at_home_other,\n    favorite_coffee_drink = what_is_your_favorite_coffee_drink,\n    coffee_black = do_you_usually_add_anything_to_your_coffee_no_just_black,\n    coffee_milk_alt_creamer = do_you_usually_add_anything_to_your_coffee_milk_dairy_alternative_or_coffee_creamer,\n    coffee_sugar = do_you_usually_add_anything_to_your_coffee_sugar_or_sweetener,\n    coffee_syrup = do_you_usually_add_anything_to_your_coffee_flavor_syrup,\n    coffee_other = do_you_usually_add_anything_to_your_coffee_other,\n    coffee_characteristic_preference = before_todays_tasting_which_of_the_following_best_described_what_kind_of_coffee_you_like,\n    coffee_strength = how_strong_do_you_like_your_coffee,\n    roast_preference = what_roast_level_of_coffee_do_you_prefer,\n    caffeine_preference = how_much_caffeine_do_you_like_in_your_coffee,\n    expertise = lastly_how_would_you_rate_your_own_coffee_expertise,\n    preference_a_to_b = between_coffee_a_coffee_b_and_coffee_c_which_did_you_prefer,\n    preference_a_to_d = between_coffee_a_and_coffee_d_which_did_you_prefer,\n    favorite_abcd = lastly_what_was_your_favorite_overall_coffee,\n    remote_work = do_you_work_from_home_or_in_person,\n    money_spend_a_month = in_total_much_money_do_you_typically_spend_on_coffee_in_a_month,\n    why_drink_taste_good = why_do_you_drink_coffee_it_tastes_good,\n    why_drink_caffeine = why_do_you_drink_coffee_i_need_the_caffeine,\n    why_drink_ritual = why_do_you_drink_coffee_i_need_the_ritual,\n    why_drink_makes_bathroom = why_do_you_drink_coffee_it_makes_me_go_to_the_bathroom,\n    why_drink_other = why_do_you_drink_coffee_other,\n    like_taste = do_you_like_the_taste_of_coffee,\n    know_where_coffee_comes_from = do_you_know_where_your_coffee_comes_from,\n    most_spent_on_cup_coffee = what_is_the_most_youve_ever_paid_for_a_cup_of_coffee,\n    willing_to_spend_cup_coffee = what_is_the_most_youd_ever_be_willing_to_pay_for_a_cup_of_coffee,\n    good_value_cafe = do_you_feel_like_you_re_getting_good_value_for_your_money_when_you_buy_coffee_at_a_cafe,\n    equipment_spent_5years = approximately_how_much_have_you_spent_on_coffee_equipment_in_the_past_5_years,\n    good_value_equipment = do_you_feel_like_you_re_getting_good_value_for_your_money_with_regards_to_your_coffee_equipment\n  )\ncoffee_logical &lt;- coffee_drop |&gt;\n  select_if(is.logical)\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  drop_na(\n    colnames(coffee_logical)\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  mutate(\n    across(\n      where(\n        is.logical\n      ),\n      ~case_when(\n        .x == TRUE ~ 1,\n        .x == FALSE ~ 0\n      )\n    ),\n    across(\n      where(\n        is.character\n      ),\n      ~as.factor(.x)\n    )\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -matches(\n      \"_notes\"\n    )\n  )\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html#using-stan",
    "href": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html#using-stan",
    "title": "Bayesian Network for US Coffee Tasting Data Using R & Stan",
    "section": "Using Stan",
    "text": "Using Stan\n\ncolnames(nona_allcat)\n\n [1] \"gender\"                 \"age\"                    \"cup_per_day\"           \n [4] \"home_brew_pour_over\"    \"home_brew_french_press\" \"home_brew_espresso\"    \n [7] \"home_brew_mr_coffee\"    \"home_brew_pods\"         \"home_brew_instant\"     \n[10] \"home_brew_bean2cup\"     \"home_brew_cold_brew\"    \"home_brew_cometeer\"    \n[13] \"home_brew_other\"        \"favorite_coffee_drink\"  \"roast_preference\"      \n[16] \"expertise\"              \"favorite_abcd\"         \n\nglimpse(nona_allcat)\n\nRows: 3,245\nColumns: 17\n$ gender                 &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\",…\n$ age                    &lt;chr&gt; \"over44\", \"25-34 years old\", \"35-44 years old\",…\n$ cup_per_day            &lt;chr&gt; \"2\", \"2\", \"one_or_less\", \"three_or_more\", \"2\", …\n$ home_brew_pour_over    &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,…\n$ home_brew_french_press &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,…\n$ home_brew_espresso     &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,…\n$ home_brew_mr_coffee    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ home_brew_pods         &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n$ home_brew_instant      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ home_brew_bean2cup     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ home_brew_cold_brew    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\n$ home_brew_cometeer     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ home_brew_other        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,…\n$ favorite_coffee_drink  &lt;chr&gt; \"drip\", \"pourover\", \"other\", \"pourover\", \"cappu…\n$ roast_preference       &lt;chr&gt; \"light\", \"light\", \"light\", \"dark\", \"light\", \"li…\n$ expertise              &lt;dbl&gt; 8, 10, 6, 7, 4, 8, 2, 7, 8, 7, 7, 6, 8, 6, 8, 8…\n$ favorite_abcd          &lt;fct&gt; Coffee B, Coffee A, Coffee D, Coffee A, Coffee …\n\ncolnames(nona_noref)\n\n [1] \"home_brew_french_press\" \"home_brew_espresso\"     \"home_brew_mr_coffee\"   \n [4] \"home_brew_pods\"         \"home_brew_instant\"      \"home_brew_bean2cup\"    \n [7] \"home_brew_cold_brew\"    \"home_brew_cometeer\"     \"home_brew_other\"       \n[10] \"favorite_abcd\"          \"female\"                 \"gen_other\"             \n[13] \"thirty544\"              \"over44\"                 \"under24\"               \n[16] \"cup_1orless\"            \"cup_3ormore\"            \"latte\"                 \n[19] \"other\"                  \"drip\"                   \"cappuccino\"            \n[22] \"espresso\"               \"cortado\"                \"americano\"             \n[25] \"roast_light\"            \"roast_dark\"             \"expert7\"               \n[28] \"expert6\"                \"expert8\"                \"expert4\"               \n[31] \"expert3\"                \"expert2\"                \"expert1\"               \n[34] \"expert9\"                \"expert10\"              \n\nnona_allcat &lt;- nona_allcat |&gt;\n  mutate(\n    gender = case_when(\n      gender == \"Male\" ~ 1,\n      gender == \"Female\" ~ 2,\n      gender == \"Other\" ~ 3\n    ),\n    age = case_when(\n      age == \"under24\" ~ 1,\n      age == \"25-34 years old\" ~ 2,\n      age == \"35-44 years old\" ~ 3,\n      age == \"over44\" ~ 4\n    ),\n    cup_per_day = case_when(\n      cup_per_day == \"one_or_less\" ~ 1,\n      cup_per_day == \"2\" ~ 2,\n      cup_per_day == \"three_or_more\" ~ 3\n    ),\n    favorite_coffee_drink = case_when(\n      favorite_coffee_drink == \"pourover\" ~ 1,\n      favorite_coffee_drink == \"latte\" ~ 2,\n      favorite_coffee_drink == \"other\" ~ 3,\n      favorite_coffee_drink == \"drip\" ~ 4,\n      favorite_coffee_drink == \"cappuccino\" ~ 5,\n      favorite_coffee_drink == \"espresso\" ~ 6,\n      favorite_coffee_drink == \"cortado\" ~ 7,\n      favorite_coffee_drink == \"americano\" ~ 8\n    ),\n    roast_preference = case_when(\n      roast_preference == \"light\" ~ 1,\n      roast_preference == \"medium\" ~ 2,\n      roast_preference == \"dark\" ~ 3\n    ),\n    expertise = case_when(\n      expertise == \"1\" ~ 1,\n      expertise == \"2\" ~ 2,\n      expertise == \"3\" ~ 3,\n      expertise == \"4\" ~ 4,\n      expertise == \"5\" ~ 5,\n      expertise == \"6\" ~ 6,\n      expertise == \"7\" ~ 7,\n      expertise == \"8\" ~ 8,\n      expertise == \"9\" ~ 9,\n      expertise == \"10\" ~ 10\n    ),\n    favorite_abcd = case_when(\n      favorite_abcd == \"Coffee A\" ~ 1,\n      favorite_abcd == \"Coffee B\" ~ 2,\n      favorite_abcd == \"Coffee C\" ~ 3,\n      favorite_abcd == \"Coffee D\" ~ 4\n    )\n  )\n\n\nnona_noref_recode &lt;- nona_noref |&gt;\n  mutate(\n    across(\n      -favorite_abcd,\n      ~case_when(\n        .x == 0 ~ -1,\n        TRUE ~ .x\n      )\n    )\n  )\n\n\nstan_allcat_list &lt;- list(\n  J = nrow(nona_allcat[,-1]),\n  y_cat = count(nona_allcat, favorite_abcd) |&gt; nrow(),\n  roast_preference_cat = count(nona_allcat, roast_preference) |&gt; nrow(),\n  expert_cat = count(nona_allcat, expertise) |&gt; nrow(),\n  cup_per_day_cat = count(nona_allcat, cup_per_day) |&gt; nrow(),\n  favorite_coffee_drink_cat = count(nona_allcat, favorite_coffee_drink) |&gt; nrow(),\n  gender_cat = count(nona_allcat, gender) |&gt; nrow(),\n  age_cat = count(nona_allcat, age) |&gt; nrow(),\n\n  Y = nona_allcat$favorite_abcd,\n  roast_preference = nona_allcat$roast_preference,\n  expert = nona_allcat$expertise,\n  favorite_coffee_drink = nona_allcat$favorite_coffee_drink,\n  cup_per_day = nona_allcat$cup_per_day,\n  home_brew_pourover = nona_allcat$home_brew_pour_over,\n  home_brew_french_press = nona_allcat$home_brew_french_press,\n  home_brew_espresso = nona_allcat$home_brew_espresso,\n  home_brew_mr_coffee = nona_allcat$home_brew_mr_coffee,\n  home_brew_pods = nona_allcat$home_brew_pods,\n  home_brew_instant = nona_allcat$home_brew_instant,\n  home_brew_bean2cup = nona_allcat$home_brew_bean2cup,\n  home_brew_cold_brew = nona_allcat$home_brew_cold_brew,\n  home_brew_cometeer = nona_allcat$home_brew_cometeer,\n  home_brew_other = nona_allcat$home_brew_other,\n  age = nona_allcat$age,\n  gender = nona_allcat$gender  \n)\n\nglimpse(stan_allcat_list)\n\nList of 25\n $ J                        : int 3245\n $ y_cat                    : int 4\n $ roast_preference_cat     : int 3\n $ expert_cat               : int 10\n $ cup_per_day_cat          : int 3\n $ favorite_coffee_drink_cat: int 8\n $ gender_cat               : int 3\n $ age_cat                  : int 4\n $ Y                        : num [1:3245] 2 1 4 1 1 4 4 4 3 2 ...\n $ roast_preference         : num [1:3245] 1 1 1 3 1 1 2 1 2 3 ...\n $ expert                   : num [1:3245] 8 10 6 7 4 8 2 7 8 7 ...\n $ favorite_coffee_drink    : num [1:3245] 4 1 3 1 5 1 2 8 6 7 ...\n $ cup_per_day              : num [1:3245] 2 2 1 3 2 2 2 1 3 2 ...\n $ home_brew_pourover       : num [1:3245] 1 1 1 1 0 1 0 0 1 0 ...\n $ home_brew_french_press   : num [1:3245] 1 0 0 1 0 0 0 0 1 0 ...\n $ home_brew_espresso       : num [1:3245] 1 1 0 0 1 0 0 0 1 1 ...\n $ home_brew_mr_coffee      : num [1:3245] 0 0 0 0 0 0 0 0 0 0 ...\n $ home_brew_pods           : num [1:3245] 0 0 0 1 1 0 0 0 0 0 ...\n $ home_brew_instant        : num [1:3245] 0 0 0 0 0 0 0 0 0 0 ...\n $ home_brew_bean2cup       : num [1:3245] 0 0 0 0 0 0 0 0 0 0 ...\n $ home_brew_cold_brew      : num [1:3245] 0 0 0 0 0 0 0 0 1 0 ...\n $ home_brew_cometeer       : num [1:3245] 0 0 0 0 0 1 0 0 0 0 ...\n $ home_brew_other          : num [1:3245] 0 0 0 0 0 0 1 1 0 0 ...\n $ age                      : num [1:3245] 4 2 3 4 3 4 3 3 3 2 ...\n $ gender                   : num [1:3245] 1 1 1 1 1 1 1 2 1 1 ...\n\n\n\nstan_list &lt;- list(\n  J = nrow(nona_noref_recode[,-1]),\n  I = ncol(nona_noref_recode[,-1]),\n  K = count(nona_noref_recode, favorite_abcd) |&gt; nrow(),\n\n  female = nona_noref_recode$female,\n  gen_other = nona_noref_recode$gen_other,\n\n  thirty544 = nona_noref_recode$thirty544,\n  over44 = nona_noref_recode$over44,\n  under24 = nona_noref_recode$under24,\n\n\n  home_brew_french_press = nona_noref_recode$home_brew_french_press,\n  home_brew_espresso = nona_noref_recode$home_brew_espresso,\n  home_brew_mr_coffee = nona_noref_recode$home_brew_mr_coffee,\n  home_brew_pods = nona_noref_recode$home_brew_pods,\n  home_brew_instant = nona_noref_recode$home_brew_instant,\n  home_brew_bean2cup = nona_noref_recode$home_brew_bean2cup,\n  home_brew_cold_brew = nona_noref_recode$home_brew_cold_brew,\n  home_brew_cometeer = nona_noref_recode$home_brew_cometeer,\n  home_brew_other = nona_noref_recode$home_brew_other,\n  \n  latte = nona_noref_recode$latte,\n  otherdrink = nona_noref_recode$other,\n  drip = nona_noref_recode$drip,\n  cappuccino = nona_noref_recode$cappuccino,\n  espresso = nona_noref_recode$espresso,\n  cortado = nona_noref_recode$cortado,\n\n  cup_1orless = nona_noref_recode$cup_1orless,\n  cup_3ormore = nona_noref_recode$cup_3ormore,\n\n\n  americano = nona_noref_recode$americano,         \n  roast_light = nona_noref_recode$roast_light,\n  roast_dark = nona_noref_recode$roast_dark,\n\n  expert7 = nona_noref_recode$expert7,\n  expert6 = nona_noref_recode$expert6,\n  expert8 = nona_noref_recode$expert8,\n  expert4 = nona_noref_recode$expert4,\n  expert3 = nona_noref_recode$expert3,\n  expert2 = nona_noref_recode$expert2,\n  expert1 = nona_noref_recode$expert1,\n  expert9 = nona_noref_recode$expert9,\n  expert10 = nona_noref_recode$expert10,\n\n  Y = nona_noref_recode$favorite_abcd\n)\n\n# glimpse(stan_list)\n\n\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"random_scripts\", \"updated_stan_model.stan\"))\n\n# mod$format(\n#   canonicalize = list(\"deprecations\"),\n#   overwrite_file = TRUE,\n#   backup = FALSE\n# )\n\n\nfit &lt;- mod$sample(\n  data = stan_list,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000,\n  # adapt_delta = .90,\n  chains = 4,\n  # step_size = .01,\n  parallel_chains = 8\n)\n\nsaveRDS(fit, \"coffee_tasting_bayes_net.RDS\")\n\n# fit &lt;- read_rds(here::here(\"random_data\", \"coffee_tasting_bayes_net.RDS\"))\n\n\nfit$output()[[1]]\n\nfit$diagnostic_summary()\n\n\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\n\n\nbn_converge |&gt;\n  arrange(rhat) |&gt;\n  mutate(\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"theta\"\n    )\n  ) |&gt; \n  mutate(\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  select(\n    variable,\n    mean,\n    sd\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"predictors\"\n    )\n  )\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"^a_\"\n    ) |\n    str_detect(\n      variable,\n      \"^b_\"\n    )\n  ) |&gt; \n  mutate(\n    across(\n      -variable,\n      ~exp(.x)\n    ),\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  select(\n    variable,\n    mean,\n    sd\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"random_scripts\", \"all_category_model.stan\"))\n\nfit &lt;- mod$sample(\n  data = stan_allcat_list,\n  seed = 12345,\n  iter_warmup = 1000, #2000\n  iter_sampling = 1000, #2000\n  # adapt_delta = .90,\n  chains = 4,\n  # step_size = .01,\n  parallel_chains = 8\n)\n\nfit$diagnostic_summary()\n\n\n\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\n\nbn_converge |&gt;\n  arrange(rhat) |&gt;\n  mutate(\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"^a_\"\n    ) |\n    str_detect(\n      variable,\n      \"^b_\"\n    ) |\n    str_detect(\n      variable,\n      \"log_lik\"\n    )\n  ) |&gt;\n  select(\n    variable,\n    log_odds = mean,\n    sd\n  ) |&gt;\n  mutate(\n    prob_use = plogis(log_odds),\n    prob_notuse = 1 - prob_use,\n    #prob = exp(log_odds)/exp(1 + log_odds),\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  )  |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\nInterpretations for select parameters\nThe alpha parameter for predicting participants’ favorite coffee showed that participants who prefer medium roasted coffee and labeled themselves as a 5 on expertise had 1.14 times greater odds of enjoying coffee C than not enjoying it.\nalpha = compared to zero\nFor example, b_roast_light_yb_roast_light_y is the change in log-odds for a given category when the roast level is light. Similarly, b_expert1_yb_expert1_y to b_expert10_yb_expert10_y represent the effects of being in each expert level on the log-odds of a particular category, again without any specific reference category.\n\nalphas &lt;- c(\n\"a_latte\",\n\"a_otherdrink\",\n\"a_drip\",\n\"a_cappuccino\",\n\"a_espresso\",\n\"a_cortado\",\n\"a_americano\",\n\"a_home_frenchpress\",\n\"a_home_espresso\",\n\"a_home_mrcoffee\",\n\"a_home_pods\",\n\"a_home_instant\",\n\"a_home_bean2cup\",\n\"a_home_coldbrew\",\n\"a_home_cometeer\",\n\"a_home_other\",\n\"a_cup_1orless\",\n\"a_cup_3ormore\",\n\"a_roast_light\",\n\"a_roast_dark\",\n\"a_expert1\",\n\"a_expert2\",\n\"a_expert3\",\n\"a_expert4\",\n\"a_expert6\",\n\"a_expert7\",\n\"a_expert8\",\n\"a_expert9\",\n\"a_expert10\",\n\"a_y[1]\",\n\"a_y[2]\", \n\"a_y[3]\",\n\"a_y[4]\"\n)\n\nb_latte &lt;- c(\n  \"b_female_latte\",\n  \"b_gen_other_latte\",\n  \"b_thirty544_latte\",\n  \"b_over44_latte\",\n  \"b_under24_latte\",\n  \"b_home_frenchpress_latte\",\n  \"b_home_espresso_latte\",\n  \"b_home_mrcoffee_latte\",\n  \"b_home_pods_latte\",\n  \"b_home_instant_latte\",\n  \"b_home_bean2cup_latte\",\n  \"b_home_coldbrew_latte\",\n  \"b_home_cometeer_latte\",\n  \"b_home_other_latte\"\n)\n\nalphas &lt;- fit$draws(alphas) |&gt; as_draws_matrix()\nbeta_var_latte &lt;- fit$draws(b_latte) |&gt; as_draws_matrix()\n\n\nmcmc_trace(alphas)\nmcmc_trace(beta_var_latte)\n\n\nmcmc_areas(alphas)\nmcmc_areas(beta_var_latte)"
  },
  {
    "objectID": "index.html#quarto-extensions",
    "href": "index.html#quarto-extensions",
    "title": "Home",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension"
  },
  {
    "objectID": "posts/2024-11-04-closeread-temp-blanket/index.html",
    "href": "posts/2024-11-04-closeread-temp-blanket/index.html",
    "title": "My submission to Posit’s Closeread Competition",
    "section": "",
    "text": "When I saw that Posit had posted a blog post about storytelling with Quarto (see blog post here) using Closeread I became interested in trying out Closeread for this competition. This is my first time reading about Closeread and it sparked my interest in trying to tell a story using a quarto document. For my submission, I thought I would do something silly about a topic that is serious. I am going to try my hardest to write this in a serious way, but this is also probably going to dissolve into a delusional fever dream of a post. I wanted to show the weather differences in Los Angeles from 2013 to 2023 by creating a story of global warming while showing visualizations of the temperature differences using “temperature blankets”. If you know anything about me, the first thing you recognize is art != JP so instead I thought I would create a temperature blanket using ggplot2.\nFor the competition I decided to do everything in R because I knew I was going to be mess around a lot using the theme() function in ggplot2. So below is going to be a walkthough of my thought process for creating the Closeread story. Then I will probably create a Closeread specific GitHub repo.\n\nLoading the Data\nI’m going to start by loading the tidyverse package and reading the data. I specifically wanted data from Los Angeles because I live here and thought I would compare a ten-year difference since I also lived here in 2013. I got data here, but I have not looked over the data at all.\n\nlibrary(tidyverse)\n\ntemp &lt;- read_csv(here::here(\"posts/2024-11-04-closeread-temp-blanket\", \"weather_data_la2013_la2023.csv\")) |&gt;\n  janitor::clean_names()\n\nSo I decided it is probably best to make sure everything is correct in the data.\n\ntemp |&gt;\n  count(city)\n\n# A tibble: 1 × 2\n  city            n\n  &lt;chr&gt;       &lt;int&gt;\n1 Los Angeles   730\n\ntemp |&gt;\n  inspectdf::inspect_na()\n\n# A tibble: 12 × 3\n   col_name                cnt  pcnt\n   &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;\n 1 daytime_h_m               7 0.959\n 2 item_number               0 0    \n 3 date                      0 0    \n 4 location_index            0 0    \n 5 city                      0 0    \n 6 state_region              0 0    \n 7 country                   0 0    \n 8 high_temperature_f        0 0    \n 9 average_temperature_f     0 0    \n10 low_temperature_f         0 0    \n11 rain_in                   0 0    \n12 snow_in                   0 0    \n\n\nOkay, so everything looks okay to me. While there are plenty of data points to look at, I think I am going to focus on the average temperature to create my temperature blankets. I also just want to point out that all temperatures will be in Farenheit. Before making my visualization, I want to break down my date category into years, months, and days. This will be easier to use facet_wrap() to separate my years.\n\ntemp &lt;- temp |&gt;\n  mutate(date2 = date) |&gt;\n  separate(\n    date2,\n    into = c(\n      \"year\", \"month\", \"day\"\n    ),\n    sep = \"-\"\n  )\n\nNow, we can focus on creating our plot. Interestingly, I have never had to make one axis on my plots be set to an amount that would range across the entirety of the axis. This was definitely one of those times where I just tried something and BAM! it worked. I set my y-axis to 1 and it worked. We will not focus on the values for the y-axis because they don’t make any sense, but I did try some other values. The value that you choose on the y-axis does not matter, especially for this visualization because we are going to remove the axis titles and text.\n\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = average_temperature_f\n    )\n  ) +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\nFor some reason, I am not a fan of the blankets being horizontal, so I’m going to change the orientation of them.\n\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = average_temperature_f\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\nWe need a color scale. This will obviously be a manual scale so after some googling I found this scale project sheet. Shoutout to the Craft Warehouse because this was the easiest scale to follow (for me) and the large balls of yarn allowed me to find color codes that matched the yarn fairly well.\n\ntemp &lt;- temp |&gt;\n  mutate(\n    temp_color = case_when(\n      average_temperature_f &gt; 96 ~ \"96+\", #cherry red\n      average_temperature_f &gt;= 89 & average_temperature_f &lt; 96 ~ \"89-95\", #really red\n      average_temperature_f &gt;= 82 & average_temperature_f &lt; 89 ~ \"82-88\", #carrot\n      average_temperature_f &gt;= 75 & average_temperature_f &lt; 82 ~ \"75-81\", #canary\n      average_temperature_f &gt;= 68 & average_temperature_f &lt; 75 ~ \"68-74\", #yellow\n      average_temperature_f &gt;= 61 & average_temperature_f &lt; 68 ~ \"61-67\", #green apple\n      average_temperature_f &gt;= 54 & average_temperature_f &lt; 61 ~ \"54-60\", #porcelain blue\n      average_temperature_f &gt;= 47 & average_temperature_f &lt; 54 ~ \"47-53\", #teal\n      average_temperature_f &gt;= 40 & average_temperature_f &lt; 47 ~ \"40-46\", #alaskan blue\n      average_temperature_f &gt;= 33 & average_temperature_f &lt; 40 ~ \"33-39\", #cobalt\n      average_temperature_f &gt;= 26 & average_temperature_f &lt; 33 ~ \"26-32\", #thistle\n      average_temperature_f &lt; 26 ~ \"Below 26\" #purple\n    ),\n    month_name = case_when(\n      month == \"01\" ~ \"Jan\",\n      month == \"02\" ~ \"Feb\",\n      month == \"03\" ~ \"Mar\",\n      month == \"04\" ~ \"Apr\",\n      month == \"05\" ~ \"May\",\n      month == \"06\" ~ \"Jun\",\n      month == \"07\" ~ \"Jul\",\n      month == \"08\" ~ \"Aug\",\n      month == \"09\" ~ \"Sept\",\n      month == \"10\" ~ \"Oct\",\n      month == \"11\" ~ \"Nov\",\n      month == \"12\" ~ \"Dec\"\n    ),\n    across(\n      c(\n        temp_color,\n        month_name\n      ),\n      ~as.factor(.x)\n    ),\n    temp_color = fct_relevel(\n      temp_color,\n      \"96+\",\n      \"89-95\",\n      \"82-88\",\n      \"75-81\",\n      \"68-74\",\n      \"61-67\",\n      \"54-60\",\n      \"47-53\",\n      \"40-46\",\n      \"33-39\",\n      \"26-32\",\n      \"Below 26\"\n    )\n  )\n\n\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  geom_text(\n    data = temp |&gt;\n    filter(\n        str_detect(\n          date,\n        \"-01$\"\n      )\n    ),\n    aes(\n      label = month_name\n    ),\n    nudge_y = -.54\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  ggmap::theme_nothing() +\n  NULL\n\n\n\n\n\n\n\n\n\ntemp |&gt;\n  group_by(year) |&gt;\n  mutate(\n    first = first(item_number)\n  ) |&gt;\n  distinct(\n    first,\n    .keep_all = TRUE\n    ) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  # geom_text(\n  #   data = temp |&gt;\n  #   filter(\n  #       str_detect(\n  #         date,\n  #       \"-01$\"\n  #     )\n  #   ),\n  #   aes(\n  #     label = month_name\n  #   ),\n  #   nudge_y = -.54\n  # ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n\n  NULL\n\n\n\n\n\n\n\n\n\ntemp |&gt;\n  select(\n    item_number,\n    date,\n    year,\n    temp_color,\n    average_temperature_f\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; 10) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nmap(\n  seq(2, 365, 7),\n  ~temp |&gt;\n  select(\n    item_number,\n    date,\n    year,\n    temp_color,\n    average_temperature_f\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]\n\n\n\n\n\n\n\n\n\n\n[[15]]\n\n\n\n\n\n\n\n\n\n\n[[16]]\n\n\n\n\n\n\n\n\n\n\n[[17]]\n\n\n\n\n\n\n\n\n\n\n[[18]]\n\n\n\n\n\n\n\n\n\n\n[[19]]\n\n\n\n\n\n\n\n\n\n\n[[20]]\n\n\n\n\n\n\n\n\n\n\n[[21]]\n\n\n\n\n\n\n\n\n\n\n[[22]]\n\n\n\n\n\n\n\n\n\n\n[[23]]\n\n\n\n\n\n\n\n\n\n\n[[24]]\n\n\n\n\n\n\n\n\n\n\n[[25]]\n\n\n\n\n\n\n\n\n\n\n[[26]]\n\n\n\n\n\n\n\n\n\n\n[[27]]\n\n\n\n\n\n\n\n\n\n\n[[28]]\n\n\n\n\n\n\n\n\n\n\n[[29]]\n\n\n\n\n\n\n\n\n\n\n[[30]]\n\n\n\n\n\n\n\n\n\n\n[[31]]\n\n\n\n\n\n\n\n\n\n\n[[32]]\n\n\n\n\n\n\n\n\n\n\n[[33]]\n\n\n\n\n\n\n\n\n\n\n[[34]]\n\n\n\n\n\n\n\n\n\n\n[[35]]\n\n\n\n\n\n\n\n\n\n\n[[36]]\n\n\n\n\n\n\n\n\n\n\n[[37]]\n\n\n\n\n\n\n\n\n\n\n[[38]]\n\n\n\n\n\n\n\n\n\n\n[[39]]\n\n\n\n\n\n\n\n\n\n\n[[40]]\n\n\n\n\n\n\n\n\n\n\n[[41]]\n\n\n\n\n\n\n\n\n\n\n[[42]]\n\n\n\n\n\n\n\n\n\n\n[[43]]\n\n\n\n\n\n\n\n\n\n\n[[44]]\n\n\n\n\n\n\n\n\n\n\n[[45]]\n\n\n\n\n\n\n\n\n\n\n[[46]]\n\n\n\n\n\n\n\n\n\n\n[[47]]\n\n\n\n\n\n\n\n\n\n\n[[48]]\n\n\n\n\n\n\n\n\n\n\n[[49]]\n\n\n\n\n\n\n\n\n\n\n[[50]]\n\n\n\n\n\n\n\n\n\n\n[[51]]\n\n\n\n\n\n\n\n\n\n\n[[52]]"
  },
  {
    "objectID": "posts/2021-04-30-twitter-conference-presentation/index.html",
    "href": "posts/2021-04-30-twitter-conference-presentation/index.html",
    "title": "Twitter Conference Presentation",
    "section": "",
    "text": "I thought I’d talk about the analyses I conducted for my submission to the American Public Health Association Physical Activity Section’s Twitter Conference. I thought it was a fun opportunity to disseminate some analyses that I conducted using public data from the County Health Rankings and Roadmap to examine what factors are associated with leisure-time physical activity (LTPA) in counties in California from 2016 to 2020. If you are interested in the presentation itself, you can follow it here.\nPhysical activity is important as its related to many physical and mental health conditions. It is also a health behavior that can be modified slightly easier than other health behaviors. While not as beneficial as extended periods of exercise, even walking for leisure can be beneficial for one’s health. I was predominately interested in California because I wanted to know how much variation there was between the counties. For instance, there are areas like San Francisco county and Los Angeles County, which may be seen as hubs for cultures of being physically active, but what about counties throughout Central California. I’m also interested in LTPA engagement because this health behavior has several social determinants of health that impact how much LTPA individuals can engage in. The social determinant that I’m most interested in is the role that access to recreational facilities and parks have on counties’ LTPA engagement. Since I was interested in looking at variation between counties while also examining the longitudinal association between access and LTPA, I decided to create a two-level multilevel model with time (level 1) nested within counties (level 2).\n\nPackages ued\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'inspectdf' was built under R version 4.3.2\n\n\nWarning: package 'psych' was built under R version 4.3.1\n\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\nWarning: package 'lme4' was built under R version 4.3.1\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nWarning: package 'lmerTest' was built under R version 4.3.3\n\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nWarning: package 'optimx' was built under R version 4.3.3\n\n\nWarning: package 'ggmap' was built under R version 4.3.3\n\n\nℹ Google's Terms of Service: &lt;https://mapsplatform.google.com&gt;\n  Stadia Maps' Terms of Service: &lt;https://stadiamaps.com/terms-of-service/&gt;\n  OpenStreetMap's Tile Usage Policy: &lt;https://operations.osmfoundation.org/policies/tiles/&gt;\nℹ Please cite ggmap if you use it! Use `citation(\"ggmap\")` for details.\n\n\nWarning: package 'maps' was built under R version 4.3.3\n\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nWarning: package 'RColorBrewer' was built under R version 4.3.1\n\n\nWarning: package 'ggrepel' was built under R version 4.3.3\n\n\nWarning: package 'gganimate' was built under R version 4.3.3\n\n\nWarning: package 'transformr' was built under R version 4.3.3\n\n\n\n\n\n\n\n\n\n\n\nFunctions\nBefore beginning I created a simple function to get the intraclass correlation coefficient (ICC). I also included a function to get data from the 5 years of County Health Rankings data. The function to get the ICC from random-intercept models is the between county variation divided by the total variation between counties and within counties. This gives you information about how much of the variation in the model can be attributed to differences between counties regarding your outcome. Random-intercept models were tested because I was only interested in knowing the variation in counties’ LTPA engagement. There were also not enough points for each county to test individual slopes for each county (random-slopes model).\n\n\nWarning: package 'curl' was built under R version 4.3.3\n\n\nUsing libcurl 8.3.0 with Schannel\n\n\n\nAttaching package: 'curl'\n\n\nThe following object is masked from 'package:readr':\n\n    parse_date\n\n\nI also made some slight changes to my data. The first was to get rid of the estimates for each state and only focus on estimates from the county. I also wanted to treat year as a continuous variable in my models but wanted to keep the year variable as a factor too. Then after filtering to only examine California counties I used the str_replace_all function from the stringr package to get rid of the county name after each observation. This was to make it easier to join with map data from the maps package. Lastly, I made the counties title case to also make joining the observations easier.\n\n\nModels\nNow when running the first model, I was first interested in examining if there was an increase in LTPA engagement in all California counties from 2016 to 2020. From the finding below, it shows that in California, there was a decrease in LTPA over that time. It’s also important to note that lmerTest and lme4 both have a lmer function. By namespacing them with two colons, you can see that the summary information is slightly different.\n\npreliminary_ltpa_long &lt;- lmerTest::lmer(ltpa_percent ~ year_num + (1 | county_fips_code), data = ca,\n                              REML = FALSE)\nsummary(preliminary_ltpa_long)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: ltpa_percent ~ year_num + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1427.9   1442.5   -709.9   1419.9      286 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6854 -0.3709  0.0361  0.5377  1.7084 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 7.187    2.681   \n Residual                     5.174    2.275   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n              Estimate Std. Error         df t value            Pr(&gt;|t|)    \n(Intercept) 1923.40172  190.60225  231.84941  10.091 &lt;0.0000000000000002 ***\nyear_num      -0.91276    0.09445  231.84760  -9.664 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr)\nyear_num -1.000\n\nprelim_ltpa_lmer &lt;- lme4::lmer(ltpa_percent ~ year_num +(1 | county_fips_code), data = ca,\n                              REML = FALSE)\nsummary(prelim_ltpa_lmer)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: ltpa_percent ~ year_num + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1427.9   1442.5   -709.9   1419.9      286 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6854 -0.3709  0.0361  0.5377  1.7084 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 7.187    2.681   \n Residual                     5.174    2.275   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept) 1923.40172  190.60225  10.091\nyear_num      -0.91276    0.09445  -9.664\n\nCorrelation of Fixed Effects:\n         (Intr)\nyear_num -1.000\n\nltpa_null_icc &lt;- as_tibble(VarCorr(preliminary_ltpa_long))\nltpa_null_icc\n\n# A tibble: 2 × 5\n  grp              var1        var2   vcov sdcor\n  &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 county_fips_code (Intercept) &lt;NA&gt;   7.19  2.68\n2 Residual         &lt;NA&gt;        &lt;NA&gt;   5.17  2.27\n\ncounty_icc_2level(ltpa_null_icc)\n\n[1] 0.5814093\n\n\nAlong with the fixed effects, we also got our random effects for both differences found between counties for LTPA engagement and differences within counties for LTPA engagement. This shows that there was a good amount of variation between counties (σ2 = 7.19) but also a large amount of variation within each county in California (σ2 = 5.17). Using the function to calculate the ICC, it found that county differences attributed to 58% of the variation in LTPA engagement. Something that should be considered is the potential for heteroscedastic residual variance at level 1. There is also the issue that the residuals could suggest spatial autocorrelation or clustering within these counties. Maybe I’ll create something on these soon. But for the time being, lets move on to what was found for the twitter conference.\n\nltpa_long_access &lt;- lmer(ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent +\n                           access_pa_percent + (1 | county_fips_code), data = ca,\n                         REML = FALSE)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nanova(preliminary_ltpa_long, ltpa_long_access)\n\nData: ca\nModels:\npreliminary_ltpa_long: ltpa_percent ~ year_num + (1 | county_fips_code)\nltpa_long_access: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + access_pa_percent + (1 | county_fips_code)\n                      npar    AIC    BIC  logLik deviance  Chisq Df\npreliminary_ltpa_long    4 1427.9 1442.5 -709.93   1419.9          \nltpa_long_access         9 1237.4 1270.4 -609.69   1219.4 200.47  5\n                                 Pr(&gt;Chisq)    \npreliminary_ltpa_long                          \nltpa_long_access      &lt; 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nother_var &lt;- lmer(ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + (1 | county_fips_code), data = ca,\n                         REML = FALSE)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nanova(other_var, ltpa_long_access)\n\nData: ca\nModels:\nother_var: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + (1 | county_fips_code)\nltpa_long_access: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + access_pa_percent + (1 | county_fips_code)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nother_var           8 1240.1 1269.4 -612.04   1224.1                       \nltpa_long_access    9 1237.4 1270.4 -609.69   1219.4 4.6883  1    0.03037 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith the inclusion of several predictors for fixed effects, a likelihood ratio test was conducted to see if the inclusion of these fixed effects revealed a significantly better fitting model. The inclusion of these predictors revealed a better fitting model. It would probably be better to see if the inclusion of one variable of interest, such as access, resulted in a better fitting model than a model with the other social determinants of health. As can be see here, the likelihood ratio test of including only access still resulted in a signifcantly better fitting model.\n\nsummary(ltpa_long_access)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income +  \n    rural_percent + access_pa_percent + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1237.4   1270.4   -609.7   1219.4      281 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8388 -0.5546  0.0010  0.6179  2.4921 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 1.286    1.134   \n Residual                     3.139    1.772   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n                              Estimate     Std. Error             df t value\n(Intercept)             1138.319788532  193.185926311  289.534084648   5.892\nyear_num                  -0.517087679    0.096244053  289.365968612  -5.373\nviolent_crime             -0.001434185    0.001190066   82.732734919  -1.205\nobesity_percent           -0.602338792    0.043760246  262.840283632 -13.765\nmedian_household_income    0.000004822    0.000014701  102.255262863   0.328\nrural_percent             -0.012178273    0.007671490   63.272282975  -1.587\naccess_pa_percent          0.027194696    0.012124625  153.692013407   2.243\n                                    Pr(&gt;|t|)    \n(Intercept)                     0.0000000106 ***\nyear_num                        0.0000001597 ***\nviolent_crime                         0.2316    \nobesity_percent         &lt; 0.0000000000000002 ***\nmedian_household_income               0.7436    \nrural_percent                         0.1174    \naccess_pa_percent                     0.0263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) yer_nm vlnt_c obsty_ mdn_h_ rrl_pr\nyear_num    -1.000                                   \nviolent_crm  0.116 -0.118                            \nobsty_prcnt  0.489 -0.494 -0.100                     \nmdn_hshld_n  0.586 -0.589  0.203  0.452              \nrural_prcnt  0.260 -0.264  0.135  0.203  0.447       \naccss_p_prc -0.147  0.142 -0.018  0.054 -0.341  0.158\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nThe model summary suggests that the fixed effect of access on LTPA engagement was significantly associated. The thing that stands out the most here is that the inclusion of the predictors resulted in more variation within counties than between counties. So lets look into that more closely.\n\nltpa_access_icc &lt;- as_tibble(VarCorr(ltpa_long_access))\nltpa_access_icc\n\n# A tibble: 2 × 5\n  grp              var1        var2   vcov sdcor\n  &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 county_fips_code (Intercept) &lt;NA&gt;   1.29  1.13\n2 Residual         &lt;NA&gt;        &lt;NA&gt;   3.14  1.77\n\ncounty_icc_2level(ltpa_access_icc)\n\n[1] 0.2906253\n\n\nThe ICC suggests that 29% of the variation explained is from differences between counties. It is also beneficial to look at all of this through visuals.\n\n\nVisuals Prep\nBelow we’ll start by using the maps package to get county-level data of the contiguous United States. The steps below were to make sure this data frame joined with the county health rankings data we had created previously.\n\n\nVisualizing model\nOne way to visualize the variation between counties in our final model (ltpa_long_access) is to use a caterpillar plot. This allows you to view variation in the residuals of each county for your outcome. From the visual, you can see the differences between Humboldt County and Tehama County.\n\nmain_effects_var &lt;- ranef(ltpa_long_access, condVar = TRUE)\n\nmain_effects_var &lt;- as.data.frame(main_effects_var)\n\nmain_effects_var &lt;- main_effects_var %&gt;% \n  mutate(main_effects_term = term,\n         county_fips_code = grp,\n         main_effects_diff = condval,\n         main_effects_se = condsd,\n         county_fips_code = as.numeric(county_fips_code))\n\nmain_effects_var$no_name_county &lt;- unique(ca$no_name_county)\n\nmain_effects_var %&gt;% \n  ggplot(aes(fct_reorder(no_name_county, main_effects_diff), main_effects_diff)) +\n  geom_errorbar(aes(ymin = main_effects_diff + qnorm(0.025)*main_effects_se,\n                  ymax = main_effects_diff + qnorm(0.975)*main_effects_se)) +\n  geom_point(aes(color = no_name_county)) +\n  coord_flip() +\n  labs(x = ' ',\n     y = 'Differences in Leisure-time Physical Activity',\n     title = 'Variation in Leisure-time Physical Activity\\nAcross California Counties') +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\nThis plot shows the fixed effect of access and LTPA across the various years.\n\nca %&gt;% \n  mutate(year = as.factor(year)) %&gt;% \n  ggplot(aes(access_pa_percent, ltpa_percent)) +\n  geom_point(aes(color = year)) +\n  geom_smooth(color = 'dodgerblue',\n            method = 'lm', se = FALSE, size = 1) +\n  theme(legend.title = element_blank()) +\n  labs(x = 'Access to Physical Activity Opportunities',\n       y = 'Leisure-time Physical Activity',\n       title = 'The Statewide Association of Access\\nand Physical Activity')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, a gif of the change of LTPA from 2016 to 2020.\n\nlibrary(gganimate)\n\nca_animate &lt;- ca_visual %&gt;%\n  ggplot(aes(frame = year,\n             cumulative = TRUE)) +\n  geom_polygon(aes(x = long, y = lat, \n                   group = group, \n                   fill = ltpa_percent),\n               color = 'black') +\n  scale_fill_gradientn(colors = brewer.pal(n = 5, name = 'RdYlGn')) + \n  theme_classic() +\n  transition_time(year) +\n  labs(x = 'Longitude',\n       y = 'Latitude',\n       title = 'Leisure-time Physical Activity\\nChange Over Time',\n       subtitle = 'Year: {frame_time}') +\n  theme(legend.title = element_blank(),\n        legend.text = element_text(size = 12),\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 10),\n        plot.title = element_text(size = 20),\n        plot.subtitle = element_text(size = 18))\n\nlibrary(gifski)\n\nWarning: package 'gifski' was built under R version 4.3.3\n\nanimate(ca_animate, renderer = gifski_renderer())"
  },
  {
    "objectID": "posts/2021-04-30-grad-student-exit-surveys/index.html",
    "href": "posts/2021-04-30-grad-student-exit-surveys/index.html",
    "title": "Graduate Student Satisfaction Exit Surveys",
    "section": "",
    "text": "This post was originally designed because I was interested in working on student experience exit survey data from my department to see if there was a change from 2012 to 2015. These questions are given to every student that graduates from a graduate program at the University of Oregon (UO). This also applies for doctoral students that complete the requirements for a master’s degree on their way to their final PhD program. This data is open to any UO student, staff, or faculty member that has login information for this data.\nThis data ended up becoming a real time commitment as there was no efficient way to collect data from the pdf files for each College at the UO. An example can be seen here. One great resource for collecting data from pdfs was to use the pdftools package, but if you look at the example link provided above the UO Graduate School decided to color code cells in the table, which threw off any function to extract all the values in an efficient manner. Anyway…\nThe data and other existing data files can be found here. When I have some more free time, I may decide to join the other datasets to the student experience data to examine some more interesting questions regarding this data. But for now, lets look at the student experience data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntheme_set(theme_minimal())\n\nexit &lt;- read_csv(here::here(\"posts\", \"2021-04-30-grad-student-exit-surveys/exit_data.csv\")) %&gt;% \n  janitor::clean_names() \n\nRows: 103 Columns: 113\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): program\ndbl (112): year, number_respondents, fac_qual_ex, pro_qual_ex, money_sup_ex,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexit$program &lt;- str_replace_all(exit$program,\"_\", \" \")\nexit$program &lt;- str_to_title(exit$program)\n\nThese exit surveys have several questions that are broken down into percentages about how many of the students agreed or disagreed with the statement. For instance, from the pdf, the first statement is Quality of the faculty in a student’s department. So we can look at that with this first plot. At the same time, we can also look at the difference between the two years of data. In order to look at all the variables at the same time that have the starting string of fac_qual, I’ll use pivot_longer to collect any variable that has that variable string about faculty quality. Since the first and second table on the pdf refer to excellent or good or excellent levels of student satisfaction about faculty quality, I decided to filter out the excellent student satisfaction and move on with only student satisfaction that is either good or excellent.\n\nexit %&gt;% \n  pivot_longer(\n    matches(\n      \"^fac_qual\"\n    ),\n    names_to = \"fac_qual\",\n    values_to = \"fac_values\"\n  ) |&gt;\n  filter(fac_qual != \"fac_qual_ex\") %&gt;%\n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_col(aes(fill = as.factor(year)), position = \"dodge2\") +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"Specific Student Experience\",\n       caption = \"Ex = Excellent\") +\n  coord_flip() +\n  facet_wrap(~fac_qual) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nSo the first shot at making a visual for the two years looks a little cluttered because of using geom_col(). My first decision was to remove the columns and change those to points to make it a little less cluttered and clearer. I already enjoyed the way this looked better. I also decided to clean some things up by changing the names of the variables to better describe what the variables were assessing. I also decided to go back and change the programs to be title case and with spaces rather than underscores.\n\nexit %&gt;% \n  pivot_longer(\n    matches(\n      \"^fac_qual\"\n    ),\n    names_to = \"fac_qual\",\n    values_to = \"fac_values\"\n  ) |&gt;\n  filter(fac_qual != \"fac_qual_ex\") %&gt;%\n  mutate(fac_qual = recode(fac_qual, \"fac_qual_ex_good\" = \"Excellent/Good Faculty Quality\",\n                           \"fac_qual_fair_poor\" = \"Fair/Poor Faculty Quality\")) %&gt;% \n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  facet_wrap(~fac_qual) +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nJust in case anyone else is interested in this data, I also created a quick function to see how this visual looked like for other variables in the dataset. For instance, I’ll look at a couple of different variables.\n\nprogram_experience &lt;- function(name){\n  exit %&gt;% \n    pivot_longer(\n      matches(\n          {{name}}\n      )\n    ) |&gt;\n    filter(name != paste0({{name}}, \"_ex\") &\n             name != paste0({{name}}, \"_strong\")) %&gt;% \n  ggplot(aes(fct_reorder(program, value), value)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"\") +\n  coord_flip() +\n  facet_wrap(~name) +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n}\n\nBelow are all the variables from the dataset.\n\n\n  [1] \"year\"                          \"program\"                      \n  [3] \"number_respondents\"            \"fac_qual_ex\"                  \n  [5] \"pro_qual_ex\"                   \"money_sup_ex\"                 \n  [7] \"field_dev_pace_ex\"             \"advising_qual_ex\"             \n  [9] \"smart_community_ex\"            \"prof_dev_ex\"                  \n [11] \"equipment_ex\"                  \"grad_involve_ex\"              \n [13] \"research_opp_ex\"               \"grad_fair_assess_ex\"          \n [15] \"promote_inclu_ex\"              \"grant_train_ex\"               \n [17] \"teach_prep_ex\"                 \"grad_clear_assess_ex\"         \n [19] \"inter_sup_ex\"                  \"prof_ethic_train_ex\"          \n [21] \"fac_qual_ex_good\"              \"pro_qual_ex_good\"             \n [23] \"money_sup_ex_good\"             \"field_dev_pace_ex_good\"       \n [25] \"advising_qual_ex_good\"         \"smart_community_ex_good\"      \n [27] \"prof_dev_ex_good\"              \"equipment_ex_good\"            \n [29] \"grad_involve_ex_good\"          \"research_opp_ex_good\"         \n [31] \"grad_fair_assess_ex_good\"      \"promote_inclu_ex_good\"        \n [33] \"grant_train_ex_good\"           \"teach_prep_ex_good\"           \n [35] \"grad_clear_assess_ex_good\"     \"inter_sup_ex_good\"            \n [37] \"prof_ethic_train_ex_good\"      \"fac_qual_fair_poor\"           \n [39] \"pro_qual_fair_poor\"            \"money_sup_fair_poor\"          \n [41] \"field_dev_pace_fair_poor\"      \"advising_qual_fair_poor\"      \n [43] \"smart_community_fair_poor\"     \"prof_dev_fair_poor\"           \n [45] \"equipment_fair_poor\"           \"grad_involve_fair_poor\"       \n [47] \"research_opp_fair_poor\"        \"grad_fair_assess_fair_poor\"   \n [49] \"promote_inclu_fair_poor\"       \"grant_train_fair_poor\"        \n [51] \"teach_prep_fair_poor\"          \"grad_clear_assess_fair_poor\"  \n [53] \"inter_sup_fair_poor\"           \"prof_ethic_train_fair_poor\"   \n [55] \"encourage_agree\"               \"idea_resp_agree\"              \n [57] \"construct_feed_agree\"          \"time_feed_agree\"              \n [59] \"avail_agree\"                   \"career_sup_agree\"             \n [61] \"stu_equit_agree\"               \"ethic_emp_agree\"              \n [63] \"help_secure_fund_agree\"        \"help_prof_dev_agree\"          \n [65] \"publish_help_agree\"            \"encourage_intel_diff_agree\"   \n [67] \"comfort_talk_issue_agree\"      \"encourage_disagree\"           \n [69] \"idea_resp_disagree\"            \"construct_feed_disagree\"      \n [71] \"time_feed_disagree\"            \"avail_disagree\"               \n [73] \"career_sup_disagree\"           \"stu_equit_disagree\"           \n [75] \"ethic_emp_disagree\"            \"help_secure_fund_disagree\"    \n [77] \"help_prof_dev_disagree\"        \"publish_help_disagree\"        \n [79] \"encourage_intel_diff_disagree\" \"comfort_talk_issue_disagree\"  \n [81] \"collegial_strong\"              \"encouraging_strong\"           \n [83] \"supportive_strong\"             \"intel_open_strong\"            \n [85] \"inter_open_strong\"             \"inclu_stu_color_strong\"       \n [87] \"inclu_gender_strong\"           \"inclu_intern_stu_strong\"      \n [89] \"inclu_stu_disab_strong\"        \"inclu_first_gen_strong\"       \n [91] \"inclu_stu_sex_orient_strong\"   \"collegial_agree\"              \n [93] \"encouraging_agree\"             \"supportive_agree\"             \n [95] \"intel_open_agree\"              \"inter_open_agree\"             \n [97] \"inclu_stu_color_agree\"         \"inclu_gender_agree\"           \n [99] \"inclu_intern_stu_agree\"        \"inclu_stu_disab_agree\"        \n[101] \"inclu_first_gen_agree\"         \"inclu_stu_sex_orient_agree\"   \n[103] \"collegial_disagree\"            \"encouraging_disagree\"         \n[105] \"supportive_disagree\"           \"intel_open_disagree\"          \n[107] \"inter_open_disagree\"           \"inclu_stu_color_disagree\"     \n[109] \"inclu_gender_disagree\"         \"inclu_intern_stu_disagree\"    \n[111] \"inclu_stu_disab_disagree\"      \"inclu_first_gen_disagree\"     \n[113] \"inclu_stu_sex_orient_disagree\"\n\n\n\n# student equitable treatment\nprogram_experience(name = \"stu_equit\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# inclusive of students of color\nprogram_experience(name = \"inclu_stu_color\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\nRemoved 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# inclusive of gender\nprogram_experience(name = \"inclu_gender\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\nRemoved 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# inclusive of international students\nprogram_experience(name = \"inclu_intern_stu\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\nRemoved 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# inclusive of students with disabilities\nprogram_experience(name = \"inclu_stu_disab\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\nRemoved 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# inclusive of first generation students\nprogram_experience(name = \"inclu_first_gen\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\nRemoved 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# inclusive of students of all sexual orientations\nprogram_experience(name = \"inclu_stu_sex_orient\")\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\nRemoved 20 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLastly, I decided to look into the difference between the variables I’m most interested in. First, I wanted to look at how graduate students perceive inclusiveness of students of color within their departments. Another variable I was interested in was inclusiveness of first-generation graduate students. Thanks to the plotly package I was able to include some interactive components to the visuals. Specifically zooming in to specific departments give a better idea of the difference between agreeing and disagreeing on these topics. With plotly, you can also click on an option in the legend to only see those values. I also removed the strongly agree option since the agree applied to students that strongly agreed or agreed with the statement.\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nstu_color &lt;- exit %&gt;% \n  pivot_longer(\n    matches(\n      \"^inclu_stu_color\"\n    ),\n    names_to = \"stu_color\",\n    values_to = \"stu_color_values\"\n  ) |&gt;\n  filter(stu_color != \"inclu_stu_color_strong\") %&gt;% \n  mutate(stu_color = recode(stu_color, \"inclu_stu_color_agree\" = \"Agree with Inclusive Environment for Students of Color\",\n                           \"inclu_stu_color_disagree\" = \"Disagree with Inclusive Environment for Students of Color\")) %&gt;% \n  ggplot(aes(fct_reorder(program, stu_color_values), stu_color_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(stu_color)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\"))\n\nstu_plot &lt;- ggplotly(stu_color)\n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n  # layout(legend = list(orientation = \"h\",\n                       # xanchor = \"center\",\n                       # x = 0,\n                       # y = -60)) \nstu_plot\n\n\n\n\nfirstgen &lt;- exit %&gt;% \n  pivot_longer(\n    matches(\n      \"^inclu_first_gen\"\n    ),\n    names_to = \"first_gen\",\n    values_to = \"first_gen_values\"\n  ) |&gt;\n  filter(first_gen != \"inclu_first_gen_strong\") %&gt;% \n  mutate(first_gen = recode(first_gen, \"inclu_first_gen_agree\" = \"Agree with Inclusive Environment for First Gen\",\n                           \"inclu_first_gen_disagree\" = \"Disagree with Inclusive Environment for First Gen\")) %&gt;% \n  ggplot(aes(fct_reorder(program, first_gen_values), first_gen_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(first_gen)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\"))\n\nfirst_plot &lt;- ggplotly(firstgen) \n\nWarning: `fct_reorder()` removing 20 missing values.\nℹ Use `.na_rm = TRUE` to silence this message.\nℹ Use `.na_rm = FALSE` to preserve NAs.\n\n  # layout(legend = list(orientation = \"h\",\n  #                      xanchor = \"center\",\n  #                      x = 0,\n  #                      y = -60)) \nfirst_plot"
  },
  {
    "objectID": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html",
    "href": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html",
    "title": "Tidy Tuesday Coffee Ratings",
    "section": "",
    "text": "With coffee being a hobby of mine, I was scrolling through past Tidy Tuesdays and found one on coffee ratings. Originally I thought looking at predictions of total cup points, but I assumed with all the coffee tasting characteristics that it wouldn’t really tell me anything. Instead, I decided to look into the processing method, as there are different taste characteristics between washed and other processing methods.\n\ncoffee &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv') %&gt;% \n  mutate(species = as.factor(species),\n         process = recode(processing_method, \"Washed / Wet\" = \"washed\",\n                          \"Semi-washed / Semi-pulped\" = \"not_washed\",\n                          \"Pulped natural / honey\" = \"not_washed\",\n                          \"Other\" = \"not_washed\",\n                          \"Natural / Dry\" = \"not_washed\",\n                          \"NA\" = NA_character_),\n         process = as.factor(process),\n         process = relevel(process, ref = \"washed\"),\n         country_of_origin = as.factor(country_of_origin)) %&gt;% \n  drop_na(process) %&gt;% \n  filter(country_of_origin != \"Cote d?Ivoire\")\n\nRows: 1339 Columns: 43\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter looking at the distributions of procssing methods, I also decided to make the processing method binary with washed and not washed. This worked out better for the prediction models. There are also some descriptives of each variable.\n\ncoffee %&gt;% \n  ggplot(aes(processing_method)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\ncoffee %&gt;% \n  ggplot(aes(process)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\npsych::describe(coffee, na.rm = TRUE)[c(\"n\", \"mean\", \"sd\", \"min\", \"max\", \"skew\", \"kurtosis\")]\n\n                          n    mean      sd   min       max  skew kurtosis\ntotal_cup_points       1168   82.06    2.71 59.83     90.58 -1.95     9.26\nspecies*               1168    1.01    0.09  1.00      2.00 10.65   111.61\nowner*                 1161  134.40   77.04  1.00    287.00  0.11    -0.99\ncountry_of_origin*     1168   14.79   10.08  1.00     36.00  0.31    -1.12\nfarm_name*              883  264.34  153.73  1.00    524.00 -0.01    -1.28\nlot_number*             239   93.18   59.86  1.00    202.00  0.18    -1.27\nmill*                   931  188.25  123.79  1.00    419.00  0.23    -1.29\nico_number*            1057  360.89  242.06  1.00    753.00  0.08    -1.27\ncompany*               1077  142.68   76.32  1.00    266.00 -0.10    -1.24\naltitude*              1014  154.49   98.46  1.00    351.00  0.32    -1.04\nregion*                1137  167.49   87.31  1.00    325.00 -0.08    -1.10\nproducer*               995  307.70  175.31  1.00    624.00 -0.02    -1.11\nnumber_of_bags         1168  153.80  130.08  1.00   1062.00  0.37     0.50\nbag_weight*            1168   23.50   16.67  1.00     45.00 -0.23    -1.71\nin_country_partner*    1168    9.68    7.04  1.00     25.00  0.41    -1.31\nharvest_year*          1161    5.82    2.95  1.00     14.00  0.76    -0.46\ngrading_date*          1168  242.39  144.52  1.00    495.00  0.11    -1.21\nowner_1*               1161  136.29   77.93  1.00    290.00  0.10    -0.99\nvariety*               1089   12.61    9.77  1.00     29.00  0.63    -1.24\nprocessing_method*     1168    3.98    1.67  1.00      5.00 -1.13    -0.62\naroma                  1168    7.56    0.31  5.08      8.75 -0.55     4.46\nflavor                 1168    7.51    0.34  6.08      8.83 -0.34     1.73\naftertaste             1168    7.39    0.34  6.17      8.67 -0.45     1.36\nacidity                1168    7.53    0.31  5.25      8.75 -0.30     3.31\nbody                   1168    7.52    0.28  6.33      8.50 -0.10     0.89\nbalance                1168    7.51    0.34  6.08      8.58 -0.10     1.17\nuniformity             1168    9.84    0.50  6.00     10.00 -4.21    20.83\nclean_cup              1168    9.84    0.75  0.00     10.00 -6.98    62.29\nsweetness              1168    9.89    0.52  1.33     10.00 -7.53    80.78\ncupper_points          1168    7.48    0.40  5.17      8.75 -0.64     2.79\nmoisture               1168    0.09    0.05  0.00      0.17 -1.41     0.35\ncategory_one_defects   1168    0.51    2.70  0.00     63.00 14.43   279.42\nquakers                1167    0.17    0.82  0.00     11.00  6.87    57.30\ncolor*                 1070    2.80    0.64  1.00      4.00 -1.51     2.57\ncategory_two_defects   1168    3.79    5.54  0.00     55.00  3.54    18.53\nexpiration*            1168  241.61  144.11  1.00    494.00  0.12    -1.21\ncertification_body*    1168    9.38    6.65  1.00     24.00  0.37    -1.31\ncertification_address* 1168   16.58    7.33  1.00     29.00 -0.19    -1.06\ncertification_contact* 1168    9.39    7.26  1.00     26.00  0.36    -0.96\nunit_of_measurement*   1168    1.87    0.34  1.00      2.00 -2.21     2.87\naltitude_low_meters    1012 1796.86 9073.21  1.00 190164.00 19.36   384.12\naltitude_high_meters   1012 1834.27 9071.86  1.00 190164.00 19.36   384.03\naltitude_mean_meters   1012 1815.56 9072.31  1.00 190164.00 19.36   384.12\nprocess*               1168    1.30    0.46  1.00      2.00  0.86    -1.27\n\n\nNow, its time to split the data into training and testing data. I also included the function of strata to stratify sampling based on process.\n\nset.seed(05132021)\n\ncoffee_split &lt;- initial_split(coffee, strata = \"process\")\n\ncoffee_train &lt;- training(coffee_split)\ncoffee_test &lt;- testing(coffee_split)\n\nI also did some cross validation for the training dataset and used the metrics I was most interested in.\n\nset.seed(05132021)\n\ncoffee_fold &lt;- vfold_cv(coffee_train, strata = \"process\", v = 10)\n\nmetric_measure &lt;- metric_set(accuracy, mn_log_loss, roc_auc)\n\nFrom the beginning I was interested in the tasting characteristics and how they would predict whether the green coffee was washed or not washed. I also included the total cup points because I wanted to see the importance of that predictor on the processing method. The only feature engineering I did was to remove any zero variance in the predictors of the model.\n\nset.seed(05132021)\n\nchar_recipe &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points,\n                      data = coffee_train) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\nchar_recipe %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;%\n  head()\n\n# A tibble: 6 × 11\n  aroma flavor aftertaste acidity  body balance uniformity clean_cup sweetness\n  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  8.17   8.58       8.42    8.42  8.5     8.25         10        10        10\n2  8.08   8.58       8.5     8.5   7.67    8.42         10        10        10\n3  8.17   8.17       8       8.17  8.08    8.33         10        10        10\n4  8.42   8.17       7.92    8.17  8.33    8            10        10        10\n5  8.5    8.5        8       8     8       8            10        10        10\n6  8      8          8       8.25  8       8.17         10        10        10\n# ℹ 2 more variables: total_cup_points &lt;dbl&gt;, process &lt;fct&gt;\n\n\n\n\nThe first model I wanted to test with the current recipe was logistic regression. The accuracy and roc auc were alright for a starting model.\n\n\nWarning: package 'glmnet' was built under R version 4.3.2\n\n\n\ncollect_metrics(lr_fit)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.698    10 0.00342 Preprocessor1_Model1\n2 mn_log_loss binary     0.589    10 0.00775 Preprocessor1_Model1\n3 roc_auc     binary     0.648    10 0.0188  Preprocessor1_Model1\n\n\n\n\n\nNow for the first penalized regression. The lasso regression did not improve in either metric. Let’s try the next penalized regression.\n\ncollect_metrics(lasso_fit)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.702    10 0.00397 Preprocessor1_Model1\n2 mn_log_loss binary     0.592    10 0.00850 Preprocessor1_Model1\n3 roc_auc     binary     0.645    10 0.0191  Preprocessor1_Model1\n\n\n\n\n\nThe ridge regression was shown to not be a good fitting model. So I tested an additional penalized regression while tuning hyper-parameters.\n\ncollect_metrics(ridge_fit)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.697    10 0.00133 Preprocessor1_Model1\n2 mn_log_loss binary     0.603    10 0.00212 Preprocessor1_Model1\n3 roc_auc     binary     0.612    10 0.0186  Preprocessor1_Model1\n\n\n\n\n\nThe elastic net regression had slightly better accuracy than the non-penalized logistic regression but the ROC AUC was exactly the same. While the elastic net regression did not take long computationally due to the small amount of data, this model would not be chosen over the logistic regression.\n\ncollect_metrics(elastic_fit)\n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n# ℹ 290 more rows\n\nshow_best(elastic_fit, metric = \"accuracy\", n = 5)\n\n# A tibble: 5 × 8\n        penalty mixture .metric  .estimator  mean     n std_err .config         \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1 0.0774          0.111 accuracy binary     0.703    10 0.00434 Preprocessor1_M…\n2 0.0000000001    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n3 0.00000000129   0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n4 0.0000000167    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n5 0.000000215     0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n\nshow_best(elastic_fit, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n2 0.00000000129       0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n3 0.0000000167        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n4 0.000000215         0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n5 0.00000278          0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n\nselect_best(elastic_fit, metric = \"accuracy\")\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1  0.0774   0.111 Preprocessor1_Model019\n\nselect_best(elastic_fit, metric = \"roc_auc\")\n\n# A tibble: 1 × 3\n       penalty mixture .config               \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001       0 Preprocessor1_Model001\n\n\n\n\n\nEven though the elastic net regression was only slightly better, I decided to update the workflow using that model. This time I decided to update the recipe by including additional predictors like if there were any defects in the green coffee beans, the species of the coffee (e.g., Robusta and Arabica), and the country of origin. I also included additional steps in my recipe by transforming the category predictors and working with the factor predictors, like species, and country of origin. The inclusion of additional steps and the predictors created a better fitting model with the elastic net regression.\n\nset.seed(05132021)\n\nbal_rec &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points + category_one_defects + category_two_defects + species +\n                        country_of_origin,\n                      data = coffee_train) %&gt;% \n  step_BoxCox(category_two_defects, category_one_defects) %&gt;% \n  step_novel(species, country_of_origin) %&gt;% \n  step_other(species, country_of_origin, threshold = .01) %&gt;%\n  step_unknown(species, country_of_origin) %&gt;% \n  step_dummy(species, country_of_origin) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n\n\ncollect_metrics(elastic_bal_fit) \n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n# ℹ 290 more rows\n\nshow_best(elastic_bal_fit, metric = \"accuracy\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000464   0.111 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n2 0.000464   0.222 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n3 0.00599    0.111 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n4 0.00599    0.222 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n5 0.00599    0.333 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n\nshow_best(elastic_bal_fit, metric = \"mn_log_loss\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric     .estimator  mean     n std_err .config           \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;             \n1 0.000464   1     mn_log_loss binary     0.420    10  0.0179 Preprocessor1_Mod…\n2 0.000464   0.889 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n3 0.000464   0.778 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n4 0.000464   0.667 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n5 0.000464   0.556 mn_log_loss binary     0.420    10  0.0177 Preprocessor1_Mod…\n\nshow_best(elastic_bal_fit, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599    0.778 roc_auc binary     0.843    10  0.0150 Preprocessor1_Model078\n2 0.000464   0.667 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model067\n3 0.00599    0.889 roc_auc binary     0.843    10  0.0148 Preprocessor1_Model088\n4 0.000464   0.444 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model047\n5 0.000464   0.556 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model057\n\nselect_best(elastic_bal_fit, metric = \"accuracy\")\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464   0.111 Preprocessor1_Model017\n\nselect_best(elastic_bal_fit, metric = \"mn_log_loss\")\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464       1 Preprocessor1_Model097\n\nselect_best(elastic_bal_fit, metric = \"roc_auc\")\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599   0.778 Preprocessor1_Model078\n\n\nNow using the testing dataset, we can see how well the final model fit the testing data. While not the best at predicting washed green coffee beans, this was a good test to show that the penalized regressions are not always the best fitting models compared to regular logistic regression. In the end, it seemed like the recipe was the most important component to predicting washed green coffee beans.\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\nfinal_results %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.823 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html#predicting-process-of-green-coffee-beans",
    "href": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html#predicting-process-of-green-coffee-beans",
    "title": "Tidy Tuesday Coffee Ratings",
    "section": "",
    "text": "With coffee being a hobby of mine, I was scrolling through past Tidy Tuesdays and found one on coffee ratings. Originally I thought looking at predictions of total cup points, but I assumed with all the coffee tasting characteristics that it wouldn’t really tell me anything. Instead, I decided to look into the processing method, as there are different taste characteristics between washed and other processing methods.\n\ncoffee &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv') %&gt;% \n  mutate(species = as.factor(species),\n         process = recode(processing_method, \"Washed / Wet\" = \"washed\",\n                          \"Semi-washed / Semi-pulped\" = \"not_washed\",\n                          \"Pulped natural / honey\" = \"not_washed\",\n                          \"Other\" = \"not_washed\",\n                          \"Natural / Dry\" = \"not_washed\",\n                          \"NA\" = NA_character_),\n         process = as.factor(process),\n         process = relevel(process, ref = \"washed\"),\n         country_of_origin = as.factor(country_of_origin)) %&gt;% \n  drop_na(process) %&gt;% \n  filter(country_of_origin != \"Cote d?Ivoire\")\n\nRows: 1339 Columns: 43\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter looking at the distributions of procssing methods, I also decided to make the processing method binary with washed and not washed. This worked out better for the prediction models. There are also some descriptives of each variable.\n\ncoffee %&gt;% \n  ggplot(aes(processing_method)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\ncoffee %&gt;% \n  ggplot(aes(process)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\npsych::describe(coffee, na.rm = TRUE)[c(\"n\", \"mean\", \"sd\", \"min\", \"max\", \"skew\", \"kurtosis\")]\n\n                          n    mean      sd   min       max  skew kurtosis\ntotal_cup_points       1168   82.06    2.71 59.83     90.58 -1.95     9.26\nspecies*               1168    1.01    0.09  1.00      2.00 10.65   111.61\nowner*                 1161  134.40   77.04  1.00    287.00  0.11    -0.99\ncountry_of_origin*     1168   14.79   10.08  1.00     36.00  0.31    -1.12\nfarm_name*              883  264.34  153.73  1.00    524.00 -0.01    -1.28\nlot_number*             239   93.18   59.86  1.00    202.00  0.18    -1.27\nmill*                   931  188.25  123.79  1.00    419.00  0.23    -1.29\nico_number*            1057  360.89  242.06  1.00    753.00  0.08    -1.27\ncompany*               1077  142.68   76.32  1.00    266.00 -0.10    -1.24\naltitude*              1014  154.49   98.46  1.00    351.00  0.32    -1.04\nregion*                1137  167.49   87.31  1.00    325.00 -0.08    -1.10\nproducer*               995  307.70  175.31  1.00    624.00 -0.02    -1.11\nnumber_of_bags         1168  153.80  130.08  1.00   1062.00  0.37     0.50\nbag_weight*            1168   23.50   16.67  1.00     45.00 -0.23    -1.71\nin_country_partner*    1168    9.68    7.04  1.00     25.00  0.41    -1.31\nharvest_year*          1161    5.82    2.95  1.00     14.00  0.76    -0.46\ngrading_date*          1168  242.39  144.52  1.00    495.00  0.11    -1.21\nowner_1*               1161  136.29   77.93  1.00    290.00  0.10    -0.99\nvariety*               1089   12.61    9.77  1.00     29.00  0.63    -1.24\nprocessing_method*     1168    3.98    1.67  1.00      5.00 -1.13    -0.62\naroma                  1168    7.56    0.31  5.08      8.75 -0.55     4.46\nflavor                 1168    7.51    0.34  6.08      8.83 -0.34     1.73\naftertaste             1168    7.39    0.34  6.17      8.67 -0.45     1.36\nacidity                1168    7.53    0.31  5.25      8.75 -0.30     3.31\nbody                   1168    7.52    0.28  6.33      8.50 -0.10     0.89\nbalance                1168    7.51    0.34  6.08      8.58 -0.10     1.17\nuniformity             1168    9.84    0.50  6.00     10.00 -4.21    20.83\nclean_cup              1168    9.84    0.75  0.00     10.00 -6.98    62.29\nsweetness              1168    9.89    0.52  1.33     10.00 -7.53    80.78\ncupper_points          1168    7.48    0.40  5.17      8.75 -0.64     2.79\nmoisture               1168    0.09    0.05  0.00      0.17 -1.41     0.35\ncategory_one_defects   1168    0.51    2.70  0.00     63.00 14.43   279.42\nquakers                1167    0.17    0.82  0.00     11.00  6.87    57.30\ncolor*                 1070    2.80    0.64  1.00      4.00 -1.51     2.57\ncategory_two_defects   1168    3.79    5.54  0.00     55.00  3.54    18.53\nexpiration*            1168  241.61  144.11  1.00    494.00  0.12    -1.21\ncertification_body*    1168    9.38    6.65  1.00     24.00  0.37    -1.31\ncertification_address* 1168   16.58    7.33  1.00     29.00 -0.19    -1.06\ncertification_contact* 1168    9.39    7.26  1.00     26.00  0.36    -0.96\nunit_of_measurement*   1168    1.87    0.34  1.00      2.00 -2.21     2.87\naltitude_low_meters    1012 1796.86 9073.21  1.00 190164.00 19.36   384.12\naltitude_high_meters   1012 1834.27 9071.86  1.00 190164.00 19.36   384.03\naltitude_mean_meters   1012 1815.56 9072.31  1.00 190164.00 19.36   384.12\nprocess*               1168    1.30    0.46  1.00      2.00  0.86    -1.27\n\n\nNow, its time to split the data into training and testing data. I also included the function of strata to stratify sampling based on process.\n\nset.seed(05132021)\n\ncoffee_split &lt;- initial_split(coffee, strata = \"process\")\n\ncoffee_train &lt;- training(coffee_split)\ncoffee_test &lt;- testing(coffee_split)\n\nI also did some cross validation for the training dataset and used the metrics I was most interested in.\n\nset.seed(05132021)\n\ncoffee_fold &lt;- vfold_cv(coffee_train, strata = \"process\", v = 10)\n\nmetric_measure &lt;- metric_set(accuracy, mn_log_loss, roc_auc)\n\nFrom the beginning I was interested in the tasting characteristics and how they would predict whether the green coffee was washed or not washed. I also included the total cup points because I wanted to see the importance of that predictor on the processing method. The only feature engineering I did was to remove any zero variance in the predictors of the model.\n\nset.seed(05132021)\n\nchar_recipe &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points,\n                      data = coffee_train) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\nchar_recipe %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;%\n  head()\n\n# A tibble: 6 × 11\n  aroma flavor aftertaste acidity  body balance uniformity clean_cup sweetness\n  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  8.17   8.58       8.42    8.42  8.5     8.25         10        10        10\n2  8.08   8.58       8.5     8.5   7.67    8.42         10        10        10\n3  8.17   8.17       8       8.17  8.08    8.33         10        10        10\n4  8.42   8.17       7.92    8.17  8.33    8            10        10        10\n5  8.5    8.5        8       8     8       8            10        10        10\n6  8      8          8       8.25  8       8.17         10        10        10\n# ℹ 2 more variables: total_cup_points &lt;dbl&gt;, process &lt;fct&gt;\n\n\n\n\nThe first model I wanted to test with the current recipe was logistic regression. The accuracy and roc auc were alright for a starting model.\n\n\nWarning: package 'glmnet' was built under R version 4.3.2\n\n\n\ncollect_metrics(lr_fit)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.698    10 0.00342 Preprocessor1_Model1\n2 mn_log_loss binary     0.589    10 0.00775 Preprocessor1_Model1\n3 roc_auc     binary     0.648    10 0.0188  Preprocessor1_Model1\n\n\n\n\n\nNow for the first penalized regression. The lasso regression did not improve in either metric. Let’s try the next penalized regression.\n\ncollect_metrics(lasso_fit)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.702    10 0.00397 Preprocessor1_Model1\n2 mn_log_loss binary     0.592    10 0.00850 Preprocessor1_Model1\n3 roc_auc     binary     0.645    10 0.0191  Preprocessor1_Model1\n\n\n\n\n\nThe ridge regression was shown to not be a good fitting model. So I tested an additional penalized regression while tuning hyper-parameters.\n\ncollect_metrics(ridge_fit)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.697    10 0.00133 Preprocessor1_Model1\n2 mn_log_loss binary     0.603    10 0.00212 Preprocessor1_Model1\n3 roc_auc     binary     0.612    10 0.0186  Preprocessor1_Model1\n\n\n\n\n\nThe elastic net regression had slightly better accuracy than the non-penalized logistic regression but the ROC AUC was exactly the same. While the elastic net regression did not take long computationally due to the small amount of data, this model would not be chosen over the logistic regression.\n\ncollect_metrics(elastic_fit)\n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n# ℹ 290 more rows\n\nshow_best(elastic_fit, metric = \"accuracy\", n = 5)\n\n# A tibble: 5 × 8\n        penalty mixture .metric  .estimator  mean     n std_err .config         \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1 0.0774          0.111 accuracy binary     0.703    10 0.00434 Preprocessor1_M…\n2 0.0000000001    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n3 0.00000000129   0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n4 0.0000000167    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n5 0.000000215     0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n\nshow_best(elastic_fit, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n2 0.00000000129       0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n3 0.0000000167        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n4 0.000000215         0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n5 0.00000278          0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n\nselect_best(elastic_fit, metric = \"accuracy\")\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1  0.0774   0.111 Preprocessor1_Model019\n\nselect_best(elastic_fit, metric = \"roc_auc\")\n\n# A tibble: 1 × 3\n       penalty mixture .config               \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001       0 Preprocessor1_Model001\n\n\n\n\n\nEven though the elastic net regression was only slightly better, I decided to update the workflow using that model. This time I decided to update the recipe by including additional predictors like if there were any defects in the green coffee beans, the species of the coffee (e.g., Robusta and Arabica), and the country of origin. I also included additional steps in my recipe by transforming the category predictors and working with the factor predictors, like species, and country of origin. The inclusion of additional steps and the predictors created a better fitting model with the elastic net regression.\n\nset.seed(05132021)\n\nbal_rec &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points + category_one_defects + category_two_defects + species +\n                        country_of_origin,\n                      data = coffee_train) %&gt;% \n  step_BoxCox(category_two_defects, category_one_defects) %&gt;% \n  step_novel(species, country_of_origin) %&gt;% \n  step_other(species, country_of_origin, threshold = .01) %&gt;%\n  step_unknown(species, country_of_origin) %&gt;% \n  step_dummy(species, country_of_origin) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n\n\ncollect_metrics(elastic_bal_fit) \n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n# ℹ 290 more rows\n\nshow_best(elastic_bal_fit, metric = \"accuracy\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000464   0.111 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n2 0.000464   0.222 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n3 0.00599    0.111 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n4 0.00599    0.222 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n5 0.00599    0.333 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n\nshow_best(elastic_bal_fit, metric = \"mn_log_loss\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric     .estimator  mean     n std_err .config           \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;             \n1 0.000464   1     mn_log_loss binary     0.420    10  0.0179 Preprocessor1_Mod…\n2 0.000464   0.889 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n3 0.000464   0.778 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n4 0.000464   0.667 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n5 0.000464   0.556 mn_log_loss binary     0.420    10  0.0177 Preprocessor1_Mod…\n\nshow_best(elastic_bal_fit, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599    0.778 roc_auc binary     0.843    10  0.0150 Preprocessor1_Model078\n2 0.000464   0.667 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model067\n3 0.00599    0.889 roc_auc binary     0.843    10  0.0148 Preprocessor1_Model088\n4 0.000464   0.444 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model047\n5 0.000464   0.556 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model057\n\nselect_best(elastic_bal_fit, metric = \"accuracy\")\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464   0.111 Preprocessor1_Model017\n\nselect_best(elastic_bal_fit, metric = \"mn_log_loss\")\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464       1 Preprocessor1_Model097\n\nselect_best(elastic_bal_fit, metric = \"roc_auc\")\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599   0.778 Preprocessor1_Model078\n\n\nNow using the testing dataset, we can see how well the final model fit the testing data. While not the best at predicting washed green coffee beans, this was a good test to show that the penalized regressions are not always the best fitting models compared to regular logistic regression. In the end, it seemed like the recipe was the most important component to predicting washed green coffee beans.\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\nfinal_results %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.823 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1"
  }
]