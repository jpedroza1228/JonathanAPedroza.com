[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Jonathan Andrew Pedroza PhD, but everyone calls me JP. I received my PhD in Prevention Science from the University of Oregon in 2021. My education in Prevention Science included training in program evaluation, implementation science, machine learning, inferential statistics, and survey design. I currently am a contractor for Posit Academy as a data science mentor. As a mentor, I train cohorts from organizations around the world to learn data science skills in R and Python, such as data wrangling, data visualization, functional programming, model building, and communicating results using dashboards and reports. I am open to learning more about a new field, especially since working with data from various fields with my academy cohorts.\n\n\n\n\n\n\nMy posts often include using R and Python for data analyses and machine learning. Some of these topics include Bayesian statistics with Stan, interactive documents, and shiny apps. When I am away from my computer, I enjoy roasting and brewing coffee, hiking, fishing, cooking, and playing with my cats. I’m currently on BlueSky for social media and infrequently check Mastodon. You can also email me at  jonpedroza1228@gmail.com with any inquires.\n\nData Science Colleagues"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resources",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension\n\n\n\nUniversity of Oregon Graduate School Dashboard\n\n\n\nShiny App for Bayesian Network\n\nGitHub Repository: (Link)\n\n\n\n\nNeuropsychology Assessment Calculations\n\nGitHub Repository: (Link)\n\nProphet Stock Price Parameters Shiny App\n\nGitHub Repository: (Link)\n\n\n\n\nReductions in Physical Activity in California Counties\nFunding for University of Oregon Teaching Assistants\nThe Importance of Including Variables in Models\n\n\n\nSuspension Levels for Students with Disabilities in California Districts"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Posts",
    "section": "",
    "text": "Looping Through Parameterized Reports\n\n\n\nPython\n\nQuarto\n\nPandas\n\nPlotnine\n\nGreat Tables\n\nPlotly\n\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a Bayesian Network For Machine Learning\n\n\n\nbayes-net\n\nbayesian network\n\nmachine learning\n\nR\n\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Net Pt. 2\n\n\n\nBayesian\n\nInference\n\nBayesian Network\n\nbayes net\n\nR\n\nstan\n\ncmdstanr\n\nposterior\n\nbayesplot\n\nggplot2\n\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation in R & Python\n\n\n\ndata manipulation\n\ndplyr\n\npandas\n\nnumpy\n\npython\n\nR\n\ndata.table\n\npolars\n\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy submission to Posit’s Closeread Competition\n\n\n\ncloseread\n\nggplot2\n\nHTML\n\nquarto\n\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nProphet Model\n\n\n\nggplot2\n\nForecast\n\nTidyModels\n\nModeltime\n\nProphet\n\nR\n\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Nets\n\n\n\nbayesian\n\nbayesian network\n\nbayes net\n\nR\n\nstan\n\ncmdstanr\n\nggdag\n\ndagitty\n\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Network for US Coffee Tasting Data\n\n\n\nBayesian\n\nBayesian Network\n\nbayes net\n\nR\n\nbnlearn\n\ndag\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student Satisfaction Exit Surveys\n\n\n\nggplot2\n\nShiny\n\nEducation\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday Coffee Ratings\n\n\n\nggplot2\n\ntidymodels\n\nTidy Tuesday\n\nthemis\n\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter Conference Presentation\n\n\n\nR\n\nggplot2\n\nVisualizations\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html",
    "href": "posts/2022-06-02-prophet-model/index.html",
    "title": "Prophet Model",
    "section": "",
    "text": "NOTE: This is for practicing forecasting skills and should not be seen as a guide for predicting stock prices.\nAfter some reading, I finally have a good understanding of how to utilize forecasting for time-series analyses. This post started because even after a BA, 2 masters degrees, and a doctorate, my brother still has no clue what I do. He, along with most of my family think I am a Clinical Psychologist.\nSo for me to try and make my brother understand what I do, I thought I would show him with something that he has become interested in recently; stocks. So for this post, I’ll\nBelow are all the sites for the packages I used.\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(prophet)\nlibrary(lubridate)\nlibrary(modeltime)\nlibrary(timetk)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "href": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "title": "Prophet Model",
    "section": "Loading Data",
    "text": "Loading Data\nTo load the Google Finance data, I decided to pick a stock that my brother had, which in this case was JetBlue. A cool feature about Google Finance and Google Sheets is that you can use the following formula in a Google Sheet on the first cell of the first column =GOOGLEFINANCE(\"JBLU\", \"price\", DATE(2000,1,1), DATE(2024, 1, 1), \"DAILY\") and it will give you the date and stock closing values for whatever period you’d like. The example above provides Google financial data for JBLU or the abbreviation for JetBlue stock. It also provides the price of the stock from the first day that there is data on JetBlue stocks, which in this case is April 12th 2002. You can also choose the interval of time for the stock prices. I decided to look at daily data because I have several years of data.\nJetBlue Sheet\nHere I have a copy of my Google Sheet for JetBlue that I will use to train and test my Prophet model. Instead of having a .csv file on my local machine, I decided to keep this on Google Drive so that it constantly updates with the Google Finance function. This meant that I had to use the googlesheets4 package to load the data from a Google Sheet. I also changed the name and class of the date variable to make it a date variable instead of a date and time variable.\n\n\nCode\ngooglesheets4::gs4_deauth()\n\ntheme_set(theme_light())\n\njet &lt;- googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1SpRXsC3kXDaQLUfC6cPIOvsqxDF6updhgHRJeT8PTog/edit#gid=0\", sheet = 1) |&gt; \n  janitor::clean_names() |&gt;\n  mutate(ds = as_date(date))\n\n\n\nCleaning Up the Data\nBased on some visualizations below, I also decided to create some additional variables from the date variable. Specifically, I used lubridate's wday() function to create a new variable that gives you the actual day from the corresponding cell’s date. I also used the ts_clean_vec function from time_tk to clean for outliers in the stock price values. There are additional arguments for the function, like applying a Box-Cox transformation but that is for a multiplicative trend, which this model does not appear to fit since the variation in the outcome does not grow exponentially. I’ll also include 2002 as the reference year for the year variable and make sure that my data is arranged by date.\n\n\nCode\njetblue &lt;- jet |&gt; \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) |&gt; \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') |&gt; \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002')) |&gt; \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') |&gt;\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) |&gt; \n  select(-drop) |&gt; \n  arrange(ds)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "href": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "title": "Prophet Model",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nStarting with some quick visualizations, we can see that the only area that there is a difference in the variation of the stock prices is in the beginning of 2020. I wonder what that could have been.\n\n\nCode\njetblue |&gt; \n  group_by(year_num, month_num) |&gt; \n  summarize(var_value = sd(close)^2) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(month_num, var_value)) + \n  geom_point() + \n  facet_wrap(vars(year_num))\n\n\n\n\n\n\n\n\n\nNext, we can look at the histograms for the outcome of interest. If we look at the histograms, we can see that there are potential outliers in the original stock prices data. We can also see that cleaning the variable removed the potential outliers.\n\n\nCode\nonly_numeric &lt;- jetblue |&gt; \n  select(close, clean)\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~ggplot(data = only_numeric,\n             aes(.x)) + \n       geom_histogram(color = 'white',\n                      fill = 'dodgerblue') +\n       geom_vline(xintercept = mean(.x) +\n                    sd(.x) +\n                    sd(.x) +\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       geom_vline(xintercept = mean(.x) -\n                    sd(.x) -\n                    sd(.x) -\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       labs(title = .y))\n\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nThere will also be a lot of use of the purrr package and the map functions, which are part of the tidyverse. We can also see that in the plot series visualization using modeltime's plot_time_series function, that the cleaned stock prices remove the outliers. So from here on out, I’ll be using the cleaned stock prices.\n\n\nCode\nmap2(only_numeric,\n     names(only_numeric),\n     ~only_numeric |&gt; \n       plot_time_series(jetblue$ds,\n                        .x,\n                        .interactive = FALSE) + \n       labs(title = .y))\n\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nWe can also look for anomalies, or points that deviate from the trend. Using the plot_anomaly_diagnostics function from the modeltime package, I can see all the anomalies in the data. I also used ggplot to create my own visualization using the same data. Lastly, we’ll deal with those anomalies by removing them from the dataset. This is not too much of a problem because the Prophet model should be able to handle this fairly easy.\n\n\nCode\njetblue |&gt; \n  plot_anomaly_diagnostics(ds,\n                           clean,\n                           .facet_ncol = 1,\n                           .interactive = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nCode\njetblue |&gt; \n  tk_anomaly_diagnostics(ds,\n                         clean) |&gt; \n  ggplot(aes(ds, observed)) + \n  geom_line() + \n  geom_point(aes(color = anomaly)) +\n  viridis::scale_color_viridis(option = 'D',\n                               discrete = TRUE,\n                               begin = .5,\n                               end = 0)\n\n\n\n\n\n\n\n\n\nCode\nanomaly &lt;- jetblue |&gt;\n  tk_anomaly_diagnostics(ds,\n                         clean)\n\njetblue &lt;- left_join(jetblue, anomaly) |&gt;\n  filter(anomaly != 'Yes')\n\n\nWe can also look into additional regressors to include in the model by looking into seasonality. We can see some fluctuation in stock prices across the years. We’ll include the year variable as another regressor on the stock prices.\n\n\nCode\njetblue |&gt; \n  plot_seasonal_diagnostics(ds,\n                            clean,\n                            .interactive = FALSE)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "href": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "title": "Prophet Model",
    "section": "Training the Prophet Model",
    "text": "Training the Prophet Model\nBefore we begin, I’m going to designate 10 cores to process any models run.\n\n\nCode\nset.seed(05262022)\n\nparallel::detectCores()\n\n\n[1] 12\n\n\nCode\nparallel_start(10,\n               .method = 'parallel')\n\n\nFirst, instead of the normal initial_split used for training and testing splits, we’ll use the initial_time_split function from tidymodels to separate the first 80% of the data into training set and the last 20% into the testing set.\n\n\nCode\nset.seed(05262022)\njet_split &lt;- initial_time_split(jetblue)\n\n\n\nProphet Model Function\nI decided to create my own Prophet function to be able to use for both training the model and testing it. In this function, I’ve also included parameters that can be changed to see if the model performs better or worse. Lastly, the train = TRUE allows us to practice with the training dataset and then when we’re happy with the model, we can use it to test our model. For our model, we’ll be predicting stock prices with date and comparing each year to the reference year (2002).\n\n\nCode\nprophet_mod &lt;- function(splits,\n                        changepoints = .05,\n                        seasonality = .01,\n                        holiday = .01,\n                        season_type = 'additive',\n                        day_season = 'auto',\n                        week_season = 'auto',\n                        year_season = 'auto',\n                        train = TRUE){\n  library(tidyverse)\n  library(tidymodels)\n  library(modeltime)\n  library(prophet)\n  \n  analy_data &lt;- analysis(splits)\n  assess_data &lt;- assessment(splits)\n  \n  model &lt;- prophet_reg() |&gt; \n    set_engine(engine = 'prophet',\n               verbose = TRUE) |&gt; \n    set_args(prior_scale_changepoints = changepoints,\n             prior_scale_seasonality = seasonality,\n             prior_scale_holidays = holiday,\n             season = season_type,\n             seasonality_daily = day_season,\n             seasonality_weekly = week_season,\n             seasonality_yearly = year_season) |&gt; \n    fit(clean ~ ds + year_num, \n        data = analy_data)\n  \n  if(train == TRUE){\n    train_cali &lt;- model |&gt; \n      modeltime_calibrate(new_data = analy_data)\n    \n    train_acc &lt;- train_cali |&gt; \n      modeltime_accuracy()\n    \n    return(list(train_cali, train_acc))\n  }\n  \n  else{\n    test_cali &lt;- model |&gt; \n      modeltime_calibrate(new_data = assess_data)\n    \n    test_acc &lt;- test_cali |&gt; \n      modeltime_accuracy()\n    \n    return(list(test_cali, test_acc))\n  }\n}\n\n\nIt is worth noting that I’m using the modeltime package to run the prophet model because I believe it is easier to use (especially for later steps) than from Prophet, but both can be implemented in this function. Let’s try running this model with the some random parameters I chose from the Prophet website until realizing that the modeltime parameters are log transformed.\n\n\nCode\nset.seed(05262022)\nbaseline &lt;- prophet_mod(jet_split,\n                 train = TRUE) |&gt; \n  pluck(2)\n\nbaseline\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted  1.02  9.14  4.71  8.94  1.38 0.950\n\n\nSo with the model, we can see that the Mean Absolute Scaled Error (MASE) is 4.7105317 and the Root Mean Square Error (RMSE) is 1.3773241. Not bad for an initial run. Let’s look at how the model fits the training data.\n\n\nCode\nprophet_mod(jet_split,\n                 train = TRUE) |&gt;  \n  pluck(1) |&gt; \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Prophet Baseline Model')\n\n\n\n\n\n\n\n\n\nSo the model appears to follow the trend line. We’ll try to tune some of these parameters to see if we can make the model better.\n\n\nTuning the Model\nNow, I’ll tune the prior scale values for the model. I’ll use the grid_latin_hypercube from the dials package in tidymodels to choose 5 sets of parameter values to run. I’m also using the rolling_origin from the rsample package in tidymodels because we are working with time series data. This does not create random samples but instead has samples with data points with consecutive values.\n\n\nCode\nset.seed(05262022)\n\nproph_model &lt;- prophet_reg() |&gt;\n  set_engine(engine = 'prophet',\n             verbose = TRUE) |&gt;\n  set_args(prior_scale_changepoints = tune(),\n           prior_scale_seasonality = tune(),\n           prior_scale_holidays = tune(),\n           season = 'additive',\n           seasonality_daily = 'auto',\n           seasonality_weekly = 'auto',\n           seasonality_yearly = 'auto')\n\nproph_rec &lt;-\n  recipe(clean ~ ds + year_num,\n         data = training(jet_split))\n\n\nset.seed(05262022)\ntrain_fold &lt;-\n  rolling_origin(training(jet_split),\n                 initial = 270,  \n                 assess = 90, \n                 skip = 30,\n                 cumulative = TRUE)\n\nset.seed(05262022)\ngrid_values &lt;-\n  grid_latin_hypercube(prior_scale_changepoints(),\n                       prior_scale_seasonality(),\n                       prior_scale_holidays(),\n                       size = 5)\n\nset.seed(05262022)\nproph_fit &lt;- tune_grid(object = proph_model,\n                       preprocessor = proph_rec,\n                       resamples = train_fold,\n                       grid = grid_values,\n                       control = control_grid(verbose = TRUE,\n                                              save_pred = TRUE,\n                                              allow_par = TRUE))\n\n\ntuned_metrics &lt;- collect_metrics(proph_fit)\ntuned_metrics |&gt;\n  filter(.metric == 'rmse') |&gt; \n  arrange(mean)\n\n# saveRDS(tuned_metrics, file = 'tuned_metrics.rds')\n\n\n\n\nCode\nmetrics &lt;-\n  readr::read_rds(here::here('posts/2022-06-02-prophet-model/tuned_metrics.rds'))\n\nmetrics |&gt; \n  filter(.metric == 'rmse') |&gt; \n  arrange(mean)\n\n\n# A tibble: 5 × 9\n  prior_scale_changepoints prior_scale_seasonality prior_scale_holidays .metric\n                     &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;  \n1                  3.53                    0.0170               1.12    rmse   \n2                  0.00139                 0.00166              0.00172 rmse   \n3                  0.884                  36.4                  0.0131  rmse   \n4                  0.0549                  0.261                0.231   rmse   \n5                 43.0                     3.80                12.2     rmse   \n# ℹ 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;,\n#   .config &lt;chr&gt;\n\n\nFor the sake of not waiting for this to render, I decided to make a RDS file of the metrics gathered from the tuned Prophet model. We can see that the RMSE value was 2.52 and the prior scale changepoint value was 3.53, the prior scale seasonality value was 0.02, and the prior scale holiday value was 1.\n\n\nFinal Training Model\nI then decided to run the prophet model on the training dataset with the new parameter values.\n\n\nCode\nfinal_train &lt;- prophet_mod(jet_split,\n                 changepoints = 3.53,\n                 seasonality = .017,\n                 holiday = 1.12,\n                 train = TRUE) |&gt;  \n  pluck(2)\n\nfinal_train\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted 0.864  7.59  4.00  7.49  1.20 0.962\n\n\n\n\nCode\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = TRUE) |&gt;  \n  pluck(1) |&gt; \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Training Model')\n\n\n\n\n\n\n\n\n\nWe can see that when using the whole training set, we have a RMSE of 1.2 and a MASE of 4 so both metrics reduced slightly."
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "href": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "title": "Prophet Model",
    "section": "Testing the Model",
    "text": "Testing the Model\nFinally, let’s test our Prophet model to see how well the model fits.\n\n\nCode\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) |&gt;\n  pluck(1) |&gt; \n  modeltime_forecast(new_data = testing(jet_split),\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Testing Model')\n\n\n\n\n\n\n\n\n\n\n\nCode\ntest_model &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) |&gt;\n  pluck(2)\n\ntest_model\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Test   10.5  119.  41.7  60.9  12.0 0.480\n\n\nBased on the visualization and the model fit metrics, we can see that our model fits much worse when used on the testing set. The MASE has gotten significantly worse (41.6831491) and so has the RMSE (11.9781056) ."
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "href": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "title": "Prophet Model",
    "section": "Forecasting Ahead a Year",
    "text": "Forecasting Ahead a Year\nWell our model fit could be better, but let’s see how the model looks when refit to the full data and forecast ahead a year. So in a year, it seems that JetBlue stock may experience an uptick in prices to almost $10 a stock at the beginning of 2024. Then it seems like the value may drop again as the year goes on. The confidence intervals for the forecast do not appear to change much from the rest of the model as well.\n\n\nCode\nfuture &lt;- jetblue |&gt; \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture &lt;-\n  future |&gt;\n  select(-year_num, -month_num, -day_num) |&gt;\n  mutate(date2 = ds) |&gt;\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') |&gt;\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num)) |&gt; \n  arrange(ds)\n\nglimpse(future)\n\n\nRows: 5,757\nColumns: 17\n$ close         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ ds            &lt;date&gt; 2002-04-12, 2002-04-15, 2002-04-16, 2002-04-17, 2002-04…\n$ actual_day    &lt;ord&gt; Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, M…\n$ clean         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ observed      &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ season        &lt;dbl&gt; -0.0036928764, 0.0015302364, -0.0071242584, 0.0004207782…\n$ trend         &lt;dbl&gt; 13.40549, 13.41502, 13.42456, 13.43409, 13.44363, 13.453…\n$ remainder     &lt;dbl&gt; -0.071797724, -0.016554889, 0.152565554, -0.074513535, -…\n$ seasadj       &lt;dbl&gt; 13.33369, 13.39847, 13.57712, 13.35958, 13.09113, 12.933…\n$ remainder_l1  &lt;dbl&gt; -2.211971, -2.211971, -2.211971, -2.211971, -2.211971, -…\n$ remainder_l2  &lt;dbl&gt; 2.223683, 2.223683, 2.223683, 2.223683, 2.223683, 2.2236…\n$ anomaly       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ recomposed_l1 &lt;dbl&gt; 11.18983, 11.20458, 11.20546, 11.22254, 11.24052, 11.237…\n$ recomposed_l2 &lt;dbl&gt; 15.62548, 15.64024, 15.64112, 15.65820, 15.67618, 15.673…\n$ year_num      &lt;fct&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 20…\n$ month_num     &lt;fct&gt; 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 05, …\n$ day_num       &lt;dbl&gt; 12, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 29, 30, 1, 2…\n\n\nCode\ntest_model1 &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) |&gt;\n  pluck(1)\n\ntest_model1 |&gt; \n  modeltime_refit(data = future) |&gt; \n  modeltime_forecast(new_data = future,\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Forecasted JetBlue Stock Prices')"
  },
  {
    "objectID": "posts/2024-03-09-bayesian-networks-pt-1/index.html",
    "href": "posts/2024-03-09-bayesian-networks-pt-1/index.html",
    "title": "Bayes Nets Pt. 1",
    "section": "",
    "text": "Under Development\nAs I am continuing to grow in understanding and conducting bayesian networks, this page and series may change in the future. -JP\nOkay, I will be the first to state that I am not an expert in the field of conducint bayeaian networks, bayesian analyses, statistics (the list goes on), but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses the programming language Stan. There are several tutorials on how to download Stan using either R or Python, so I will not be covering that. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using Python and Stan. Additionally, I will be creating data that will represent educational assessment data, with latent variables representing proficiency in certain skills (e.g., math, English/language arts, and science) for students. While most of my experience of using bayes nets is to represent measurement models, bayes net can be used outside of this field. Bayes net is similar to path analysis and structural equation modeling; however, EXPLAIN DIFFERENCE BETWEEN THE TWO METHODS. I will also start referring to everything in this series in a bayesian network framework. For instance, instead of using variables, whether they are observed or unobserved (latent), I will be referring to them as nodes and latent nodes, respectively. When it comes to showing the “paths” between nodes, I wwill now be referring to them as edges. Lastly, any image that shows all of the nodes and the edges connecting to one another will be referred to as a directed acyclic graph or DAG.\nOkay, now on to this post. For this post I will simply discuss creating the data in R to be used in Stan, as well as creating the object of data that will be used in the Stan calculations. One last comment before diving in, I will be using cmdstanr instead of rstan for my Stan computations."
  },
  {
    "objectID": "posts/2024-03-09-bayesian-networks-pt-1/index.html#getting-the-data-set-up",
    "href": "posts/2024-03-09-bayesian-networks-pt-1/index.html#getting-the-data-set-up",
    "title": "Bayes Nets Pt. 1",
    "section": "Getting the Data Set Up",
    "text": "Getting the Data Set Up\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(bayestestR)\nlibrary(bayesplot)\nlibrary(posterior)\n\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 1000, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .8),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nThe first thing I am going to do is load in all the necessary libraries that you need. Then I decided to create a function that would create a binomial distribution with a single trial, so essentially a bernoulii distribution. I decided on some random numbers for the probabilities of correct responses to the 15 different items and decided to create some fake studentids for each row.\n\n\n\n\n\n\n\n\n\nI decided to create a simple table that shows all of the students and their responses for the 15 items in this assessment. I’m not sure why I have all the data in the table, but I used some pagination so there is not a laundry list of rows with 0s and 1s clogging up this post…hopefully.\n\n# map(y |&gt; select(-studentid), table)\n# map(y |&gt; select(-studentid), ~round(prop.table(table(.x)), 2))\n\nmap(y |&gt; select(-studentid), table)[[1]]\n\n\n  0   1 \n199 801 \n\n\nAfter seeing that the data looks correct, I am also neurotic and need to make sure that my created data is how I imagined it would be. So I looped through each of my items to make sure the proportions are correct. More importantly, I like to see the counts of the data and get an understanding of how many are answering each item correctly. I commented out the loop and am only going to show the counts for the first item. So seeing at how my function had approximately 80% of the students answering the item correctly, I can now see that 801 answered item 1 correctly.\n\nQ Matrix\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  gt::gt() |&gt;\n  gt::opt_interactive(\n    use_filters = TRUE,\n    use_resizers = TRUE,\n    use_highlight = TRUE,\n    use_compact_mode = TRUE\n  )\n\n\n\n\n\n\n\n\nOkay, now on to the Q-matrix. This is the only other piece of information we may need for our model in Stan. WARNING I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The gt table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column. So now I believe we have everything we need to get started on a bayes net using Stan and Markov chain Monte Carlo (MCMC) sampling.\n\n\nStan Data\n\nstan_data &lt;- list(\n  J = nrow(y[, -1]), # Number of students/rows\n  I = ncol(y[, -1]), # Number of items\n  K = ncol(q_matrix[, -1]), #Number of latent attributes/skills\n  y = y[,-1], # Student responses on all items\n  Q = q_matrix[,-1] # Items that measure each attribute\n)\n\nprint(stan_data)\nglimpse(stan_data)"
  },
  {
    "objectID": "posts/2024-03-16-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-03-16-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Under Development - Not Complete\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ndata { int&lt;lower=1&gt; J; // number of examinees int&lt;lower=1&gt; I; // number of items int&lt;lower=1&gt; K; // number of latent variables int&lt;lower=1&gt; C; // number of classes matrix[J, I] X; // response matrix x matrix[I, K] Q; // Q matrix Q matrix[C, K] alpha; // attribute profile matrix } parameters { simplex[C] nu; // class probabilities vector&lt;lower=0, upper=1&gt;[I] false_pos; vector&lt;lower=0, upper=1&gt;[I] true_pos; real&lt;lower=0, upper=1&gt; lambda1; real&lt;lower=0, upper=1&gt; lambda20; real&lt;lower=0, upper=1&gt; lambda21; real&lt;lower=0, upper=1&gt; lambda30; real&lt;lower=0, upper=1&gt; lambda31; real&lt;lower=0, upper=1&gt; lambda40; real&lt;lower=0, upper=1&gt; lambda41; real&lt;lower=0, upper=1&gt; lambda50; real&lt;lower=0, upper=1&gt; lambda51; } transformed parameters { vector[C] log_nu; log_nu = log(nu); } model { vector[2] theta_log1; vector[2] theta_log2; vector[2] theta_log3; vector[2] theta_log4; vector[2] theta_log5; vector[C] theta1; vector[C] theta2; vector[C] theta3; vector[C] theta4; vector[C] theta5; matrix[I, C] delta; real pie; vector[I] log_item; vector[C] log_lik;\n// Priors lambda1 ~ beta(25, 5); lambda20 ~ beta(10, 20); lambda21 ~ beta(20, 10); lambda30 ~ beta(5, 25); lambda31 ~ beta(25, 5); lambda40 ~ beta(5, 25); lambda41 ~ beta(25, 5); lambda50 ~ beta(12, 18); lambda51 ~ beta(18, 12);\nfor (i in 1 : I) { false_pos[i] ~ beta(4, 26); true_pos[i] ~ beta(26, 4); }\ntheta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1); theta_log1[2] = bernoulli_lpmf(1 | lambda1);\ntheta_log2[1] = bernoulli_lpmf(1 | lambda20); theta_log2[2] = bernoulli_lpmf(1 | lambda21);\ntheta_log3[1] = bernoulli_lpmf(1 | lambda30); theta_log3[2] = bernoulli_lpmf(1 | lambda31);\ntheta_log4[1] = bernoulli_lpmf(1 | lambda40); theta_log4[2] = bernoulli_lpmf(1 | lambda41);\ntheta_log5[1] = bernoulli_lpmf(1 | lambda50); theta_log5[2] = bernoulli_lpmf(1 | lambda51);\nfor (c in 1 : C) { if (alpha[c, 1] &gt; 0) { theta1[c] = theta_log1[2]; } else { theta1[c] = theta_log1[1]; } if (alpha[c, 2] &gt; 0) { theta2[c] = theta_log2[2]; } else { theta2[c] = theta_log2[1]; } if (alpha[c, 3] &gt; 0) { theta3[c] = theta_log3[2]; } else { theta3[c] = theta_log3[1]; } if (alpha[c, 4] &gt; 0) { theta4[c] = theta_log4[2]; } else { theta4[c] = theta_log4[1]; } if (alpha[c, 5] &gt; 0) { theta5[c] = theta_log5[2]; } else { theta5[c] = theta_log5[1]; } }\n//Likelihood for (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2]) * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4]) * pow(exp(theta5[c]), Q[i, 5]);\n    pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n    log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n  }\n  log_lik[c] = log_nu[c] + sum(log_item);\n}\ntarget += log_sum_exp(log_lik);\n} } generated quantities { vector[2] theta_log1; vector[2] theta_log2; vector[2] theta_log3; vector[2] theta_log4; vector[2] theta_log5; vector[C] theta1; vector[C] theta2; vector[C] theta3; vector[C] theta4; vector[C] theta5; matrix[I, C] delta; real pie; vector[I] log_item;\nmatrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k row_vector[C] prob_joint; vector[C] prob_attr_class;\nmatrix[J, I] x_rep;\ntheta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1); theta_log1[2] = bernoulli_lpmf(1 | lambda1);\ntheta_log2[1] = bernoulli_lpmf(1 | lambda20); theta_log2[2] = bernoulli_lpmf(1 | lambda21);\ntheta_log3[1] = bernoulli_lpmf(1 | lambda30); theta_log3[2] = bernoulli_lpmf(1 | lambda31);\ntheta_log4[1] = bernoulli_lpmf(1 | lambda40); theta_log4[2] = bernoulli_lpmf(1 | lambda41);\ntheta_log5[1] = bernoulli_lpmf(1 | lambda50); theta_log5[2] = bernoulli_lpmf(1 | lambda51);\nfor (c in 1 : C) { if (alpha[c, 1] &gt; 0) { theta1[c] = theta_log1[2]; } else { theta1[c] = theta_log1[1]; } if (alpha[c, 2] &gt; 0) { theta2[c] = theta_log2[2]; } else { theta2[c] = theta_log2[1]; } if (alpha[c, 3] &gt; 0) { theta3[c] = theta_log3[2]; } else { theta3[c] = theta_log3[1]; } if (alpha[c, 4] &gt; 0) { theta4[c] = theta_log4[2]; } else { theta4[c] = theta_log4[1]; } if (alpha[c, 5] &gt; 0) { theta5[c] = theta_log5[2]; } else { theta5[c] = theta_log5[1]; } }\nfor (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2]) * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4]) * pow(exp(theta5[c]), Q[i, 5]);\n    pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n    log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n    }\n  prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n}\nprob_resp_class[j] = prob_joint / sum(prob_joint);\n}\nfor (j in 1 : J) { for (k in 1 : K) { for (c in 1 : C) { // Calculate the probability of mastering attribute k given class c prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k]; } // Sum the probabilities to get the posterior probability of mastering attribute k prob_resp_attr[j, k] = sum(prob_attr_class); } }\nfor (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie); } } } }"
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html",
    "title": "Using Typst To Create Documents",
    "section": "",
    "text": "Under Development - Not Complete\nI have been using Typst, an awesome app for working on pdf files at the same time as colleagues. You could think of this as something similar to Google Docs or GitHub for code. This also got me thinking about creating a small series of blog posts about using Typst and then creating Typst templates for documents using Quarto. The latter topics would be using Quarto extensions and if following along, you would need Quarto version 1.4 at least to be able to use Typst code chunks on a Quarto document. So first, I will show the Typst file I will be using because let’s face it I’m on the job market and free publicity is always good.\n.\nHere is the link for the Typst resume to view. If you want, you can just copy and paste that into Typst and change the information. I will walk through each section of the document with Typst code in the post, as well as a cover letter post, and then end the series with a Quarto extension to create a Typst template so you can just write your resume and/or cover letter in Quarto."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#typst-documentation",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#typst-documentation",
    "title": "Using Typst To Create Documents",
    "section": "Typst Documentation",
    "text": "Typst Documentation\nI will be the first to state that Typst documentation is a little difficult to follow at first. Hopefully with this tutorial you will get a better understanding of the basics of Typst code. As someone who tried to learn LaTeX to edit the previous resume I had found a template for, I wish Typst existed earlier."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "title": "Using Typst To Create Documents",
    "section": "Creating a Typst Document",
    "text": "Creating a Typst Document\nWorking in the Typst app is pretty straightforward with you creating a username, followed by your dashboard with nothing there. This will be the location of all of your documents as you get started with Typst. While there are Typst templates already for resumes I really wanted to create something similar to the resume I had in LaTeX. Working in Quarto, you will have to learn how to create Typst code chunks. They are slightly different from other languages’ code chunks but you can still use all of the Quarto code chunk arguments.\nFrom what I have seen online, there does not seem to be much difference in the ordering of some of the beginning Typst documentation. I have decided to start my Typst document with any variables I will be including, followed by any Typst packages I will need, and then setting up the general parameters for the document. These general parameters are for the document overall. Things like setting the font to a specific font, size and maybe weight would be a good parameter to set at the top of your document."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "title": "Using Typst To Create Documents",
    "section": "Breakdown of Variables, Packages, and # Set function for parameters",
    "text": "Breakdown of Variables, Packages, and # Set function for parameters\n\n//Variables\n#let name = [Jonathan A. Pedroza Ph.D]\n\n//Packages\n#import \"@preview/tablex:0.0.8\": tablex\n#import \"lib-gen.typ\": *\n#import \"lib-impl.typ\": *\n#import \"lib.typ\": *\n\n#set page(\n  margin: (\n    top: 0cm,\n    bottom: 0cm,\n    left: .5cm,\n    right: 0cm\n  )\n)\n\n#set block(spacing: 0.5em)\n\n#set rect(\n  width: 37%,\n  height: 100%\n)\n\nAbove is the beginning Typst code for the resume I created. I’m going to walk through the code a little, but for more detailed information, check out the help documentation. The // syntax can be included wherever to include comments. Since I have been showing others how to use Typst to create quick pretty PDFs, I have been including a lot of comments for things like variables. To create variables in Typst, you will need to use the #let function followed by your variable name, an equal sign, and the information you want to include. So I created the variable name, which would be used as #name in Typst and the document will spell out my full name. After that, just as the comment states, I included the tablex package, which I have found to be useful for creating tables and grids. If you’d like you can use the #table or #grid functions from Typst. Additionally, I also included the files for using the FontAwesome Typst package. You can find all the icons and other information about FontAwesome at the FontAwesome website. I was just being lazy as I wanted to create my resume quick so I could get it out into the world ASAP. Next I set the margins to maximize the amount of space I would have for my resume and I created a block after the titles Education and Professional Experience since I did not want the default amount of space before my education and experience entries. The #set function creates rules for the document as a whole. So For the whole document I have the same margins throughout, a block to create more space between headers and text underneath and the rectangle to separate the sections of the document. Lastly, I set a rectangle for 37% of the document’s width and 100% of the height. There are other metrics that can be used to create the rectangle but I personally was enjoying using percentages for this document. This rectangle is for the right side of the document that includes the contact information."
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html",
    "title": "Data Manipulation in R & Python",
    "section": "",
    "text": "One of my favorite posts is the comparison between data.table and the tidyverse’s dplyr packages. Here is the link to that post. I have used that when trying to build my competence in using data.table. Now I’m going to try and expand on that by creating this post that compares cases of using dplyr, data.table, and now pandas. Hopefully this can be as useful as the comparison between dplyr an data.table post was for me. This is not an extensive way of comparing them but just to get started for anyone that wants to use python more."
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-integers",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-integers",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Integers)",
    "text": "Filtering (Integers)\n\n\nCode\nr_data |&gt;\n  filter(\n    x &gt; 1\n  ) |&gt;\n  head()\n\n\n# A tibble: 6 × 3\n      x    x2     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  1.26  6.09     1\n2  1.13 -1.44     1\n3  2.12  2.60     0\n4  1.00  2.13     1\n5  1.59  3.99     1\n6  2.20 -1.62     1\n\n\n\n\nCode\nhead(\n  r_table[x &gt; 1]\n)\n\n\n          x         x2     y\n      &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1: 1.674498  3.6659813     0\n2: 3.004482  3.0656903     1\n3: 2.551585  8.1276025     1\n4: 1.733821 -0.8900152     1\n5: 1.263874 -1.2767909     1\n6: 1.540138  3.6267555     0\n\n\n\n\nCode\npy_data[py_data[\"x\"] &gt; 1].head()\n\n\n           x        x2  y\n0   1.120220  3.810238  1\n20  1.515091  1.237811  0\n28  2.129698  3.715099  1\n29  1.028248  0.270362  1\n33  1.886046  0.232325  1\n\n\n\n\nCode\npl_data.filter(pl.col('x') &gt; 1).head()\n\n\n\n\nshape: (5, 3)\n\n\n\nx\nx2\ny\n\n\nf64\nf64\ni32\n\n\n\n\n1.12022\n3.810238\n1\n\n\n1.515091\n1.237811\n0\n\n\n2.129698\n3.715099\n1\n\n\n1.028248\n0.270362\n1\n\n\n1.886046\n0.232325\n1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-categorical",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-categorical",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Categorical)",
    "text": "Filtering (Categorical)\n\n\nCode\nr_data |&gt;\n  filter(\n    y == 1\n  ) |&gt;\n  head()\n\n\n# A tibble: 6 × 3\n        x    x2     y\n    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  0.409   1.17     1\n2 -0.846  -3.77     1\n3  0.0941 -2.09     1\n4  0.322   1.16     1\n5  1.26    6.09     1\n6  0.423  -2.02     1\n\n\n\n\nCode\nhead(\n  r_table[y == 1]\n)\n\n\n            x         x2     y\n        &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1: -0.9763540 -1.3373400     1\n2: -0.5736134  2.5926507     1\n3: -0.8102045 -2.5416087     1\n4: -0.4948199  0.4980446     1\n5:  0.5398953  3.0874990     1\n6:  0.1005187  0.3697423     1\n\n\n\n\nCode\npy_data[py_data[\"y\"] == 1].head()\n\n\n          x        x2  y\n0  1.120220  3.810238  1\n3  0.134986 -0.321329  1\n5 -1.493926  3.362695  1\n8  0.907419  4.916472  1\n9 -0.244917 -3.831616  1\n\n\n\n\nCode\npl_data.filter(pl.col('y') == 1).head()\n\n\n\n\nshape: (5, 3)\n\n\n\nx\nx2\ny\n\n\nf64\nf64\ni32\n\n\n\n\n1.12022\n3.810238\n1\n\n\n0.134986\n-0.321329\n1\n\n\n-1.493926\n3.362695\n1\n\n\n0.907419\n4.916472\n1\n\n\n-0.244917\n-3.831616\n1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering Multiple Columns",
    "text": "Filtering Multiple Columns\n\n\nCode\nr_data |&gt;\n  filter(\n    y == 1 &\n    x2 &lt; 0\n  ) |&gt;\n  head()\n\n\n# A tibble: 6 × 3\n        x     x2     y\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 -0.846  -3.77      1\n2  0.0941 -2.09      1\n3  0.423  -2.02      1\n4  0.631  -0.861     1\n5  0.0307 -0.592     1\n6  0.759  -4.35      1\n\n\n\n\nCode\nhead(\n  r_table[\n    y == 1 &\n    x2 &gt; 0\n  ]\n)\n\n\n            x        x2     y\n        &lt;num&gt;     &lt;num&gt; &lt;int&gt;\n1: -0.5736134 2.5926507     1\n2: -0.4948199 0.4980446     1\n3:  0.5398953 3.0874990     1\n4:  0.1005187 0.3697423     1\n5:  0.2489273 3.2787428     1\n6: -1.4057361 3.9976275     1\n\n\n\n\nCode\npy_data[\n  (py_data[\"y\"] == 1) & \n  (py_data[\"x2\"] &gt; 0)\n    ].head()\n\n\n           x        x2  y\n0   1.120220  3.810238  1\n5  -1.493926  3.362695  1\n8   0.907419  4.916472  1\n15  0.091052  1.497482  1\n17 -0.438709  0.148849  1\n\n\n\n\nCode\npl_data.filter(pl.col('y') == 1, pl.col('x2') &gt; 0).head()\n\n\n\n\nshape: (5, 3)\n\n\n\nx\nx2\ny\n\n\nf64\nf64\ni32\n\n\n\n\n1.12022\n3.810238\n1\n\n\n-1.493926\n3.362695\n1\n\n\n0.907419\n4.916472\n1\n\n\n0.091052\n1.497482\n1\n\n\n-0.438709\n0.148849\n1\n\n\n\n\n\n\n\nCode\n# uses a comma instead of using &"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#sorting-rows",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#sorting-rows",
    "title": "Data Manipulation in R & Python",
    "section": "Sorting Rows",
    "text": "Sorting Rows\n\n\nCode\nr_data |&gt; \n  arrange(y) |&gt;\n  head()\n\n\n# A tibble: 6 × 3\n        x     x2     y\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 -1.39    1.35      0\n2  0.710  -0.134     0\n3 -2.00    0.169     0\n4 -0.116  -4.16      0\n5  0.328  -2.07      0\n6 -0.0749  3.35      0\n\n\n\n\nCode\nhead(\n  r_table[order(y)]\n)\n\n\n            x         x2     y\n        &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1: -0.1754454 -0.3782700     0\n2:  0.8446304 -0.2514460     0\n3: -0.1584563  1.2388419     0\n4:  1.6744982  3.6659813     0\n5:  0.1953033 -6.5178594     0\n6:  0.2178685 -0.9440392     0\n\n\n\n\nCode\npy_data.sort_values(by = \"y\").head()\n\n\n            x        x2  y\n991 -1.018447  1.546185  0\n990  0.073119  2.523895  0\n989  0.764942 -0.644956  0\n986  0.295163  2.893048  0\n984  1.247925  2.175303  0\n\n\n\n\nCode\npl_data.sort(pl.col('y')).head()\n\n\n\n\nshape: (5, 3)\n\n\n\nx\nx2\ny\n\n\nf64\nf64\ni32\n\n\n\n\n0.947493\n-4.892057\n0\n\n\n0.679544\n-0.393661\n0\n\n\n0.551375\n-1.034227\n0\n\n\n-0.261602\n1.120449\n0\n\n\n-0.500885\n1.855574\n0"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-specific-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-specific-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Specific Columns",
    "text": "Selecting Specific Columns\n\n\nCode\nr_data |&gt;\n  select(\n    y\n  ) |&gt;\n  head()\n\n\n# A tibble: 6 × 1\n      y\n  &lt;int&gt;\n1     1\n2     1\n3     1\n4     1\n5     0\n6     1\n\n\n\n\nCode\nhead(\n  r_table[,\"y\"]\n)\n\n\n       y\n   &lt;int&gt;\n1:     0\n2:     1\n3:     0\n4:     1\n5:     1\n6:     0\n\n\n\n\nCode\npy_data[\"y\"].head()\n\n\n0    1\n1    0\n2    0\n3    1\n4    0\nName: y, dtype: int32\n\n\nCode\n\n# py_data.filter(items = \"y\").head()\n\n\n\n\nCode\npl_data.select(pl.col('y')).head()\n\n\n\n\nshape: (5, 1)\n\n\n\ny\n\n\ni32\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Multiple Columns",
    "text": "Selecting Multiple Columns\n\n\nCode\nr_data |&gt; \n  select(x, x2) |&gt; \n  head()\n\n\n# A tibble: 6 × 2\n        x    x2\n    &lt;dbl&gt; &lt;dbl&gt;\n1  0.409   1.17\n2 -0.846  -3.77\n3  0.0941 -2.09\n4  0.322   1.16\n5 -1.39    1.35\n6  1.26    6.09\n\n\n\n\nCode\nhead(\n  r_table[,list(x, x2)]\n)\n\n\n            x        x2\n        &lt;num&gt;     &lt;num&gt;\n1: -0.1754454 -0.378270\n2: -0.9763540 -1.337340\n3:  0.8446304 -0.251446\n4: -0.5736134  2.592651\n5: -0.8102045 -2.541609\n6: -0.1584563  1.238842\n\n\n\n\nCode\npy_data[[\"x\", \"x2\"]].head()\n\n\n          x        x2\n0  1.120220  3.810238\n1  0.947493 -4.892057\n2  0.679544 -0.393661\n3  0.134986 -0.321329\n4  0.551375 -1.034227\n\n\nCode\n# or\npy_data.filter(items = [\"x\", \"x2\"]).head()\n\n\n          x        x2\n0  1.120220  3.810238\n1  0.947493 -4.892057\n2  0.679544 -0.393661\n3  0.134986 -0.321329\n4  0.551375 -1.034227\n\n\n\n\nCode\npl_data.select(pl.col('x'), pl.col('x2')).head()\n\n\n\n\nshape: (5, 2)\n\n\n\nx\nx2\n\n\nf64\nf64\n\n\n\n\n1.12022\n3.810238\n\n\n0.947493\n-4.892057\n\n\n0.679544\n-0.393661\n\n\n0.134986\n-0.321329\n\n\n0.551375\n-1.034227"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-using-regex",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-using-regex",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Using Regex",
    "text": "Selecting Using Regex\n\n\nCode\nr_data |&gt;\n  select(\n    matches(\"x\")\n  ) |&gt;\n  head()\n\n\n# A tibble: 6 × 2\n        x    x2\n    &lt;dbl&gt; &lt;dbl&gt;\n1  0.409   1.17\n2 -0.846  -3.77\n3  0.0941 -2.09\n4  0.322   1.16\n5 -1.39    1.35\n6  1.26    6.09\n\n\n\n\nCode\ncols &lt;- grep(\"^x\", names(r_table))\n\nhead(\n  r_table[, ..cols]\n)\n\n\n            x        x2\n        &lt;num&gt;     &lt;num&gt;\n1: -0.1754454 -0.378270\n2: -0.9763540 -1.337340\n3:  0.8446304 -0.251446\n4: -0.5736134  2.592651\n5: -0.8102045 -2.541609\n6: -0.1584563  1.238842\n\n\n\n\nCode\npy_data.filter(regex = \"x\").head()\n\n\n          x        x2\n0  1.120220  3.810238\n1  0.947493 -4.892057\n2  0.679544 -0.393661\n3  0.134986 -0.321329\n4  0.551375 -1.034227\n\n\n\n\nCode\nimport polars.selectors as cs\n\npl_data.select(cs.starts_with('x')).head()\n\n\n\n\nshape: (5, 2)\n\n\n\nx\nx2\n\n\nf64\nf64\n\n\n\n\n1.12022\n3.810238\n\n\n0.947493\n-4.892057\n\n\n0.679544\n-0.393661\n\n\n0.134986\n-0.321329\n\n\n0.551375\n-1.034227"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#summarize-data",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#summarize-data",
    "title": "Data Manipulation in R & Python",
    "section": "Summarize Data",
    "text": "Summarize Data\n\n\nCode\nr_data |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n\n# A tibble: 1 × 1\n      avg\n    &lt;dbl&gt;\n1 -0.0366\n\n\nCode\n  r_data |&gt;\n  summarize(\n    total = sum(x)\n  )\n\n\n# A tibble: 1 × 1\n  total\n  &lt;dbl&gt;\n1 -36.6\n\n\n\n\nCode\nr_table[, .(avg = mean(x))]\n\n\n          avg\n        &lt;num&gt;\n1: 0.05005754\n\n\nCode\nr_table[, .(total = sum(x))]\n\n\n      total\n      &lt;num&gt;\n1: 50.05754\n\n\n\n\nCode\npy_data[\"x\"].mean()\n\n\nnp.float64(0.04849950488018116)\n\n\nCode\npy_data[\"x\"].sum()\n\n\nnp.float64(48.49950488018116)\n\n\n\n\nCode\npl_data.select(pl.mean('x'))\n\n\n\n\nshape: (1, 1)\n\n\n\nx\n\n\nf64\n\n\n\n\n0.0485\n\n\n\n\n\n\n\nCode\npl_data.select(pl.sum('x'))\n\n\n\n\nshape: (1, 1)\n\n\n\nx\n\n\nf64\n\n\n\n\n48.499505"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Adding/Updating/Deleting Columns",
    "text": "Adding/Updating/Deleting Columns\n\n\nCode\nr_data &lt;- r_data |&gt;\n  mutate(\n    x_mult = x*x2\n  )\nhead(r_data)\n\n\n# A tibble: 6 × 4\n        x    x2     y x_mult\n    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1  0.409   1.17     1  0.478\n2 -0.846  -3.77     1  3.18 \n3  0.0941 -2.09     1 -0.197\n4  0.322   1.16     1  0.374\n5 -1.39    1.35     0 -1.88 \n6  1.26    6.09     1  7.67 \n\n\n\n\nCode\nr_table[, x_mult := x*x2]\nhead(r_table[, \"x_mult\"])\n\n\n        x_mult\n         &lt;num&gt;\n1:  0.06636573\n2:  1.30571727\n3: -0.21237892\n4: -1.48717919\n5:  2.05922268\n6: -0.19630230\n\n\n\n\nCode\npy_data[\"x_mult\"] = py_data[\"x\"] * py_data[\"x2\"]\npy_data[\"x_mult\"].head()\n\n\n0    4.268305\n1   -4.635190\n2   -0.267510\n3   -0.043375\n4   -0.570247\nName: x_mult, dtype: float64\n\n\n\n\nCode\npl_data.with_columns((pl.col('x') * pl.col('x2')).alias('x_mult'))\n\n\n\n\nshape: (1_000, 4)\n\n\n\nx\nx2\ny\nx_mult\n\n\nf64\nf64\ni32\nf64\n\n\n\n\n1.12022\n3.810238\n1\n4.268305\n\n\n0.947493\n-4.892057\n0\n-4.63519\n\n\n0.679544\n-0.393661\n0\n-0.26751\n\n\n0.134986\n-0.321329\n1\n-0.043375\n\n\n0.551375\n-1.034227\n0\n-0.570247\n\n\n…\n…\n…\n…\n\n\n1.635647\n-1.750633\n1\n-2.863418\n\n\n-0.604228\n3.279034\n0\n-1.981286\n\n\n0.428933\n0.942513\n0\n0.404275\n\n\n1.734978\n-4.54333\n0\n-7.882577\n\n\n-0.218969\n4.305527\n0\n-0.942777"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#counting",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#counting",
    "title": "Data Manipulation in R & Python",
    "section": "Counting",
    "text": "Counting\n\n\nCode\nr_data |&gt; count(y)\n\n\n# A tibble: 2 × 2\n      y     n\n  &lt;int&gt; &lt;int&gt;\n1     0   406\n2     1   594\n\n\n\n\nCode\nr_table[, .N, by = (y)]\n\n\n       y     N\n   &lt;int&gt; &lt;int&gt;\n1:     0   400\n2:     1   600\n\n\n\n\nCode\npy_data[\"y\"].value_counts()\n\n\ny\n1    581\n0    419\nName: count, dtype: int64\n\n\n\n\nCode\npl.Series(pl_data.select(pl.col('y'))).value_counts()\n\n\n\n\nshape: (2, 2)\n\n\n\ny\ncount\n\n\ni32\nu32\n\n\n\n\n0\n419\n\n\n1\n581"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#group-by",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#group-by",
    "title": "Data Manipulation in R & Python",
    "section": "Group By",
    "text": "Group By\n\n\nCode\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n\n# A tibble: 2 × 2\n      y     avg\n  &lt;int&gt;   &lt;dbl&gt;\n1     0 -0.0440\n2     1 -0.0316\n\n\n\n\nCode\nr_table[, .(avg = mean(x)), by = \"y\"]\n\n\n       y         avg\n   &lt;int&gt;       &lt;num&gt;\n1:     0 0.001986386\n2:     1 0.082104971\n\n\n\n\nCode\npy_data.groupby(\"y\")[\"x\"].mean()\n\n\ny\n0    0.095591\n1    0.014539\nName: x, dtype: float64\n\n\n\n\nCode\npl_data.group_by('y').agg(pl.col('x').mean())\n\n\n\n\nshape: (2, 2)\n\n\n\ny\nx\n\n\ni32\nf64\n\n\n\n\n1\n0.014539\n\n\n0\n0.095591"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#chain-expressions",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#chain-expressions",
    "title": "Data Manipulation in R & Python",
    "section": "Chain Expressions",
    "text": "Chain Expressions\n\n\nCode\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  ) |&gt;\n  filter(\n    y == 1\n  )\n\n\n# A tibble: 1 × 2\n      y     avg\n  &lt;int&gt;   &lt;dbl&gt;\n1     1 -0.0316\n\n\n\n\nCode\nr_table[, \n  by = y,\n  .(avg = mean(x))\n  ][\n    y == 1\n  ]\n\n\n       y        avg\n   &lt;int&gt;      &lt;num&gt;\n1:     1 0.08210497\n\n\n\n\nCode\npy_group = py_data.groupby(\"y\")[\"x\"].mean().reset_index()\n\npy_group.iloc[1:, ]\n\n\n   y         x\n1  1  0.014539\n\n\n\n\nCode\npl_group = pl_data.group_by('y').agg(pl.col('x').mean())\n\npl_group.filter(pl.col('y') == 1)\n\n\n\n\nshape: (1, 2)\n\n\n\ny\nx\n\n\ni32\nf64\n\n\n\n\n1\n0.014539"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#pivot-data",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#pivot-data",
    "title": "Data Manipulation in R & Python",
    "section": "Pivot Data",
    "text": "Pivot Data\n\n\nCode\nr_data |&gt;\n  pivot_longer(\n    -y\n  ) |&gt;\n  head()\n\n\n# A tibble: 6 × 3\n      y name    value\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 x       0.409\n2     1 x2      1.17 \n3     1 x_mult  0.478\n4     1 x      -0.846\n5     1 x2     -3.77 \n6     1 x_mult  3.18 \n\n\n\n\nCode\nhead(melt(r_table, id.vars = \"y\"))\n\n\n       y variable      value\n   &lt;int&gt;   &lt;fctr&gt;      &lt;num&gt;\n1:     0        x -0.1754454\n2:     1        x -0.9763540\n3:     0        x  0.8446304\n4:     1        x -0.5736134\n5:     1        x -0.8102045\n6:     0        x -0.1584563\n\n\n\n\nCode\npy_data.melt(id_vars = ['y'], value_vars = ['x', 'x2', 'x_mult']).head()\n\n\n   y variable     value\n0  1        x  1.120220\n1  0        x  0.947493\n2  0        x  0.679544\n3  1        x  0.134986\n4  0        x  0.551375\n\n\n\n\nCode\npl_data.unpivot(index = 'y').head()\n\n\n\n\nshape: (5, 3)\n\n\n\ny\nvariable\nvalue\n\n\ni32\nstr\nf64\n\n\n\n\n1\n\"x\"\n1.12022\n\n\n0\n\"x\"\n0.947493\n\n\n0\n\"x\"\n0.679544\n\n\n1\n\"x\"\n0.134986\n\n\n0\n\"x\"\n0.551375"
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#grid-of-entries",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#grid-of-entries",
    "title": "Using Typst To Create Documents",
    "section": "Grid of Entries",
    "text": "Grid of Entries\n\n#grid(\n  columns: (70%, 82%),\n  [\n    #linebreak()\n\n    #set text(\n      font: \"Source Sans Pro\",\n      size: 10pt\n    )\n    #set align(center)\n    \n    = Education #fa-graduation-cap()\n    #line(\n      length: 94%,\n      stroke: black\n    )\n\nI decided to separate some of these functions because it might be easier to talk about. So the grid here is actually separating the main text and the contact information text. As I am reading this, I can see that I probably should have set the font to be 10pt throughout the document rather than within the grid."
  },
  {
    "objectID": "posts/2024-07-09-optimal-threshold/index.html",
    "href": "posts/2024-07-09-optimal-threshold/index.html",
    "title": "Finding Optimal Thresholds",
    "section": "",
    "text": "Under Development - Not Complete\nNow that I’m playing catch up with some posts I have wanted to write, I thought now would be an excellent time to write about this method I have been trying out to figure out the closest optimal threshold. While I found other ways to find the optimal threshold at a much faster rate, I still thought this was an interesting use of machine learning to try and figure out the optimal threshold. Particularly, this is a method to try and find an optimal threshold when the truth is not known. From what I could find there was not much literature on trying to find an optimal threshold when the truth was not known. So I’ll first show this method when the truth is known followed by trying this method out when there is no truth."
  },
  {
    "objectID": "posts/2024-07-09-optimal-threshold/index.html#fabricating-some-data",
    "href": "posts/2024-07-09-optimal-threshold/index.html#fabricating-some-data",
    "title": "Finding Optimal Thresholds",
    "section": "Fabricating Some Data",
    "text": "Fabricating Some Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071)\ncat_map &lt;- purrr::map\n\nn &lt;- 500\nseed &lt;- 12345\ntruth &lt;- rbinom(n = n, size = 1, prob = .6)\nestimates &lt;- bayestestR::distribution_beta(n = n, shape1 = 12, shape2 = 18)\n\ndata &lt;- tibble(\n  estimates = estimates,\n  truth = truth\n)\n\ndata |&gt; head()\n\n# A tibble: 6 × 2\n  estimates truth\n      &lt;dbl&gt; &lt;int&gt;\n1     0.160     0\n2     0.181     0\n3     0.192     0\n4     0.200     1\n5     0.206     1\n6     0.212     1\n\ndata_train &lt;- data |&gt; slice_sample(prop = .75)\ndata_test &lt;- anti_join(data, data_train)\n\nJoining with `by = join_by(estimates, truth)`\n\n\n\nset.seed(seed)\nclass_weights &lt;- data_train |&gt; count(truth) |&gt; arrange(n)\ncls_weights &lt;- class_weights[2, 2]/class_weights[1, 2]\n\nsvm_mod &lt;- svm(\n  truth ~ estimates,\n  data = data_train,\n  type = \"C-classification\",\n  kernel = \"radial\",\n  cost = 10,\n  class.weights = c(\"0\" = as.numeric(cls_weights), \"1\" = 1),\n  probability = TRUE\n)\nsummary(svm_mod)\n\n\nCall:\nsvm(formula = truth ~ estimates, data = data_train, type = \"C-classification\", \n    kernel = \"radial\", cost = 10, class.weights = c(`0` = as.numeric(cls_weights), \n        `1` = 1), probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  10 \n\nNumber of Support Vectors:  354\n\n ( 198 156 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\nsvm_pred &lt;- predict(svm_mod, data_test)\nsvm_pred |&gt; as_tibble() |&gt; count(value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;fct&gt; &lt;int&gt;\n1 0        69\n2 1        56\n\n\n\nmap_dbl(\n  data$estimates,\n  ~rbinom(n = 1, size = 1, prob = .x)\n)\n\n  [1] 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n [38] 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1\n [75] 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n[112] 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1\n[149] 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0\n[186] 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n[223] 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n[260] 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1\n[297] 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n[334] 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n[371] 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n[408] 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n[445] 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1\n[482] 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0\n\n\n\nprobably_threshold &lt;- function(\n  r6,\n  task,\n  threshold,\n  output = TRUE\n){\n  thresholds &lt;- c(threhold, 1 - threshold)\n  names(thresholds) &lt;- {{task}}$class_names\n\n  if(output == TRUE){\n    confuse &lt;- {{r6}}$clone(deep = TRUE)$set_threshold(threhsolds)$confusion |&gt; t()\n    score &lt;- {{r6}}$clone(deep = TRUE)$set_threshold$score(msrs(c(\"classif.mcc\", \"classif.acc\", \"classif.auc\", \"classif.ce\", \"classif.sensitivity\", \"classif.specificity\")))\n\n    list(confuse, score)\n  }\n  else{\n    preds &lt;- {{r6}}$clone(deep = TRUE)$set_threshold(thresholds)\n\n    preds\n  }\n}"
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html",
    "title": "Bayes Nets",
    "section": "",
    "text": "I will be the first to state that I am not an expert in the field of conducting psychometric models, Bayesian networks, Bayesian analyses, but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses Stan. The purpose of this post is to walk through Stan and some bayes net terminology to get a basic understanding of some psychometric models conducted using Bayesian inference.\nTo get started, make sure you follow the detailed instructions on installing RStan. I know if using Mac, make sure to also download Xcode so that Stan will work correctly. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using PyStan, Cmdstanpy, or PyMC, but there are just more readily available tools using R so I will be using R instead. I’m also creating some data to be used in the following posts on latent bayes nets. For these posts, I’ll be creating binary data that will represent items for an education assessment where a 1 indicates that a student has answered the item correctly and a 0 indicates they did not answer the item correctly. The model will also include three latent attributes/skills/variables where a 1 would indicate that the student has mastered the skill and a 0 would indicate that they do not have mastery of the skill.\nWhile I will be discussing bayes net through an educational measurement lens, bayes net can be used outside of education to show that individuals have skills that are not directly measured. Instead of items on an assessment, tasks that capture each skill can be assessed. Before walking through some bayes net terminology, it is important to note that this model is simply for educational purposes. Components of the psychometric models I will be writing about require expert opinion and domain knowledge. For example, bayes net models require expert opinions on the assignment of items to skills. Additionally, bayes net models require expert opinion on the priors for the lambda (\\(\\lambda\\)) parameters.\nSince there is different opinions on using different terms, I am going to stick to the following terms.\nFor this introductory post into bayes net, I thought it would be best to create some artificial data and show visually the models I will be planning on creating using R and Stan. I will be using cmdstanr instead of rstan for my Stan computations. The main difference between the two packages is that rstan avoids using R6 classes, while cmdstanr uses R6 classes. If you’d like more information on trade-offs of different object-oriented programming classes, you can read more here. Finally, I will state that while this is introductory to a bayes net model, this post assumes that you have a basic understanding of Bayesian inference."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#q-matrix",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#q-matrix",
    "title": "Bayes Nets",
    "section": "Q Matrix",
    "text": "Q Matrix\n\n\nCode\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  react_table()\n\n\n\n\n\n\nOkay, now on to the Q-matrix. As previously stated, I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#attribute-profile-matrix",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#attribute-profile-matrix",
    "title": "Bayes Nets",
    "section": "Attribute Profile Matrix",
    "text": "Attribute Profile Matrix\nIf we only wanted to examine how the posterior distributions compare to each student and their responses, then I would only need to have my student data and the Q-matrix. However, I also want to put students into latent classes. Because of this, I also have to create an attribute profile matrix. I am going to create this matrix by creating every possible combination of skills, which will create every potential latent class. Then I will just add each row as a numbered class. Below is the final matrix created for 3 skills.\n\n\nCode\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\nalpha |&gt; react_table()\n\n\n\n\n\n\nNote: Latent classes are different from our latent nodes/attributes/skills. The matrix created above (alpha) is a matrix where each row is a different latent class and each column corresponds to each of the skills.\nSo now we have everything to build our bayes net model. Before we get to that, I do want to visually show the models I will be creating in this series."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#naive-bayes",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#naive-bayes",
    "title": "Bayes Nets",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\nCode\nnaive_dag &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - lambda1\" [latent,pos=\"0.175,0.076\"]\n\"Q-matrix\" [pos=\"0.874,0.402\"]\natt1 [latent,pos=\"0.220,0.209\"]\natt2 [latent,pos=\"0.488,0.182\"]\natt3 [latent,pos=\"0.709,0.169\"]\ndelta [latent,pos=\"0.481,0.421\"]\nfalse_positive [latent,pos=\"0.572,0.888\"]\nlambda1 [latent,pos=\"0.252,0.082\"]\nlambda20 [latent,pos=\"0.450,0.076\"]\nlambda21 [latent,pos=\"0.522,0.081\"]\nlambda30 [latent,pos=\"0.679,0.068\"]\nlambda31 [latent,pos=\"0.741,0.069\"]\ntrue_positive [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - lambda1\" -&gt; att1\n\"Q-matrix\" -&gt; delta\natt1 -&gt; delta\natt2 -&gt; delta\natt3 -&gt; delta\ndelta -&gt; y1\ndelta -&gt; y10\ndelta -&gt; y11\ndelta -&gt; y12\ndelta -&gt; y13\ndelta -&gt; y14\ndelta -&gt; y15\ndelta -&gt; y2\ndelta -&gt; y3\ndelta -&gt; y4\ndelta -&gt; y5\ndelta -&gt; y6\ndelta -&gt; y7\ndelta -&gt; y8\ndelta -&gt; y9\nfalse_positive -&gt; y1\nfalse_positive -&gt; y10\nfalse_positive -&gt; y11\nfalse_positive -&gt; y12\nfalse_positive -&gt; y13\nfalse_positive -&gt; y14\nfalse_positive -&gt; y15\nfalse_positive -&gt; y2\nfalse_positive -&gt; y3\nfalse_positive -&gt; y4\nfalse_positive -&gt; y5\nfalse_positive -&gt; y6\nfalse_positive -&gt; y7\nfalse_positive -&gt; y8\nfalse_positive -&gt; y9\nlambda1 -&gt; att1\nlambda20 -&gt; att2\nlambda21 -&gt; att2\nlambda30 -&gt; att3\nlambda31 -&gt; att3\ntrue_positive -&gt; y1\ntrue_positive -&gt; y10\ntrue_positive -&gt; y11\ntrue_positive -&gt; y12\ntrue_positive -&gt; y13\ntrue_positive -&gt; y14\ntrue_positive -&gt; y15\ntrue_positive -&gt; y2\ntrue_positive -&gt; y3\ntrue_positive -&gt; y4\ntrue_positive -&gt; y5\ntrue_positive -&gt; y6\ntrue_positive -&gt; y7\ntrue_positive -&gt; y8\ntrue_positive -&gt; y9\n}\n')\n\nggdag(naive_dag) + theme_dag()\n\n\n\n\n\n\n\n\n\nThe first model I will go over is essentially a naive bayes model; however, naive bayes models do not correct for what I have labeled as true positive and false positive probabilities. These are priors that will be discussed in the next post. This model mimics a deterministic inputs, noisy “and” gate (DINA) model. Essentially, the model assumes that each student has mastered all skills in order to correctly respond to an assessment item. See here for an excellent post about the DINA model and what the bayes netmodel will compare to in this series"
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#dcm",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#dcm",
    "title": "Bayes Nets",
    "section": "DCM",
    "text": "DCM\nThe DCM is somewhat difficult to visualize. Based on the model in the blog post above, the skills are not determined by priors and instead of a delta parameter, the parameter is already created in the data. I will talk shortly about that when conducting the DCM. Additionally, while the other two models have priors for true positives and false positives, the DCM includes similar parameters with prior distributions; the slip (essentially the student slipped up and made a mistake even though they have the skills) parameter and the guess (got the answer correctly but do not have the necessary skills) parameter. Those priors are different from the ones created for the bayes net models so we’ll talk about those next post."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#bayes-net",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#bayes-net",
    "title": "Bayes Nets",
    "section": "Bayes Net",
    "text": "Bayes Net\n\n\nCode\nbayes_net &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - lambda1\" [latent,pos=\"0.175,0.076\"]\n\"Q-matrix\" [pos=\"0.874,0.402\"]\natt1 [latent,pos=\"0.220,0.209\"]\natt2 [latent,pos=\"0.488,0.182\"]\natt3 [latent,pos=\"0.709,0.169\"]\ndelta [latent,pos=\"0.481,0.421\"]\nfalse_positive [latent,pos=\"0.572,0.888\"]\nlambda1 [latent,pos=\"0.252,0.082\"]\nlambda20 [latent,pos=\"0.450,0.076\"]\nlambda21 [latent,pos=\"0.522,0.081\"]\nlambda30 [latent,pos=\"0.679,0.068\"]\nlambda31 [latent,pos=\"0.741,0.069\"]\ntrue_positive [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - lambda1\" -&gt; att1\n\"Q-matrix\" -&gt; delta\natt1 -&gt; att2\natt1 -&gt; delta\natt2 -&gt; att3\natt2 -&gt; delta\natt3 -&gt; delta\ndelta -&gt; y1\ndelta -&gt; y10\ndelta -&gt; y11\ndelta -&gt; y12\ndelta -&gt; y13\ndelta -&gt; y14\ndelta -&gt; y15\ndelta -&gt; y2\ndelta -&gt; y3\ndelta -&gt; y4\ndelta -&gt; y5\ndelta -&gt; y6\ndelta -&gt; y7\ndelta -&gt; y8\ndelta -&gt; y9\nfalse_positive -&gt; y1\nfalse_positive -&gt; y10\nfalse_positive -&gt; y11\nfalse_positive -&gt; y12\nfalse_positive -&gt; y13\nfalse_positive -&gt; y14\nfalse_positive -&gt; y15\nfalse_positive -&gt; y2\nfalse_positive -&gt; y3\nfalse_positive -&gt; y4\nfalse_positive -&gt; y5\nfalse_positive -&gt; y6\nfalse_positive -&gt; y7\nfalse_positive -&gt; y8\nfalse_positive -&gt; y9\nlambda1 -&gt; att1\nlambda20 -&gt; att2\nlambda21 -&gt; att2\nlambda30 -&gt; att3\nlambda31 -&gt; att3\ntrue_positive -&gt; y1\ntrue_positive -&gt; y10\ntrue_positive -&gt; y11\ntrue_positive -&gt; y12\ntrue_positive -&gt; y13\ntrue_positive -&gt; y14\ntrue_positive -&gt; y15\ntrue_positive -&gt; y2\ntrue_positive -&gt; y3\ntrue_positive -&gt; y4\ntrue_positive -&gt; y5\ntrue_positive -&gt; y6\ntrue_positive -&gt; y7\ntrue_positive -&gt; y8\ntrue_positive -&gt; y9\n}\n')\n\nggdag(bayes_net) + theme_dag()\n\n\n\n\n\n\n\n\n\nLastly, the bayes net model is similar to the first model; however, now there are edges between the 3 skills. Other than that, nothing else has changed. In the next post I will be estimating the first bayes net model and doing some posterior checks to see how the model works."
  },
  {
    "objectID": "posts/2024-07-10-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-07-10-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table &lt;- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n\n\nAs mentioned in the previous post, the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the cmdstanr package to call on Stan for the Bayesian analyses, I am using the posterior package to manipulate the chains, iterations, and draws from the analyses and the bayesplot package to visualize the convergence of each parameter included in the bayes net model. I’m also using the reactable package to showcase the parameters for the model.\n\nData Creation\n\n\nCode\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\n\nCode\nstan_file &lt;- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The J, I, K, and C list values are all important for looping through:\n\nJ = The number of rows of data; in this case there are 30 “students”\nI = The number of columns in the dataset; which is 15 excluding the first column\nK = The number of latent attributes/skills\nC = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from y for the actual data and then X in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\n\nCode\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n\nfit &lt;- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n\n# fit$save_object(\"simple_bayes_net.RDS\")\n\n\nThis next part will be different depending on whether or not you are using RStan or like in this case cmdstanR. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For cmdstanR, you call on your Stan file. Below is the Stan code or if you’d like to see it side-by-side, the Stan file can be found here. I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations.\n\n\nCode\n\"\ndata {\n  int&lt;lower=1&gt; J; // number of examinees\n  int&lt;lower=1&gt; I; // number of items\n  int&lt;lower=1&gt; K; // number of latent variables\n  int&lt;lower=1&gt; C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\nparameters {\n  simplex[C] nu; // class probabilities\n  vector&lt;lower=0, upper=1&gt;[I] false_pos;\n  vector&lt;lower=0, upper=1&gt;[I] true_pos;\n  real&lt;lower=0, upper=1&gt; lambda1;\n  real&lt;lower=0, upper=1&gt; lambda20;\n  real&lt;lower=0, upper=1&gt; lambda21;\n  real&lt;lower=0, upper=1&gt; lambda30;\n  real&lt;lower=0, upper=1&gt; lambda31;\n}\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] &gt; 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] &gt; 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] &gt; 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n\n\nLooking over the Stan code, there is a lot here. I’ll break down each section, but will not be spending an extensive amount of time for each.\n\n\nCode\n\"\ndata {\n  int&lt;lower=1&gt; J; // number of examinees\n  int&lt;lower=1&gt; I; // number of items\n  int&lt;lower=1&gt; K; // number of latent variables\n  int&lt;lower=1&gt; C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\n\"\n\n\nThe data section of stan code is including what you called the components of the stan_filelist object. If you deviate from what you named the components in your list, then your model will show an error. While not entirely necessary, you may want to put constraints on these values. For instance, I know that I have more than 1 student, item, latent variable, and class, so I will put a constraint that the lowest possible value is 1.\n\n\nCode\n\"\nparameters {\n  simplex[C] nu; // class probabilities\n  vector&lt;lower=0, upper=1&gt;[I] false_pos;\n  vector&lt;lower=0, upper=1&gt;[I] true_pos;\n  real&lt;lower=0, upper=1&gt; lambda1;\n  real&lt;lower=0, upper=1&gt; lambda20;\n  real&lt;lower=0, upper=1&gt; lambda21;\n  real&lt;lower=0, upper=1&gt; lambda30;\n  real&lt;lower=0, upper=1&gt; lambda31;\n}\n\"\n\n\nThe parameters section includes any parameters that are being included in your model. For instance, if creating a Bayesian linear regression, you would include the alpha and beta parameters in this section. For these models, I have the class probabilities for each latent class (to read more about the simplex function see here). Then I will have the probabilities of a student being either a true or false positive mastery case for the latent classes. These are vectors due to there being a true and false positive parameter for each item. The last parameters are the lambda parameters, which are the probabilities for mastery of the three latent attributes. These often require expert domain knowledge to specify informative priors.\n\n\nCode\n\"\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] &gt; 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] &gt; 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] &gt; 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\n\"\n\n\nWhile this section is optional, I like to include it because I use this section to do many of my calculations. For instance, in this section I like to use the prior lambda values to get the log probabilities of theta_log values, which are the log probabilities based on the level of mastery from the lambda values. I looped through the latent classes so when a latent class’ value is 1, then it takes the greater log probability, and when the value is 0, then it takes the lower log probability. I also did my delta calculations in this section. The delta calculation takes theta values based on the latent classes values and it uses the Q-matrix for each item. Then by multiplying the theta values raised to the power of the Q-matrix gets the probability of mastery for each item within each latent class. This value indicates whether a given student will have mastery over all of the latent attributes.\n\n\nCode\n\"\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\n\"\n\n\nFor the model section, which is necessary, I always start with declaring any new variables, followed by priors for my lambda values and the true and false positive probabilities for each item. Lastly, this section is always where you will do your calculations for each item and for each latent class. Finally, the target calculation at the end is for the target log density.\n\n\nCode\n\"\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); \n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n\n\nThe last section, the generated quantities, is “generate additional quantities of interest from a fitted model without re-running the sampler” (Stan). For this series, I am using this section to calculate posterior probabilities, such as the probability of a student being in a specific latent class and the probability that students have mastered the attributes.\n\n\nCode\nfit &lt;- read_rds(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9376389 0.9111992 0.8754096 0.9844359\n\n\nCode\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |&gt; arrange(desc(rhat)) |&gt; head()\n\n\n# A tibble: 6 × 4\n  variable               rhat ess_bulk ess_tail\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 prob_resp_class[12,8]  1.00    3869.    4877.\n2 nu[5]                  1.00    7371.    5381.\n3 log_nu[5]              1.00    7371.    5381.\n4 prob_resp_class[5,8]   1.00    5686.    5902.\n5 prob_resp_class[18,8]  1.00    3686.    4569.\n6 prob_resp_class[27,5]  1.00    5974.    5744.\n\n\nCode\nbn_measure |&gt; mutate(across(-variable, ~round(.x, 3))) |&gt; react_table()\n\n\n\n\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\n\nCode\nbn_measure |&gt; \n  mutate(across(-variable, ~round(.x, 3))) |&gt; \n  filter(str_detect(variable, \"prob_resp_attr\")) |&gt;\n  react_table()\n\n\n\n\n\n\nI decided to filter in on the probabilities for students to have mastery over the attributes. The first index in the square brackets indicates the student and then the second index value indicates the three attributes. Obviously for something more thought out this would line up for meaningful attributes, but for this example, the values align with arbitrary values.\n\n\nCode\ny_rep &lt;- fit$draws(\"x_rep\") |&gt; as_draws_matrix()\nstu_resp_attr &lt;- fit$draws(\"prob_resp_attr\") |&gt; as_draws_matrix()\n\n\nI decided to extract the replicated values for the items and the probabilities of each student’s mastery of each of the three latent attributes.\n\n\nCode\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n\n\n\n\n\n\n\n\n\nCode\ny |&gt; react_table()\n\n\n\n\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the original data, the original responses and the probabilities with a probability threshold of 0.5 match one another.\n\n\nCode\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\n\n\nCode\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\n\n\nCode\nppc_intervals(\n  y = y |&gt; pull(y1) |&gt; as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n\n\n\n\n\n\n\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\n\nCode\nactual_stu_resp_attr &lt;- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |&gt;\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.\n\n\nCode\nstu_resp_attr_mean &lt;- stu_resp_attr |&gt;\n  as_tibble() |&gt;\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_mean |&gt;\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(\n    everything()\n  ) |&gt;\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |&gt;\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |&gt;\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute.\n\n\nCode\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n\n\n$att1\n   .y\n.x   0  1\n  0  6  5\n  1  7 12\n\n$att2\n   .y\n.x   0  1\n  0  6  8\n  1  5 11\n\n$att3\n   .y\n.x   0  1\n  0  4  2\n  1 11 13\n\n\nCode\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n\n\n$att1\n   .y\n.x          0         1\n  0 0.2000000 0.1666667\n  1 0.2333333 0.4000000\n\n$att2\n   .y\n.x          0         1\n  0 0.2000000 0.2666667\n  1 0.1666667 0.3666667\n\n$att3\n   .y\n.x           0          1\n  0 0.13333333 0.06666667\n  1 0.36666667 0.43333333\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model.\n\n\nCode\nstu_resp_attr_long &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long &lt;- actual_stu_resp_attr |&gt;\n  pivot_longer(-studentid)\n\naccuracy_att &lt;- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n\n\n[1] 0.5777778\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 58%. This is a good starting point, but this may indicate that the model needs better defined priors and may require the edges between the attributes to show latent relationships. The low accuracy value may also be indicative of the importance of domain knowledge in building a latent bayes net."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html",
    "title": "Using Typst To Create Documents",
    "section": "",
    "text": "Under Development - Not Complete\nI have been using Typst, an awesome app for working on pdf files at the same time as colleagues. You could think of this as something similar to Google Docs or GitHub for code. This also got me thinking about creating a small series of blog posts about using Typst and then creating Typst templates for documents using Quarto. The latter topics would be using Quarto extensions and if following along, you would need Quarto version 1.4 at least to be able to use Typst code chunks on a Quarto document. So first, I will show the Typst file I will be using because let’s face it I’m on the job market and free publicity is always good.\n.\nHere is the link for the Typst resume to view. If you want, you can just copy and paste that into Typst and change the information. I will walk through each section of the document with Typst code in the post, as well as a cover letter post, and then end the series with a Quarto extension to create a Typst template so you can just write your resume and/or cover letter in Quarto."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#typst-documentation",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#typst-documentation",
    "title": "Using Typst To Create Documents",
    "section": "Typst Documentation",
    "text": "Typst Documentation\nI will be the first to state that Typst documentation is a little difficult to follow at first. Hopefully with this tutorial you will get a better understanding of the basics of Typst code. As someone who tried to learn LaTeX to edit the previous resume I had found a template for, I wish Typst existed earlier."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "title": "Using Typst To Create Documents",
    "section": "Creating a Typst Document",
    "text": "Creating a Typst Document\nWorking in the Typst app is pretty straightforward with you creating a username, followed by your dashboard with nothing there. This will be the location of all of your documents as you get started with Typst. While there are Typst templates already for resumes I really wanted to create something similar to the resume I had in LaTeX. Working in Quarto, you will have to learn how to create Typst code chunks. They are slightly different from other languages’ code chunks but you can still use all of the Quarto code chunk arguments.\nFrom what I have seen online, there does not seem to be much difference in the ordering of some of the beginning Typst documentation. I have decided to start my Typst document with any variables I will be including, followed by any Typst packages I will need, and then setting up the general parameters for the document. These general parameters are for the document overall. Things like setting the font to a specific font, size and maybe weight would be a good parameter to set at the top of your document."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "title": "Using Typst To Create Documents",
    "section": "Breakdown of Variables, Packages, and # Set function for parameters",
    "text": "Breakdown of Variables, Packages, and # Set function for parameters\n\n//Variables\n#let name = [Jonathan A. Pedroza Ph.D]\n\n//Packages\n#import \"@preview/tablex:0.0.8\": tablex\n#import \"lib-gen.typ\": *\n#import \"lib-impl.typ\": *\n#import \"lib.typ\": *\n\n#set page(\n  margin: (\n    top: 0cm,\n    bottom: 0cm,\n    left: .5cm,\n    right: 0cm\n  )\n)\n\n#set block(spacing: 0.5em)\n\n#set rect(\n  width: 37%,\n  height: 100%\n)\n\nAbove is the beginning Typst code for the resume I created. I’m going to walk through the code a little, but for more detailed information, check out the help documentation. The // syntax can be included wherever to include comments. Since I have been showing others how to use Typst to create quick pretty PDFs, I have been including a lot of comments for things like variables. To create variables in Typst, you will need to use the #let function followed by your variable name, an equal sign, and the information you want to include. So I created the variable name, which would be used as #name in Typst and the document will spell out my full name. After that, just as the comment states, I included the tablex package, which I have found to be useful for creating tables and grids. If you’d like you can use the #table or #grid functions from Typst. Additionally, I also included the files for using the FontAwesome Typst package. You can find all the icons and other information about FontAwesome at the FontAwesome website. I was just being lazy as I wanted to create my resume quick so I could get it out into the world ASAP. Next I set the margins to maximize the amount of space I would have for my resume and I created a block after the titles Education and Professional Experience since I did not want the default amount of space before my education and experience entries. The #set function creates rules for the document as a whole. So For the whole document I have the same margins throughout, a block to create more space between headers and text underneath and the rectangle to separate the sections of the document. Lastly, I set a rectangle for 37% of the document’s width and 100% of the height. There are other metrics that can be used to create the rectangle but I personally was enjoying using percentages for this document. This rectangle is for the right side of the document that includes the contact information."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#grid-of-entries",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#grid-of-entries",
    "title": "Using Typst To Create Documents",
    "section": "Grid of Entries",
    "text": "Grid of Entries\n\n#grid(\n  columns: (70%, 82%),\n  [\n    #linebreak()\n\n    #set text(\n      font: \"Source Sans Pro\",\n      size: 10pt\n    )\n    #set align(center)\n    \n    = Education #fa-graduation-cap()\n    #line(\n      length: 94%,\n      stroke: black\n    )\n\nI decided to separate some of these functions because it might be easier to talk about. So the grid here is actually separating the main text and the contact information text. As I am reading this, I can see that I probably should have set the font to be 10pt throughout the document rather than within the grid."
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html",
    "href": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(inspectdf)\nlibrary(bnlearn)\nlibrary(Rgraphviz)\n\n\nLoading required package: graph\nLoading required package: BiocGenerics\n\nAttaching package: 'BiocGenerics'\n\nThe following object is masked from 'package:bnlearn':\n\n    score\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\nAttaching package: 'graph'\n\nThe following objects are masked from 'package:bnlearn':\n\n    degree, nodes, nodes&lt;-\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\nLoading required package: grid\n\n\nCode\nlibrary(reactable)\nlibrary(ggdag)\n\n\n\nAttaching package: 'ggdag'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nCode\nlibrary(dagitty)\n\n\n\nAttaching package: 'dagitty'\n\nThe following object is masked from 'package:Rgraphviz':\n\n    graphLayout\n\nThe following object is masked from 'package:graph':\n\n    edges\n\nThe following objects are masked from 'package:bnlearn':\n\n    ancestors, children, descendants, parents, spouses\n\n\nCode\nbn_score &lt;- bnlearn::score\n\ntheme_set(theme_light())\n\ncoffee &lt;- read_csv(here::here(\"posts/2024-11-03-bayes-net-us-coffee-tasting\", \"hoffmann_america_taste_data.csv\")) |&gt;\n  janitor::clean_names()\n\n\nRows: 4042 Columns: 113\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): Submission ID, What is your age?, How many cups of coffee do you t...\ndbl (13): Lastly, how would you rate your own coffee expertise?, Coffee A - ...\nlgl (56): Where do you typically drink coffee? (At home), Where do you typic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nCode\ncoffee |&gt;\n  inspect_na() |&gt;\n  show_plot()\nCode\ncoffee_drop &lt;- coffee[, which(colMeans(!is.na(coffee)) &gt; 0.5)]\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()\nCode\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -c(\n      where_do_you_typically_drink_coffee,\n      # how_do_you_brew_coffee_at_home,\n      do_you_usually_add_anything_to_your_coffee,\n      why_do_you_drink_coffee\n    )\n  ) |&gt;\n  rename(\n    age = what_is_your_age,\n    cup_per_day = how_many_cups_of_coffee_do_you_typically_drink_per_day,\n    drink_at_home = where_do_you_typically_drink_coffee_at_home,\n    drink_at_office = where_do_you_typically_drink_coffee_at_the_office,\n    drink_on_go = where_do_you_typically_drink_coffee_on_the_go,\n    drink_at_cafe = where_do_you_typically_drink_coffee_at_a_cafe,\n    drink_none_of_these = where_do_you_typically_drink_coffee_none_of_these,\n    home_brew_pour_over = how_do_you_brew_coffee_at_home_pour_over,\n    home_brew_french_press = how_do_you_brew_coffee_at_home_french_press,\n    home_brew_espresso = how_do_you_brew_coffee_at_home_espresso,\n    home_brew_mr_coffee = how_do_you_brew_coffee_at_home_coffee_brewing_machine_e_g_mr_coffee,\n    home_brew_pods = how_do_you_brew_coffee_at_home_pod_capsule_machine_e_g_keurig_nespresso,\n    home_brew_instant = how_do_you_brew_coffee_at_home_instant_coffee,\n    home_brew_bean2cup = how_do_you_brew_coffee_at_home_bean_to_cup_machine,\n    home_brew_cold_brew = how_do_you_brew_coffee_at_home_cold_brew,\n    home_brew_cometeer = how_do_you_brew_coffee_at_home_coffee_extract_e_g_cometeer,\n    home_brew_other = how_do_you_brew_coffee_at_home_other,\n    favorite_coffee_drink = what_is_your_favorite_coffee_drink,\n    coffee_black = do_you_usually_add_anything_to_your_coffee_no_just_black,\n    coffee_milk_alt_creamer = do_you_usually_add_anything_to_your_coffee_milk_dairy_alternative_or_coffee_creamer,\n    coffee_sugar = do_you_usually_add_anything_to_your_coffee_sugar_or_sweetener,\n    coffee_syrup = do_you_usually_add_anything_to_your_coffee_flavor_syrup,\n    coffee_other = do_you_usually_add_anything_to_your_coffee_other,\n    coffee_characteristic_preference = before_todays_tasting_which_of_the_following_best_described_what_kind_of_coffee_you_like,\n    coffee_strength = how_strong_do_you_like_your_coffee,\n    roast_preference = what_roast_level_of_coffee_do_you_prefer,\n    caffeine_preference = how_much_caffeine_do_you_like_in_your_coffee,\n    expertise = lastly_how_would_you_rate_your_own_coffee_expertise,\n    preference_a_to_b = between_coffee_a_coffee_b_and_coffee_c_which_did_you_prefer,\n    preference_a_to_d = between_coffee_a_and_coffee_d_which_did_you_prefer,\n    favorite_abcd = lastly_what_was_your_favorite_overall_coffee,\n    remote_work = do_you_work_from_home_or_in_person,\n    money_spend_a_month = in_total_much_money_do_you_typically_spend_on_coffee_in_a_month,\n    why_drink_taste_good = why_do_you_drink_coffee_it_tastes_good,\n    why_drink_caffeine = why_do_you_drink_coffee_i_need_the_caffeine,\n    why_drink_ritual = why_do_you_drink_coffee_i_need_the_ritual,\n    why_drink_makes_bathroom = why_do_you_drink_coffee_it_makes_me_go_to_the_bathroom,\n    why_drink_other = why_do_you_drink_coffee_other,\n    like_taste = do_you_like_the_taste_of_coffee,\n    know_where_coffee_comes_from = do_you_know_where_your_coffee_comes_from,\n    most_spent_on_cup_coffee = what_is_the_most_youve_ever_paid_for_a_cup_of_coffee,\n    willing_to_spend_cup_coffee = what_is_the_most_youd_ever_be_willing_to_pay_for_a_cup_of_coffee,\n    good_value_cafe = do_you_feel_like_you_re_getting_good_value_for_your_money_when_you_buy_coffee_at_a_cafe,\n    equipment_spent_5years = approximately_how_much_have_you_spent_on_coffee_equipment_in_the_past_5_years,\n    good_value_equipment = do_you_feel_like_you_re_getting_good_value_for_your_money_with_regards_to_your_coffee_equipment\n  )\nCode\ncoffee_logical &lt;- coffee_drop |&gt;\n  select_if(is.logical)\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  drop_na(\n    colnames(coffee_logical)\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  mutate(\n    across(\n      where(\n        is.logical\n      ),\n      ~case_when(\n        .x == TRUE ~ 1,\n        .x == FALSE ~ 0\n      )\n    ),\n    across(\n      where(\n        is.character\n      ),\n      ~as.factor(.x)\n    )\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -matches(\n      \"_notes\"\n    )\n  )\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()\nCode\ncoffee_drop |&gt;\n  drop_na(favorite_abcd) |&gt;\n  ggplot(\n    aes(\n      favorite_abcd\n    )\n  ) +\n  geom_bar(\n    aes(\n      fill = favorite_abcd\n    )\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  labs(\n    x = \"Coffee Choices\",\n    y = \"Counts\",\n    title = \"Counts of Each Coffee\"\n  ) +\n  theme(\n    legend.position = \"none\"\n  )\nCode\ncoffee_drop |&gt;\n  select(\n    age,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -age\n  ) |&gt;\n  group_by(\n    age,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    age = as.factor(age),\n    age = fct_relevel(\n      age,\n      \"&lt;18 years old\",\n      \"18-24 years old\",\n      \"25-34 years old\",\n      \"35-44 years old\",\n      \"45-54 years old\",\n      \"55-64 years old\",\n      \"&gt;65 years old\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  ggplot(\n    aes(\n      age,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    ),\n    axis.text.x = element_text(\n      angle = 45,\n      vjust = 0.5\n      )\n    ) +\n  NULL\nCode\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    gender = as.factor(gender),\n    name = as.factor(name),\n    gender = fct_relevel(\n      gender,\n      \"Male\",\n      \"Female\",\n      \"Non-binary\",\n      \"Other (please specify)\",\n      \"Prefer not to say\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  ggplot(\n    aes(\n      gender,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    )\n    ) +\n  NULL\nCode\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    gender = as.factor(gender),\n    name = as.factor(name),\n    gender = fct_relevel(\n      gender,\n      \"Male\",\n      \"Female\",\n      \"Non-binary\",\n      \"Other (please specify)\",\n      \"Prefer not to say\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  filter(\n    !gender %in% c(\"Male\", \"Female\")\n  ) |&gt;\n  ggplot(\n    aes(\n      gender,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    )\n    ) +\n  NULL\nCode\ncoffee_drop |&gt;\n  group_by(\n    favorite_abcd,\n    expertise\n  ) |&gt;\n  count() |&gt; \n  ungroup(\n    favorite_abcd\n    ) |&gt;\n  mutate(\n    percent = n/sum(n),\n    percent = percent*100\n  ) |&gt;\n  drop_na() |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise),\n      percent\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = as.factor(favorite_abcd)\n    )\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  labs(\n    title = \"Favorite Coffees By Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Percentage\",\n    fill = \"\"\n  ) +\n  NULL"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#using-bnlearn",
    "href": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#using-bnlearn",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Using Bnlearn",
    "text": "Using Bnlearn\n\n\nCode\n# Building Bayesian Network\ndag &lt;- empty.graph(nodes = colnames(no_fact))\n\narcs &lt;- matrix(\n  c(\"gender\", \"cup_per_day\",\n    \"gender\", \"favorite_coffee_drink\",\n    \"gender\", \"home_brew_pour_over\",\n    \"gender\", \"home_brew_french_press\",\n    \"gender\", \"home_brew_espresso\",\n    \"gender\", \"home_brew_mr_coffee\",\n    \"gender\", \"home_brew_pods\",\n    \"gender\", \"home_brew_instant\",\n    \"gender\", \"home_brew_bean2cup\",\n    \"gender\", \"home_brew_cold_brew\",\n    \"gender\", \"home_brew_cometeer\",\n    \"gender\", \"home_brew_other\",\n    \"age\", \"cup_per_day\",\n    \"age\", \"favorite_coffee_drink\",\n    \"age\", \"home_brew_pour_over\",\n    \"age\", \"home_brew_french_press\",\n    \"age\", \"home_brew_espresso\",\n    \"age\", \"home_brew_mr_coffee\",\n    \"age\", \"home_brew_pods\",\n    \"age\", \"home_brew_instant\",\n    \"age\", \"home_brew_bean2cup\",\n    \"age\", \"home_brew_cold_brew\",\n    \"age\", \"home_brew_cometeer\",\n    \"age\", \"home_brew_other\",\n\n    \"cup_per_day\", \"roast_preference\",\n    \"favorite_coffee_drink\", \"roast_preference\",\n    \"home_brew_pour_over\", \"roast_preference\",\n    \"home_brew_french_press\", \"roast_preference\",\n    \"home_brew_espresso\", \"roast_preference\",\n    \"home_brew_mr_coffee\", \"roast_preference\",\n    \"home_brew_pods\", \"roast_preference\",\n    \"home_brew_instant\", \"roast_preference\",\n    \"home_brew_bean2cup\", \"roast_preference\",\n    \"home_brew_cold_brew\", \"roast_preference\",\n    \"home_brew_cometeer\", \"roast_preference\",\n    \"home_brew_other\", \"roast_preference\",\n\n    \"roast_preference\", \"expertise\",\n    \"roast_preference\", \"favorite_abcd\",\n    \"expertise\", \"favorite_abcd\"),\n  byrow = TRUE,\n  ncol = 2,\n  dimnames = list(NULL, c(\"from\", \"to\"))\n)\n\narcs(dag) &lt;- arcs\n\ngraphviz.plot(dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(12345)\ndag_fit &lt;- bn.fit(dag, data = no_fact, method = \"bayes\", iss = 5000)\n\n\n\nGender - Conditional Probability Table (CPT)\n\n\nCode\ntibble(\n  Gender = attributes(dag_fit$gender$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$gender$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\n\nAge - CPT\n\n\nCode\ntibble(\n  Age = attributes(dag_fit$age$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$age$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\n\nCups of Coffee Per Day - CPT\n\n\nCode\ndag_fit$cup_per_day$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = cup_per_day, y = n, fill = gender, facet = age) +\n  labs(\n    title = \"Probability of Cups of Coffee Per Day From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Pourover\n\n\nCode\ndag_fit$home_brew_pour_over$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pour_over, facet = age) +\n  labs(\n    title = \"Probability of Making Pourover at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - French Press\n\n\nCode\ndag_fit$home_brew_french_press$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_french_press, facet = age) +\n  labs(\n    title = \"Probability of Making French Press at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Espresso\n\n\nCode\ndag_fit$home_brew_espresso$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_espresso, facet = age) +\n  labs(\n    title = \"Probability of Making Espresso at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - coffee Brewing Machine\n\n\nCode\ndag_fit$home_brew_mr_coffee$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_mr_coffee, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Coffee Brewing Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Pods\n\n\nCode\ndag_fit$home_brew_pods$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pods, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using Pods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Instant Coffee\n\n\nCode\ndag_fit$home_brew_instant$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_instant, facet = age) +\n  labs(\n    title = \"Probability of Making Instant Coffee at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Bean 2 Cup\n\n\nCode\ndag_fit$home_brew_bean2cup$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_bean2cup, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Bean 2 Cup Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Cold Brew\n\n\nCode\ndag_fit$home_brew_cold_brew$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cold_brew, facet = age) +\n  labs(\n    title = \"Probability of Making Cold Brew at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Cometeer\n\n\nCode\ndag_fit$home_brew_cometeer$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cometeer, facet = age) +\n  labs(\n    title = \"Probability of Making Cometeer Coffees at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Other\n\n\nCode\ndag_fit$home_brew_other$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_other, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee From Other Methods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nFavorite Coffee Drink - CPT\n\n\nCode\ndag_fit$favorite_coffee_drink$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(\n    x = favorite_coffee_drink,\n    y = n,\n    fill = gender,\n    facet = age\n  ) +\n  labs(\n    title = \"Probability of Favorite Coffee Drinks From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nRoast Preference - CPT\n\n\nCode\nroast_pref_func &lt;- function(\n  dag_table,\n  x,\n  y = n,\n  fill,\n  facet_x,\n  facet_y\n){\n  {{dag_table}} |&gt;\n  as_tibble() |&gt;\n  mutate(\n    fill = str_to_title({{fill}}),\n    n = round(n, 2),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    x = fct_relevel(\n      {{x}},\n      \"three_or_more\",\n      \"2\",\n      \"one_or_less\"\n    ),\n    fill = fct_relevel(\n      fill,\n      \"Dark\",\n      \"Medium\",\n      \"Light\"\n    )\n  ) |&gt;\n  ggplot(\n    aes(\n      x,\n      {{y}}\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = fill\n      )\n  ) +\n  coord_flip() +\n  facet_grid(\n    vars({{facet_x}}),\n    vars({{facet_y}})\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  guides(\n    fill = guide_legend(\n      reverse = TRUE\n      )\n    )\n}\n\n\nThe CPT here only include the probabilities for whether participants used one home brewer or not and considered all of the other home brewers as not using those at home.\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pour_over\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Pourover\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_french_press\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a French Press\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_espresso\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Espresso at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_mr_coffee\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Coffee Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pods\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Pods\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_instant\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Instant Coffee\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_bean2cup\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Bean 2 Cup Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cold_brew\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Cold Brew at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cometeer\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Cometeer\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_other\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using an Other Method\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nExpertise Level - CPT\n\n\nCode\nexpertise_tbl &lt;- dag_fit$expertise$prob |&gt; as_tibble() |&gt;\n  mutate(\n    roast_preference = str_to_title(roast_preference),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"Light\",\n      \"Medium\",\n      \"Dark\"\n    )\n  ) \n\nexpertise_tbl |&gt;\n  ggplot(\n    aes(\n      roast_preference,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = expertise\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = expertise_tbl |&gt; filter(roast_preference == \"Light\"),\n    aes(\n      label = expertise,\n      group = expertise,\n      color = expertise\n    ),\n    position = position_dodge(width = .9),\n    vjust = -.5\n  ) +\n   labs(\n    title = \"Probability of One's Roast Preference From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level\",\n    x = \"\",\n    y = \"Probability\",\n    caption = \"Note: Probabilities range from 0 to 1. The scale is reduced to visually compare groups.\"\n  ) +\n  viridis::scale_color_viridis(\n    discrete = TRUE\n    ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  scale_x_discrete(\n    expand = c(0, .5)\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text = element_text(\n      color = \"black\"\n    ),\n    axis.title = element_text(\n      color = \"black\"\n    ),\n    plot.title = element_text(\n      color = \"black\"\n    ),\n    plot.subtitle = element_text(\n      color = \"black\"\n    ),\n    plot.caption = element_text(\n      color = \"black\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nFavorite Coffee (A, B, C, D) - CPT\n\n\nCode\nfavorite_abcd_prob &lt;- dag_fit$favorite_abcd$prob |&gt; as_tibble() |&gt;\n  mutate(\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"light\",\n      \"medium\",\n      \"dark\"\n    ),\n    favorite_abcd = fct_relevel(\n      favorite_abcd,\n      \"Coffee A\",\n      \"Coffee B\",\n      \"Coffee C\",\n      \"Coffee D\"\n    )\n  )\n\n# scales::show_col(viridis::viridis_pal(option = \"E\")(3))\n\nfavorite_abcd_prob |&gt;\n  ggplot(\n    aes(\n      favorite_abcd,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = roast_preference\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee D\" &\n      roast_preference == \"light\"\n    ),\n    label = \"Light\\nRoast\",\n    nudge_y = .03,\n    color = \"#00204DFF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee C\" &\n      roast_preference == \"medium\"\n    ),\n    label = \"Medium\\nRoast\",\n    nudge_y = .03,\n    color = \"#7C7B78FF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee B\" &\n      roast_preference == \"dark\"\n    ),\n    label = \"Dark\\nRoast\",\n    nudge_y = .03,\n    color = \"#FFEA46FF\"\n  ) +\n  facet_wrap(\n    ~expertise,\n    ncol = 5\n  ) +\n  scale_y_continuous(\n    breaks = seq(.1, .6, .1)\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE,\n    option = \"cividis\"\n  ) +\n  labs(\n    title = \"Probability of One's Favorite Coffees From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level & Roast Level Preference\",\n    x = \"\",\n    y = \"Probability\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    strip.background = element_rect(\n      fill = \"#7C7B78FF\"\n    ),\n    axis.text = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    axis.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.subtitle = element_text(\n      color = \"#7C7B78FF\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nscore(\n  dag,\n  data = no_fact, \n  type = \"bde\",\n  iss = 5000\n)\n\n\n[1] -49810.62\n\n\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\"),\n  evidence = (gender == \"Male\")\n)\n\n\n[1] 0.2386035\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\"),\n  evidence = (gender == \"Female\")\n)\n\n\n[1] 0.228005\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Male\")\n)\n\n\n[1] 0.302377\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Female\")\n)\n\n\n[1] 0.2890071\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Male\")\n)\n\n\n[1] 0.5395322\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Female\")\n)\n\n\n[1] 0.5265312"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "href": "posts/2024-11-03-bayes-net-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Probabilities of Coffee A or Coffee D Based on Expertise Level",
    "text": "Probabilities of Coffee A or Coffee D Based on Expertise Level\n\n\nCode\nexpert1 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"1\")\n)\nexpert2 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"2\")\n)\nexpert3 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"3\")\n)\nexpert4 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"4\")\n)\nexpert5 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"5\")\n)\nexpert6 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"6\")\n)\nexpert7 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"7\")\n)\nexpert8 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"8\")\n)\nexpert9 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"9\")\n)\nexpert10 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"10\")\n)\n\ntibble(\n  expertise_level = seq(1, 10, 1),\n  probability_of_a_or_d = c(expert1, expert2, expert3, expert4, expert5, expert6, expert7, expert8, expert9, expert10)\n) |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise_level),\n      probability_of_a_or_d\n    )\n  ) +\n  geom_col(\n    aes(fill = as.factor(expertise_level)),\n    position = position_dodge()\n  ) +\n  geom_text(\n    aes(\n      label = round(probability_of_a_or_d, 2)\n    ),\n    color = \"black\",\n    vjust = -.3\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, .1)\n  ) +\n  labs(\n    title = \"Probability of Choosing Coffee A or D as Their Favorite Coffee\",\n    subtitle = \"By Level of Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Probability\"\n  ) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n  theme(\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html",
    "href": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html",
    "title": "Bayesian Network for US Coffee Tasting Data Using R & Stan",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(inspectdf)\nlibrary(bnlearn)\nlibrary(Rgraphviz)\n\n\nLoading required package: graph\nLoading required package: BiocGenerics\n\nAttaching package: 'BiocGenerics'\n\nThe following object is masked from 'package:bnlearn':\n\n    score\n\nThe following objects are masked from 'package:lubridate':\n\n    intersect, setdiff, union\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\nAttaching package: 'graph'\n\nThe following objects are masked from 'package:bnlearn':\n\n    degree, nodes, nodes&lt;-\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\nLoading required package: grid\n\n\nCode\nlibrary(reactable)\nlibrary(ggdag)\n\n\n\nAttaching package: 'ggdag'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nCode\nlibrary(dagitty)\n\n\n\nAttaching package: 'dagitty'\n\nThe following object is masked from 'package:Rgraphviz':\n\n    graphLayout\n\nThe following object is masked from 'package:graph':\n\n    edges\n\nThe following objects are masked from 'package:bnlearn':\n\n    ancestors, children, descendants, parents, spouses\n\n\nCode\nbn_score &lt;- bnlearn::score\n\ntheme_set(theme_light())\n\ncoffee &lt;- read_csv(here::here(\"posts/2024-11-03-bayes-net-us-coffee-tasting\", \"hoffmann_america_taste_data.csv\")) |&gt;\n  janitor::clean_names()\n\n\nRows: 4042 Columns: 113\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): Submission ID, What is your age?, How many cups of coffee do you t...\ndbl (13): Lastly, how would you rate your own coffee expertise?, Coffee A - ...\nlgl (56): Where do you typically drink coffee? (At home), Where do you typic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nCode\ncoffee_drop &lt;- coffee[, which(colMeans(!is.na(coffee)) &gt; 0.5)]\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()\nCode\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -c(\n      where_do_you_typically_drink_coffee,\n      # how_do_you_brew_coffee_at_home,\n      do_you_usually_add_anything_to_your_coffee,\n      why_do_you_drink_coffee\n    )\n  ) |&gt;\n  rename(\n    age = what_is_your_age,\n    cup_per_day = how_many_cups_of_coffee_do_you_typically_drink_per_day,\n    drink_at_home = where_do_you_typically_drink_coffee_at_home,\n    drink_at_office = where_do_you_typically_drink_coffee_at_the_office,\n    drink_on_go = where_do_you_typically_drink_coffee_on_the_go,\n    drink_at_cafe = where_do_you_typically_drink_coffee_at_a_cafe,\n    drink_none_of_these = where_do_you_typically_drink_coffee_none_of_these,\n    home_brew_pour_over = how_do_you_brew_coffee_at_home_pour_over,\n    home_brew_french_press = how_do_you_brew_coffee_at_home_french_press,\n    home_brew_espresso = how_do_you_brew_coffee_at_home_espresso,\n    home_brew_mr_coffee = how_do_you_brew_coffee_at_home_coffee_brewing_machine_e_g_mr_coffee,\n    home_brew_pods = how_do_you_brew_coffee_at_home_pod_capsule_machine_e_g_keurig_nespresso,\n    home_brew_instant = how_do_you_brew_coffee_at_home_instant_coffee,\n    home_brew_bean2cup = how_do_you_brew_coffee_at_home_bean_to_cup_machine,\n    home_brew_cold_brew = how_do_you_brew_coffee_at_home_cold_brew,\n    home_brew_cometeer = how_do_you_brew_coffee_at_home_coffee_extract_e_g_cometeer,\n    home_brew_other = how_do_you_brew_coffee_at_home_other,\n    favorite_coffee_drink = what_is_your_favorite_coffee_drink,\n    coffee_black = do_you_usually_add_anything_to_your_coffee_no_just_black,\n    coffee_milk_alt_creamer = do_you_usually_add_anything_to_your_coffee_milk_dairy_alternative_or_coffee_creamer,\n    coffee_sugar = do_you_usually_add_anything_to_your_coffee_sugar_or_sweetener,\n    coffee_syrup = do_you_usually_add_anything_to_your_coffee_flavor_syrup,\n    coffee_other = do_you_usually_add_anything_to_your_coffee_other,\n    coffee_characteristic_preference = before_todays_tasting_which_of_the_following_best_described_what_kind_of_coffee_you_like,\n    coffee_strength = how_strong_do_you_like_your_coffee,\n    roast_preference = what_roast_level_of_coffee_do_you_prefer,\n    caffeine_preference = how_much_caffeine_do_you_like_in_your_coffee,\n    expertise = lastly_how_would_you_rate_your_own_coffee_expertise,\n    preference_a_to_b = between_coffee_a_coffee_b_and_coffee_c_which_did_you_prefer,\n    preference_a_to_d = between_coffee_a_and_coffee_d_which_did_you_prefer,\n    favorite_abcd = lastly_what_was_your_favorite_overall_coffee,\n    remote_work = do_you_work_from_home_or_in_person,\n    money_spend_a_month = in_total_much_money_do_you_typically_spend_on_coffee_in_a_month,\n    why_drink_taste_good = why_do_you_drink_coffee_it_tastes_good,\n    why_drink_caffeine = why_do_you_drink_coffee_i_need_the_caffeine,\n    why_drink_ritual = why_do_you_drink_coffee_i_need_the_ritual,\n    why_drink_makes_bathroom = why_do_you_drink_coffee_it_makes_me_go_to_the_bathroom,\n    why_drink_other = why_do_you_drink_coffee_other,\n    like_taste = do_you_like_the_taste_of_coffee,\n    know_where_coffee_comes_from = do_you_know_where_your_coffee_comes_from,\n    most_spent_on_cup_coffee = what_is_the_most_youve_ever_paid_for_a_cup_of_coffee,\n    willing_to_spend_cup_coffee = what_is_the_most_youd_ever_be_willing_to_pay_for_a_cup_of_coffee,\n    good_value_cafe = do_you_feel_like_you_re_getting_good_value_for_your_money_when_you_buy_coffee_at_a_cafe,\n    equipment_spent_5years = approximately_how_much_have_you_spent_on_coffee_equipment_in_the_past_5_years,\n    good_value_equipment = do_you_feel_like_you_re_getting_good_value_for_your_money_with_regards_to_your_coffee_equipment\n  )\nCode\ncoffee_logical &lt;- coffee_drop |&gt;\n  select_if(is.logical)\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  drop_na(\n    colnames(coffee_logical)\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  mutate(\n    across(\n      where(\n        is.logical\n      ),\n      ~case_when(\n        .x == TRUE ~ 1,\n        .x == FALSE ~ 0\n      )\n    ),\n    across(\n      where(\n        is.character\n      ),\n      ~as.factor(.x)\n    )\n  )\n\ncoffee_drop &lt;- coffee_drop |&gt;\n  select(\n    -matches(\n      \"_notes\"\n    )\n  )\n\ncoffee_drop |&gt;\n  inspect_na() |&gt;\n  show_plot()"
  },
  {
    "objectID": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html#using-stan",
    "href": "posts/2024-11-03-bayes-net-stan-us-coffee-tasting/index.html#using-stan",
    "title": "Bayesian Network for US Coffee Tasting Data Using R & Stan",
    "section": "Using Stan",
    "text": "Using Stan\n\n\nCode\ncolnames(nona_allcat)\n\n\n [1] \"gender\"                 \"age\"                    \"cup_per_day\"           \n [4] \"home_brew_pour_over\"    \"home_brew_french_press\" \"home_brew_espresso\"    \n [7] \"home_brew_mr_coffee\"    \"home_brew_pods\"         \"home_brew_instant\"     \n[10] \"home_brew_bean2cup\"     \"home_brew_cold_brew\"    \"home_brew_cometeer\"    \n[13] \"home_brew_other\"        \"favorite_coffee_drink\"  \"roast_preference\"      \n[16] \"expertise\"              \"favorite_abcd\"         \n\n\nCode\nglimpse(nona_allcat)\n\n\nRows: 3,245\nColumns: 17\n$ gender                 &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\",…\n$ age                    &lt;chr&gt; \"over44\", \"25-34 years old\", \"35-44 years old\",…\n$ cup_per_day            &lt;chr&gt; \"2\", \"2\", \"one_or_less\", \"three_or_more\", \"2\", …\n$ home_brew_pour_over    &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,…\n$ home_brew_french_press &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,…\n$ home_brew_espresso     &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,…\n$ home_brew_mr_coffee    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ home_brew_pods         &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n$ home_brew_instant      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ home_brew_bean2cup     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ home_brew_cold_brew    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\n$ home_brew_cometeer     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ home_brew_other        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,…\n$ favorite_coffee_drink  &lt;chr&gt; \"drip\", \"pourover\", \"other\", \"pourover\", \"cappu…\n$ roast_preference       &lt;chr&gt; \"light\", \"light\", \"light\", \"dark\", \"light\", \"li…\n$ expertise              &lt;dbl&gt; 8, 10, 6, 7, 4, 8, 2, 7, 8, 7, 7, 6, 8, 6, 8, 8…\n$ favorite_abcd          &lt;fct&gt; Coffee B, Coffee A, Coffee D, Coffee A, Coffee …\n\n\nCode\ncolnames(nona_noref)\n\n\n [1] \"home_brew_french_press\" \"home_brew_espresso\"     \"home_brew_mr_coffee\"   \n [4] \"home_brew_pods\"         \"home_brew_instant\"      \"home_brew_bean2cup\"    \n [7] \"home_brew_cold_brew\"    \"home_brew_cometeer\"     \"home_brew_other\"       \n[10] \"favorite_abcd\"          \"female\"                 \"gen_other\"             \n[13] \"thirty544\"              \"over44\"                 \"under24\"               \n[16] \"cup_1orless\"            \"cup_3ormore\"            \"latte\"                 \n[19] \"other\"                  \"drip\"                   \"cappuccino\"            \n[22] \"espresso\"               \"cortado\"                \"americano\"             \n[25] \"roast_light\"            \"roast_dark\"             \"expert7\"               \n[28] \"expert6\"                \"expert8\"                \"expert4\"               \n[31] \"expert3\"                \"expert2\"                \"expert1\"               \n[34] \"expert9\"                \"expert10\"              \n\n\nCode\nnona_allcat &lt;- nona_allcat |&gt;\n  mutate(\n    gender = case_when(\n      gender == \"Male\" ~ 1,\n      gender == \"Female\" ~ 2,\n      gender == \"Other\" ~ 3\n    ),\n    age = case_when(\n      age == \"under24\" ~ 1,\n      age == \"25-34 years old\" ~ 2,\n      age == \"35-44 years old\" ~ 3,\n      age == \"over44\" ~ 4\n    ),\n    cup_per_day = case_when(\n      cup_per_day == \"one_or_less\" ~ 1,\n      cup_per_day == \"2\" ~ 2,\n      cup_per_day == \"three_or_more\" ~ 3\n    ),\n    favorite_coffee_drink = case_when(\n      favorite_coffee_drink == \"pourover\" ~ 1,\n      favorite_coffee_drink == \"latte\" ~ 2,\n      favorite_coffee_drink == \"other\" ~ 3,\n      favorite_coffee_drink == \"drip\" ~ 4,\n      favorite_coffee_drink == \"cappuccino\" ~ 5,\n      favorite_coffee_drink == \"espresso\" ~ 6,\n      favorite_coffee_drink == \"cortado\" ~ 7,\n      favorite_coffee_drink == \"americano\" ~ 8\n    ),\n    roast_preference = case_when(\n      roast_preference == \"light\" ~ 1,\n      roast_preference == \"medium\" ~ 2,\n      roast_preference == \"dark\" ~ 3\n    ),\n    expertise = case_when(\n      expertise == \"1\" ~ 1,\n      expertise == \"2\" ~ 2,\n      expertise == \"3\" ~ 3,\n      expertise == \"4\" ~ 4,\n      expertise == \"5\" ~ 5,\n      expertise == \"6\" ~ 6,\n      expertise == \"7\" ~ 7,\n      expertise == \"8\" ~ 8,\n      expertise == \"9\" ~ 9,\n      expertise == \"10\" ~ 10\n    ),\n    favorite_abcd = case_when(\n      favorite_abcd == \"Coffee A\" ~ 1,\n      favorite_abcd == \"Coffee B\" ~ 2,\n      favorite_abcd == \"Coffee C\" ~ 3,\n      favorite_abcd == \"Coffee D\" ~ 4\n    )\n  )\n\n\nnona_noref_recode &lt;- nona_noref |&gt;\n  mutate(\n    across(\n      -favorite_abcd,\n      ~case_when(\n        .x == 0 ~ -1,\n        TRUE ~ .x\n      )\n    )\n  )\n\n\n\n\nCode\nstan_allcat_list &lt;- list(\n  J = nrow(nona_allcat[,-1]),\n  y_cat = count(nona_allcat, favorite_abcd) |&gt; nrow(),\n  roast_preference_cat = count(nona_allcat, roast_preference) |&gt; nrow(),\n  expert_cat = count(nona_allcat, expertise) |&gt; nrow(),\n  cup_per_day_cat = count(nona_allcat, cup_per_day) |&gt; nrow(),\n  favorite_coffee_drink_cat = count(nona_allcat, favorite_coffee_drink) |&gt; nrow(),\n  gender_cat = count(nona_allcat, gender) |&gt; nrow(),\n  age_cat = count(nona_allcat, age) |&gt; nrow(),\n\n  Y = nona_allcat$favorite_abcd,\n  roast_preference = nona_allcat$roast_preference,\n  expert = nona_allcat$expertise,\n  favorite_coffee_drink = nona_allcat$favorite_coffee_drink,\n  cup_per_day = nona_allcat$cup_per_day,\n  home_brew_pourover = nona_allcat$home_brew_pour_over,\n  home_brew_french_press = nona_allcat$home_brew_french_press,\n  home_brew_espresso = nona_allcat$home_brew_espresso,\n  home_brew_mr_coffee = nona_allcat$home_brew_mr_coffee,\n  home_brew_pods = nona_allcat$home_brew_pods,\n  home_brew_instant = nona_allcat$home_brew_instant,\n  home_brew_bean2cup = nona_allcat$home_brew_bean2cup,\n  home_brew_cold_brew = nona_allcat$home_brew_cold_brew,\n  home_brew_cometeer = nona_allcat$home_brew_cometeer,\n  home_brew_other = nona_allcat$home_brew_other,\n  age = nona_allcat$age,\n  gender = nona_allcat$gender  \n)\n\nglimpse(stan_allcat_list)\n\n\nList of 25\n $ J                        : int 3245\n $ y_cat                    : int 4\n $ roast_preference_cat     : int 3\n $ expert_cat               : int 10\n $ cup_per_day_cat          : int 3\n $ favorite_coffee_drink_cat: int 8\n $ gender_cat               : int 3\n $ age_cat                  : int 4\n $ Y                        : num [1:3245] 2 1 4 1 1 4 4 4 3 2 ...\n $ roast_preference         : num [1:3245] 1 1 1 3 1 1 2 1 2 3 ...\n $ expert                   : num [1:3245] 8 10 6 7 4 8 2 7 8 7 ...\n $ favorite_coffee_drink    : num [1:3245] 4 1 3 1 5 1 2 8 6 7 ...\n $ cup_per_day              : num [1:3245] 2 2 1 3 2 2 2 1 3 2 ...\n $ home_brew_pourover       : num [1:3245] 1 1 1 1 0 1 0 0 1 0 ...\n $ home_brew_french_press   : num [1:3245] 1 0 0 1 0 0 0 0 1 0 ...\n $ home_brew_espresso       : num [1:3245] 1 1 0 0 1 0 0 0 1 1 ...\n $ home_brew_mr_coffee      : num [1:3245] 0 0 0 0 0 0 0 0 0 0 ...\n $ home_brew_pods           : num [1:3245] 0 0 0 1 1 0 0 0 0 0 ...\n $ home_brew_instant        : num [1:3245] 0 0 0 0 0 0 0 0 0 0 ...\n $ home_brew_bean2cup       : num [1:3245] 0 0 0 0 0 0 0 0 0 0 ...\n $ home_brew_cold_brew      : num [1:3245] 0 0 0 0 0 0 0 0 1 0 ...\n $ home_brew_cometeer       : num [1:3245] 0 0 0 0 0 1 0 0 0 0 ...\n $ home_brew_other          : num [1:3245] 0 0 0 0 0 0 1 1 0 0 ...\n $ age                      : num [1:3245] 4 2 3 4 3 4 3 3 3 2 ...\n $ gender                   : num [1:3245] 1 1 1 1 1 1 1 2 1 1 ...\n\n\n\n\nCode\nstan_list &lt;- list(\n  J = nrow(nona_noref_recode[,-1]),\n  I = ncol(nona_noref_recode[,-1]),\n  K = count(nona_noref_recode, favorite_abcd) |&gt; nrow(),\n\n  female = nona_noref_recode$female,\n  gen_other = nona_noref_recode$gen_other,\n\n  thirty544 = nona_noref_recode$thirty544,\n  over44 = nona_noref_recode$over44,\n  under24 = nona_noref_recode$under24,\n\n\n  home_brew_french_press = nona_noref_recode$home_brew_french_press,\n  home_brew_espresso = nona_noref_recode$home_brew_espresso,\n  home_brew_mr_coffee = nona_noref_recode$home_brew_mr_coffee,\n  home_brew_pods = nona_noref_recode$home_brew_pods,\n  home_brew_instant = nona_noref_recode$home_brew_instant,\n  home_brew_bean2cup = nona_noref_recode$home_brew_bean2cup,\n  home_brew_cold_brew = nona_noref_recode$home_brew_cold_brew,\n  home_brew_cometeer = nona_noref_recode$home_brew_cometeer,\n  home_brew_other = nona_noref_recode$home_brew_other,\n  \n  latte = nona_noref_recode$latte,\n  otherdrink = nona_noref_recode$other,\n  drip = nona_noref_recode$drip,\n  cappuccino = nona_noref_recode$cappuccino,\n  espresso = nona_noref_recode$espresso,\n  cortado = nona_noref_recode$cortado,\n\n  cup_1orless = nona_noref_recode$cup_1orless,\n  cup_3ormore = nona_noref_recode$cup_3ormore,\n\n\n  americano = nona_noref_recode$americano,         \n  roast_light = nona_noref_recode$roast_light,\n  roast_dark = nona_noref_recode$roast_dark,\n\n  expert7 = nona_noref_recode$expert7,\n  expert6 = nona_noref_recode$expert6,\n  expert8 = nona_noref_recode$expert8,\n  expert4 = nona_noref_recode$expert4,\n  expert3 = nona_noref_recode$expert3,\n  expert2 = nona_noref_recode$expert2,\n  expert1 = nona_noref_recode$expert1,\n  expert9 = nona_noref_recode$expert9,\n  expert10 = nona_noref_recode$expert10,\n\n  Y = nona_noref_recode$favorite_abcd\n)\n\n# glimpse(stan_list)\n\n\n\n\nCode\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"random_scripts\", \"updated_stan_model.stan\"))\n\n# mod$format(\n#   canonicalize = list(\"deprecations\"),\n#   overwrite_file = TRUE,\n#   backup = FALSE\n# )\n\n\n\n\nCode\nfit &lt;- mod$sample(\n  data = stan_list,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000,\n  # adapt_delta = .90,\n  chains = 4,\n  # step_size = .01,\n  parallel_chains = 8\n)\n\nsaveRDS(fit, \"coffee_tasting_bayes_net.RDS\")\n\n# fit &lt;- read_rds(here::here(\"random_data\", \"coffee_tasting_bayes_net.RDS\"))\n\n\n\n\nCode\nfit$output()[[1]]\n\nfit$diagnostic_summary()\n\n\n\n\nCode\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\n\n\n\n\nCode\nbn_converge |&gt;\n  arrange(rhat) |&gt;\n  mutate(\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"theta\"\n    )\n  ) |&gt; \n  mutate(\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  select(\n    variable,\n    mean,\n    sd\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\nCode\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"predictors\"\n    )\n  )\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"^a_\"\n    ) |\n    str_detect(\n      variable,\n      \"^b_\"\n    )\n  ) |&gt; \n  mutate(\n    across(\n      -variable,\n      ~exp(.x)\n    ),\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  select(\n    variable,\n    mean,\n    sd\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\nCode\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"random_scripts\", \"all_category_model.stan\"))\n\nfit &lt;- mod$sample(\n  data = stan_allcat_list,\n  seed = 12345,\n  iter_warmup = 1000, #2000\n  iter_sampling = 1000, #2000\n  # adapt_delta = .90,\n  chains = 4,\n  # step_size = .01,\n  parallel_chains = 8\n)\n\nfit$diagnostic_summary()\n\n\n\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\n\nbn_converge |&gt;\n  arrange(rhat) |&gt;\n  mutate(\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_measure |&gt;\n  filter(\n    str_detect(\n      variable,\n      \"^a_\"\n    ) |\n    str_detect(\n      variable,\n      \"^b_\"\n    ) |\n    str_detect(\n      variable,\n      \"log_lik\"\n    )\n  ) |&gt;\n  select(\n    variable,\n    log_odds = mean,\n    sd\n  ) |&gt;\n  mutate(\n    prob_use = plogis(log_odds),\n    prob_notuse = 1 - prob_use,\n    #prob = exp(log_odds)/exp(1 + log_odds),\n    across(\n      -variable,\n      ~round(.x, 2)\n    )\n  )  |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\nInterpretations for select parameters\nThe alpha parameter for predicting participants’ favorite coffee showed that participants who prefer medium roasted coffee and labeled themselves as a 5 on expertise had 1.14 times greater odds of enjoying coffee C than not enjoying it.\nalpha = compared to zero\nFor example, b_roast_light_yb_roast_light_y is the change in log-odds for a given category when the roast level is light. Similarly, b_expert1_yb_expert1_y to b_expert10_yb_expert10_y represent the effects of being in each expert level on the log-odds of a particular category, again without any specific reference category.\n\n\nCode\nalphas &lt;- c(\n\"a_latte\",\n\"a_otherdrink\",\n\"a_drip\",\n\"a_cappuccino\",\n\"a_espresso\",\n\"a_cortado\",\n\"a_americano\",\n\"a_home_frenchpress\",\n\"a_home_espresso\",\n\"a_home_mrcoffee\",\n\"a_home_pods\",\n\"a_home_instant\",\n\"a_home_bean2cup\",\n\"a_home_coldbrew\",\n\"a_home_cometeer\",\n\"a_home_other\",\n\"a_cup_1orless\",\n\"a_cup_3ormore\",\n\"a_roast_light\",\n\"a_roast_dark\",\n\"a_expert1\",\n\"a_expert2\",\n\"a_expert3\",\n\"a_expert4\",\n\"a_expert6\",\n\"a_expert7\",\n\"a_expert8\",\n\"a_expert9\",\n\"a_expert10\",\n\"a_y[1]\",\n\"a_y[2]\", \n\"a_y[3]\",\n\"a_y[4]\"\n)\n\nb_latte &lt;- c(\n  \"b_female_latte\",\n  \"b_gen_other_latte\",\n  \"b_thirty544_latte\",\n  \"b_over44_latte\",\n  \"b_under24_latte\",\n  \"b_home_frenchpress_latte\",\n  \"b_home_espresso_latte\",\n  \"b_home_mrcoffee_latte\",\n  \"b_home_pods_latte\",\n  \"b_home_instant_latte\",\n  \"b_home_bean2cup_latte\",\n  \"b_home_coldbrew_latte\",\n  \"b_home_cometeer_latte\",\n  \"b_home_other_latte\"\n)\n\nalphas &lt;- fit$draws(alphas) |&gt; as_draws_matrix()\nbeta_var_latte &lt;- fit$draws(b_latte) |&gt; as_draws_matrix()\n\n\n\n\nCode\nmcmc_trace(alphas)\nmcmc_trace(beta_var_latte)\n\n\n\n\nCode\nmcmc_areas(alphas)\nmcmc_areas(beta_var_latte)"
  },
  {
    "objectID": "index.html#quarto-extensions",
    "href": "index.html#quarto-extensions",
    "title": "Resources",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension"
  },
  {
    "objectID": "posts/2024-11-04-closeread-temp-blanket/index.html",
    "href": "posts/2024-11-04-closeread-temp-blanket/index.html",
    "title": "My submission to Posit’s Closeread Competition",
    "section": "",
    "text": "When I saw that Posit had posted a blog post about storytelling with Quarto (see blog post here) using Closeread I became interested in trying out Closeread for this competition. This is my first time reading about Closeread and it sparked my interest in trying to tell a story using a quarto document. For my submission, I thought I would do something silly about a topic that is serious. I am going to try my hardest to write this in a serious way, but this is also probably going to dissolve into a delusional fever dream of a post. I wanted to show the weather differences in Los Angeles from 2013 to 2023 by creating a story of global warming while showing visualizations of the temperature differences using “temperature blankets”. If you know anything about me, the first thing you recognize is art != JP so instead I thought I would create a temperature blanket using ggplot2.\nFor the competition I decided to do everything in R because I knew I was going to be mess around a lot using the theme() function in ggplot2. So below is going to be a walkthough of my thought process for creating the Closeread story. Then I will probably create a Closeread specific GitHub repo.\n\nLoading the Data\nI’m going to start by loading the tidyverse package and reading the data. I specifically wanted data from Los Angeles because I live here and thought I would compare a ten-year difference since I also lived here in 2013. I got data here, but I have not looked over the data at all.\n\n\nCode\nlibrary(tidyverse)\n\ntemp &lt;- read_csv(here::here(\"posts/2024-11-04-closeread-temp-blanket\", \"weather_data_la2013_la2023.csv\")) |&gt;\n  janitor::clean_names()\n\n\nSo I decided it is probably best to make sure everything is correct in the data.\n\n\nCode\ntemp |&gt;\n  count(city)\n\n\n# A tibble: 1 × 2\n  city            n\n  &lt;chr&gt;       &lt;int&gt;\n1 Los Angeles   730\n\n\nCode\ntemp |&gt;\n  inspectdf::inspect_na()\n\n\n# A tibble: 12 × 3\n   col_name                cnt  pcnt\n   &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;\n 1 daytime_h_m               7 0.959\n 2 item_number               0 0    \n 3 date                      0 0    \n 4 location_index            0 0    \n 5 city                      0 0    \n 6 state_region              0 0    \n 7 country                   0 0    \n 8 high_temperature_f        0 0    \n 9 average_temperature_f     0 0    \n10 low_temperature_f         0 0    \n11 rain_in                   0 0    \n12 snow_in                   0 0    \n\n\nOkay, so everything looks okay to me. While there are plenty of data points to look at, I think I am going to focus on the average temperature to create my temperature blankets. I also just want to point out that all temperatures will be in Farenheit. Before making my visualization, I want to break down my date category into years, months, and days. This will be easier to use facet_wrap() to separate my years.\n\n\nCode\ntemp &lt;- temp |&gt;\n  mutate(date2 = date) |&gt;\n  separate(\n    date2,\n    into = c(\n      \"year\", \"month\", \"day\"\n    ),\n    sep = \"-\"\n  )\n\n\nNow, we can focus on creating our plot. Interestingly, I have never had to make one axis on my plots be set to an amount that would range across the entirety of the axis. This was definitely one of those times where I just tried something and BAM! it worked. I set my y-axis to 1 and it worked. We will not focus on the values for the y-axis because they don’t make any sense, but I did try some other values. The value that you choose on the y-axis does not matter, especially for this visualization because we are going to remove the axis titles and text.\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = average_temperature_f\n    )\n  ) +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nFor some reason, I am not a fan of the blankets being horizontal, so I’m going to change the orientation of them.\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = average_temperature_f\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nWe need a color scale. This will be a manual scale to try and make a cool blanket style so after some googling I found this scale project sheet. Shoutout to the Craft Warehouse because this was the easiest scale to follow (for me) and the large balls of yarn allowed me to find color codes that matched the yarn fairly well.\nSome quick coding of the temperature ranges and the months should set up the template for my blanket design.\n\n\nCode\ntemp &lt;- temp |&gt;\n  mutate(\n    temp_color = case_when(\n      average_temperature_f &gt; 96 ~ \"96+\", #cherry red\n      average_temperature_f &gt;= 89 & average_temperature_f &lt; 96 ~ \"89-95\", #really red\n      average_temperature_f &gt;= 82 & average_temperature_f &lt; 89 ~ \"82-88\", #carrot\n      average_temperature_f &gt;= 75 & average_temperature_f &lt; 82 ~ \"75-81\", #canary\n      average_temperature_f &gt;= 68 & average_temperature_f &lt; 75 ~ \"68-74\", #yellow\n      average_temperature_f &gt;= 61 & average_temperature_f &lt; 68 ~ \"61-67\", #green apple\n      average_temperature_f &gt;= 54 & average_temperature_f &lt; 61 ~ \"54-60\", #porcelain blue\n      average_temperature_f &gt;= 47 & average_temperature_f &lt; 54 ~ \"47-53\", #teal\n      average_temperature_f &gt;= 40 & average_temperature_f &lt; 47 ~ \"40-46\", #alaskan blue\n      average_temperature_f &gt;= 33 & average_temperature_f &lt; 40 ~ \"33-39\", #cobalt\n      average_temperature_f &gt;= 26 & average_temperature_f &lt; 33 ~ \"26-32\", #thistle\n      average_temperature_f &lt; 26 ~ \"Below 26\" #purple\n    ),\n    month_name = case_when(\n      month == \"01\" ~ \"Jan\",\n      month == \"02\" ~ \"Feb\",\n      month == \"03\" ~ \"Mar\",\n      month == \"04\" ~ \"Apr\",\n      month == \"05\" ~ \"May\",\n      month == \"06\" ~ \"Jun\",\n      month == \"07\" ~ \"Jul\",\n      month == \"08\" ~ \"Aug\",\n      month == \"09\" ~ \"Sept\",\n      month == \"10\" ~ \"Oct\",\n      month == \"11\" ~ \"Nov\",\n      month == \"12\" ~ \"Dec\"\n    ),\n    across(\n      c(\n        temp_color,\n        month_name\n      ),\n      ~as.factor(.x)\n    ),\n    temp_color = fct_relevel(\n      temp_color,\n      \"96+\",\n      \"89-95\",\n      \"82-88\",\n      \"75-81\",\n      \"68-74\",\n      \"61-67\",\n      \"54-60\",\n      \"47-53\",\n      \"40-46\",\n      \"33-39\",\n      \"26-32\",\n      \"Below 26\"\n    )\n  )\n\n\nThis is now how the full blanket for both 2013 and 2023. I still need to clean up the axis labels, the legend, and the facet titles.\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nI was thinking about how to show the progress of the blanket. I decided to use the dplyr::first() function, which was a first for me. Here I have the first day of the year with the temperature range values for 2013 and 2023.\n\n\nCode\ntemp |&gt;\n  group_by(year) |&gt;\n  mutate(\n    first = first(item_number)\n  ) |&gt;\n  distinct(\n    first,\n    .keep_all = TRUE\n    ) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nNext, I decided to do a quick loop where I used two different seq() functions. The first sequence was to creatte a row for each day of the year from 1 to 365 by each day. I was able to create rows called year_row for both years. There is probably a better way of doing this but I decided on using a filter() where I looped through each day and put that each day was less than the year_row column created. For instance, in the code chunk below you can see it as filter(year_row &lt; .x). So when the loop starts at 2 it will filter for values that are lower than 2, with the only value being 1. This will be important later on because I am not sure if I want to present a story of the weather and a creation of my blanket for every single day or if there is another metric to go by.\n\n\nCode\nmap(\n  seq(2, 10, 1),\n  ~temp |&gt;\n  select(\n    item_number,\n    date,\n    year,\n    temp_color,\n    average_temperature_f\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\nI ended up deciding on showing my blanket every week. I’m still not sure about it since there are are 52 plots to look through. Also, now that I think about it, maybe LA was not the best place to showcase temperature for a blanket. Looking at the final blanket plot, it screams “Sorry its always nice here!” and that might not be the most interesting for a temperature blanket. I’ll have to start looking at other locations for better blanket designs. At least now I have the basic design of what I’ll do for the competition.\n\n\nCode\nmap(\n  seq(2, 365, 7),\n  ~temp |&gt;\n  select(\n    item_number,\n    date,\n    year,\n    temp_color,\n    average_temperature_f\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]\n\n\n\n\n\n\n\n\n\n\n[[15]]\n\n\n\n\n\n\n\n\n\n\n[[16]]\n\n\n\n\n\n\n\n\n\n\n[[17]]\n\n\n\n\n\n\n\n\n\n\n[[18]]\n\n\n\n\n\n\n\n\n\n\n[[19]]\n\n\n\n\n\n\n\n\n\n\n[[20]]\n\n\n\n\n\n\n\n\n\n\n[[21]]\n\n\n\n\n\n\n\n\n\n\n[[22]]\n\n\n\n\n\n\n\n\n\n\n[[23]]\n\n\n\n\n\n\n\n\n\n\n[[24]]\n\n\n\n\n\n\n\n\n\n\n[[25]]\n\n\n\n\n\n\n\n\n\n\n[[26]]\n\n\n\n\n\n\n\n\n\n\n[[27]]\n\n\n\n\n\n\n\n\n\n\n[[28]]\n\n\n\n\n\n\n\n\n\n\n[[29]]\n\n\n\n\n\n\n\n\n\n\n[[30]]\n\n\n\n\n\n\n\n\n\n\n[[31]]\n\n\n\n\n\n\n\n\n\n\n[[32]]\n\n\n\n\n\n\n\n\n\n\n[[33]]\n\n\n\n\n\n\n\n\n\n\n[[34]]\n\n\n\n\n\n\n\n\n\n\n[[35]]\n\n\n\n\n\n\n\n\n\n\n[[36]]\n\n\n\n\n\n\n\n\n\n\n[[37]]\n\n\n\n\n\n\n\n\n\n\n[[38]]\n\n\n\n\n\n\n\n\n\n\n[[39]]\n\n\n\n\n\n\n\n\n\n\n[[40]]\n\n\n\n\n\n\n\n\n\n\n[[41]]\n\n\n\n\n\n\n\n\n\n\n[[42]]\n\n\n\n\n\n\n\n\n\n\n[[43]]\n\n\n\n\n\n\n\n\n\n\n[[44]]\n\n\n\n\n\n\n\n\n\n\n[[45]]\n\n\n\n\n\n\n\n\n\n\n[[46]]\n\n\n\n\n\n\n\n\n\n\n[[47]]\n\n\n\n\n\n\n\n\n\n\n[[48]]\n\n\n\n\n\n\n\n\n\n\n[[49]]\n\n\n\n\n\n\n\n\n\n\n[[50]]\n\n\n\n\n\n\n\n\n\n\n[[51]]\n\n\n\n\n\n\n\n\n\n\n[[52]]"
  },
  {
    "objectID": "posts/2021-04-30-twitter-conference-presentation/index.html",
    "href": "posts/2021-04-30-twitter-conference-presentation/index.html",
    "title": "Twitter Conference Presentation",
    "section": "",
    "text": "I thought I’d talk about the analyses I conducted for my submission to the American Public Health Association Physical Activity Section’s Twitter Conference. I thought it was a fun opportunity to disseminate some analyses that I conducted using public data from the County Health Rankings and Roadmap to examine what factors are associated with leisure-time physical activity (LTPA) in counties in California from 2016 to 2020. If you are interested in the presentation itself, you can follow it here.\nPhysical activity is important as its related to many physical and mental health conditions. It is also a health behavior that can be modified slightly easier than other health behaviors. While not as beneficial as extended periods of exercise, even walking for leisure can be beneficial for one’s health. I was predominately interested in California because I wanted to know how much variation there was between the counties. For instance, there are areas like San Francisco county and Los Angeles County, which may be seen as hubs for cultures of being physically active, but what about counties throughout Central California. I’m also interested in LTPA engagement because this health behavior has several social determinants of health that impact how much LTPA individuals can engage in. The social determinant that I’m most interested in is the role that access to recreational facilities and parks have on counties’ LTPA engagement. Since I was interested in looking at variation between counties while also examining the longitudinal association between access and LTPA, I decided to create a two-level multilevel model with time (level 1) nested within counties (level 2).\n\nPackages ued\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: package 'inspectdf' was built under R version 4.3.2\n\n\nWarning: package 'psych' was built under R version 4.3.1\n\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\nWarning: package 'lme4' was built under R version 4.3.1\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nWarning: package 'lmerTest' was built under R version 4.3.3\n\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nWarning: package 'optimx' was built under R version 4.3.3\n\n\nWarning: package 'ggmap' was built under R version 4.3.3\n\n\nℹ Google's Terms of Service: &lt;https://mapsplatform.google.com&gt;\n  Stadia Maps' Terms of Service: &lt;https://stadiamaps.com/terms-of-service/&gt;\n  OpenStreetMap's Tile Usage Policy: &lt;https://operations.osmfoundation.org/policies/tiles/&gt;\nℹ Please cite ggmap if you use it! Use `citation(\"ggmap\")` for details.\n\n\nWarning: package 'maps' was built under R version 4.3.3\n\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nWarning: package 'RColorBrewer' was built under R version 4.3.1\n\n\nWarning: package 'ggrepel' was built under R version 4.3.3\n\n\nWarning: package 'gganimate' was built under R version 4.3.3\n\n\nWarning: package 'transformr' was built under R version 4.3.3\n\n\n\n\n\n\n\n\n\n\n\nFunctions\nBefore beginning I created a simple function to get the intraclass correlation coefficient (ICC). I also included a function to get data from the 5 years of County Health Rankings data. The function to get the ICC from random-intercept models is the between county variation divided by the total variation between counties and within counties. This gives you information about how much of the variation in the model can be attributed to differences between counties regarding your outcome. Random-intercept models were tested because I was only interested in knowing the variation in counties’ LTPA engagement. There were also not enough points for each county to test individual slopes for each county (random-slopes model).\n\n\nWarning: package 'curl' was built under R version 4.3.3\n\n\nUsing libcurl 8.3.0 with Schannel\n\n\n\nAttaching package: 'curl'\n\n\nThe following object is masked from 'package:readr':\n\n    parse_date\n\n\nI also made some slight changes to my data. The first was to get rid of the estimates for each state and only focus on estimates from the county. I also wanted to treat year as a continuous variable in my models but wanted to keep the year variable as a factor too. Then after filtering to only examine California counties I used the str_replace_all function from the stringr package to get rid of the county name after each observation. This was to make it easier to join with map data from the maps package. Lastly, I made the counties title case to also make joining the observations easier.\n\n\nModels\nNow when running the first model, I was first interested in examining if there was an increase in LTPA engagement in all California counties from 2016 to 2020. From the finding below, it shows that in California, there was a decrease in LTPA over that time. It’s also important to note that lmerTest and lme4 both have a lmer function. By namespacing them with two colons, you can see that the summary information is slightly different.\n\n\nCode\npreliminary_ltpa_long &lt;- lmerTest::lmer(ltpa_percent ~ year_num + (1 | county_fips_code), data = ca,\n                              REML = FALSE)\nsummary(preliminary_ltpa_long)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: ltpa_percent ~ year_num + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1427.9   1442.5   -709.9   1419.9      286 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6854 -0.3709  0.0361  0.5377  1.7084 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 7.187    2.681   \n Residual                     5.174    2.275   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n              Estimate Std. Error         df t value            Pr(&gt;|t|)    \n(Intercept) 1923.40172  190.60225  231.84941  10.091 &lt;0.0000000000000002 ***\nyear_num      -0.91276    0.09445  231.84760  -9.664 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr)\nyear_num -1.000\n\n\nCode\nprelim_ltpa_lmer &lt;- lme4::lmer(ltpa_percent ~ year_num +(1 | county_fips_code), data = ca,\n                              REML = FALSE)\nsummary(prelim_ltpa_lmer)\n\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: ltpa_percent ~ year_num + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1427.9   1442.5   -709.9   1419.9      286 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6854 -0.3709  0.0361  0.5377  1.7084 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 7.187    2.681   \n Residual                     5.174    2.275   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept) 1923.40172  190.60225  10.091\nyear_num      -0.91276    0.09445  -9.664\n\nCorrelation of Fixed Effects:\n         (Intr)\nyear_num -1.000\n\n\nCode\nltpa_null_icc &lt;- as_tibble(VarCorr(preliminary_ltpa_long))\nltpa_null_icc\n\n\n# A tibble: 2 × 5\n  grp              var1        var2   vcov sdcor\n  &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 county_fips_code (Intercept) &lt;NA&gt;   7.19  2.68\n2 Residual         &lt;NA&gt;        &lt;NA&gt;   5.17  2.27\n\n\nCode\ncounty_icc_2level(ltpa_null_icc)\n\n\n[1] 0.5814093\n\n\nAlong with the fixed effects, we also got our random effects for both differences found between counties for LTPA engagement and differences within counties for LTPA engagement. This shows that there was a good amount of variation between counties (σ2 = 7.19) but also a large amount of variation within each county in California (σ2 = 5.17). Using the function to calculate the ICC, it found that county differences attributed to 58% of the variation in LTPA engagement. Something that should be considered is the potential for heteroscedastic residual variance at level 1. There is also the issue that the residuals could suggest spatial autocorrelation or clustering within these counties. Maybe I’ll create something on these soon. But for the time being, lets move on to what was found for the twitter conference.\n\n\nCode\nltpa_long_access &lt;- lmer(ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent +\n                           access_pa_percent + (1 | county_fips_code), data = ca,\n                         REML = FALSE)\n\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nCode\nanova(preliminary_ltpa_long, ltpa_long_access)\n\n\nData: ca\nModels:\npreliminary_ltpa_long: ltpa_percent ~ year_num + (1 | county_fips_code)\nltpa_long_access: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + access_pa_percent + (1 | county_fips_code)\n                      npar    AIC    BIC  logLik deviance  Chisq Df\npreliminary_ltpa_long    4 1427.9 1442.5 -709.93   1419.9          \nltpa_long_access         9 1237.4 1270.4 -609.69   1219.4 200.47  5\n                                 Pr(&gt;Chisq)    \npreliminary_ltpa_long                          \nltpa_long_access      &lt; 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nother_var &lt;- lmer(ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + (1 | county_fips_code), data = ca,\n                         REML = FALSE)\n\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nCode\nanova(other_var, ltpa_long_access)\n\n\nData: ca\nModels:\nother_var: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + (1 | county_fips_code)\nltpa_long_access: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + access_pa_percent + (1 | county_fips_code)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nother_var           8 1240.1 1269.4 -612.04   1224.1                       \nltpa_long_access    9 1237.4 1270.4 -609.69   1219.4 4.6883  1    0.03037 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith the inclusion of several predictors for fixed effects, a likelihood ratio test was conducted to see if the inclusion of these fixed effects revealed a significantly better fitting model. The inclusion of these predictors revealed a better fitting model. It would probably be better to see if the inclusion of one variable of interest, such as access, resulted in a better fitting model than a model with the other social determinants of health. As can be see here, the likelihood ratio test of including only access still resulted in a signifcantly better fitting model.\n\n\nCode\nsummary(ltpa_long_access)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income +  \n    rural_percent + access_pa_percent + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1237.4   1270.4   -609.7   1219.4      281 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8388 -0.5546  0.0010  0.6179  2.4921 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 1.286    1.134   \n Residual                     3.139    1.772   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n                              Estimate     Std. Error             df t value\n(Intercept)             1138.319788532  193.185926311  289.534084648   5.892\nyear_num                  -0.517087679    0.096244053  289.365968612  -5.373\nviolent_crime             -0.001434185    0.001190066   82.732734919  -1.205\nobesity_percent           -0.602338792    0.043760246  262.840283632 -13.765\nmedian_household_income    0.000004822    0.000014701  102.255262863   0.328\nrural_percent             -0.012178273    0.007671490   63.272282975  -1.587\naccess_pa_percent          0.027194696    0.012124625  153.692013407   2.243\n                                    Pr(&gt;|t|)    \n(Intercept)                     0.0000000106 ***\nyear_num                        0.0000001597 ***\nviolent_crime                         0.2316    \nobesity_percent         &lt; 0.0000000000000002 ***\nmedian_household_income               0.7436    \nrural_percent                         0.1174    \naccess_pa_percent                     0.0263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) yer_nm vlnt_c obsty_ mdn_h_ rrl_pr\nyear_num    -1.000                                   \nviolent_crm  0.116 -0.118                            \nobsty_prcnt  0.489 -0.494 -0.100                     \nmdn_hshld_n  0.586 -0.589  0.203  0.452              \nrural_prcnt  0.260 -0.264  0.135  0.203  0.447       \naccss_p_prc -0.147  0.142 -0.018  0.054 -0.341  0.158\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nThe model summary suggests that the fixed effect of access on LTPA engagement was significantly associated. The thing that stands out the most here is that the inclusion of the predictors resulted in more variation within counties than between counties. So lets look into that more closely.\n\n\nCode\nltpa_access_icc &lt;- as_tibble(VarCorr(ltpa_long_access))\nltpa_access_icc\n\n\n# A tibble: 2 × 5\n  grp              var1        var2   vcov sdcor\n  &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 county_fips_code (Intercept) &lt;NA&gt;   1.29  1.13\n2 Residual         &lt;NA&gt;        &lt;NA&gt;   3.14  1.77\n\n\nCode\ncounty_icc_2level(ltpa_access_icc)\n\n\n[1] 0.2906253\n\n\nThe ICC suggests that 29% of the variation explained is from differences between counties. It is also beneficial to look at all of this through visuals.\n\n\nVisuals Prep\nBelow we’ll start by using the maps package to get county-level data of the contiguous United States. The steps below were to make sure this data frame joined with the county health rankings data we had created previously.\n\n\nVisualizing model\nOne way to visualize the variation between counties in our final model (ltpa_long_access) is to use a caterpillar plot. This allows you to view variation in the residuals of each county for your outcome. From the visual, you can see the differences between Humboldt County and Tehama County.\n\n\nCode\nmain_effects_var &lt;- ranef(ltpa_long_access, condVar = TRUE)\n\nmain_effects_var &lt;- as.data.frame(main_effects_var)\n\nmain_effects_var &lt;- main_effects_var %&gt;% \n  mutate(main_effects_term = term,\n         county_fips_code = grp,\n         main_effects_diff = condval,\n         main_effects_se = condsd,\n         county_fips_code = as.numeric(county_fips_code))\n\nmain_effects_var$no_name_county &lt;- unique(ca$no_name_county)\n\nmain_effects_var %&gt;% \n  ggplot(aes(fct_reorder(no_name_county, main_effects_diff), main_effects_diff)) +\n  geom_errorbar(aes(ymin = main_effects_diff + qnorm(0.025)*main_effects_se,\n                  ymax = main_effects_diff + qnorm(0.975)*main_effects_se)) +\n  geom_point(aes(color = no_name_county)) +\n  coord_flip() +\n  labs(x = ' ',\n     y = 'Differences in Leisure-time Physical Activity',\n     title = 'Variation in Leisure-time Physical Activity\\nAcross California Counties') +\n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\nThis plot shows the fixed effect of access and LTPA across the various years.\n\n\nCode\nca %&gt;% \n  mutate(year = as.factor(year)) %&gt;% \n  ggplot(aes(access_pa_percent, ltpa_percent)) +\n  geom_point(aes(color = year)) +\n  geom_smooth(color = 'dodgerblue',\n            method = 'lm', se = FALSE, size = 1) +\n  theme(legend.title = element_blank()) +\n  labs(x = 'Access to Physical Activity Opportunities',\n       y = 'Leisure-time Physical Activity',\n       title = 'The Statewide Association of Access\\nand Physical Activity')\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, a gif of the change of LTPA from 2016 to 2020.\n\n\nCode\nlibrary(gganimate)\n\nca_animate &lt;- ca_visual %&gt;%\n  ggplot(aes(frame = year,\n             cumulative = TRUE)) +\n  geom_polygon(aes(x = long, y = lat, \n                   group = group, \n                   fill = ltpa_percent),\n               color = 'black') +\n  scale_fill_gradientn(colors = brewer.pal(n = 5, name = 'RdYlGn')) + \n  theme_classic() +\n  transition_time(year) +\n  labs(x = 'Longitude',\n       y = 'Latitude',\n       title = 'Leisure-time Physical Activity\\nChange Over Time',\n       subtitle = 'Year: {frame_time}') +\n  theme(legend.title = element_blank(),\n        legend.text = element_text(size = 12),\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 10),\n        plot.title = element_text(size = 20),\n        plot.subtitle = element_text(size = 18))\n\nlibrary(gifski)\n\n\nWarning: package 'gifski' was built under R version 4.3.3\n\n\nCode\nanimate(ca_animate, renderer = gifski_renderer())"
  },
  {
    "objectID": "posts/2021-04-30-grad-student-exit-surveys/index.html",
    "href": "posts/2021-04-30-grad-student-exit-surveys/index.html",
    "title": "Graduate Student Satisfaction Exit Surveys",
    "section": "",
    "text": "Photo by Nik on Unsplash\n\n\nThis post was originally designed because I was interested in working on student experience exit survey data from my department to see if there was a change from 2012 to 2015. These questions are given to every student that graduates from a graduate program at the University of Oregon (UO). This includes responses for terminal masters degrees, students that get masters degrees and move on to a doctoral degree, and the same potential students that respond again when they get their doctorate. This data is open to any UO student, staff, or faculty member that has login information for this data.\nThis data ended up becoming a time commitment as there was no efficient way to collect data from the pdf files for each college at the UO. An example can be seen here. One great resource for collecting data from pdfs was to use the pdftools package, but if you look at the example link provided above, the UO Graduate School decided to color code cells in the table, which threw off any function to extract all the values in an efficient manner. Anyway…\nThe data and other existing data files can be found here. When I have some more free time, I may decide to join the other datasets to the student experience data to examine some more interesting questions regarding this data. But for now, lets look at the student experience data.\n\n\nCode\nlibrary(tidyverse)\n\ntheme_set(theme_minimal())\n\nexit &lt;- read_csv(here::here(\"posts\", \"2021-04-30-grad-student-exit-surveys/exit_data.csv\")) |&gt; \n  janitor::clean_names() |&gt;\n  mutate(\n    program = str_replace_all(program, \"_\", \" \"),\n    program = str_to_title(program)\n  )\n\n\nThese exit surveys have several questions that are broken down into percentages about how many of the students agreed or disagreed with the statement. For instance, from the pdf, the first statement is Quality of the faculty in a student’s department. So we can look at that with this first plot. At the same time, we can also look at the difference between the two years of data. In order to look at all the variables at the same time that have the starting string of fac_qual, I’ll use pivot_longer to collect any variable that has that variable string about faculty quality. Since the first and second table on the pdf refer to excellent or good or excellent levels of student satisfaction about faculty quality, I decided to filter out the excellent student satisfaction and move on with only student satisfaction that is either good or excellent.\n\n\nCode\nexit |&gt; \n  pivot_longer(\n    matches(\n      \"^fac_qual\"\n    ),\n    names_to = \"fac_qual\",\n    values_to = \"fac_values\"\n  ) |&gt; \n  filter(fac_qual == \"fac_qual_ex_good\") |&gt;\n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_col(aes(fill = as.factor(year)), position = \"dodge2\") +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"Specific Student Experience\",\n       caption = \"Ex = Excellent\") +\n  coord_flip() +\n  facet_wrap(~fac_qual, scales = \"free\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\nexit |&gt; \n  pivot_longer(\n    matches(\n      \"^fac_qual\"\n    ),\n    names_to = \"fac_qual\",\n    values_to = \"fac_values\"\n  ) |&gt; \n  filter(fac_qual == \"fac_qual_fair_poor\") |&gt;\n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_col(aes(fill = as.factor(year)), position = \"dodge2\") +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"Specific Student Experience\",\n       caption = \"Ex = Excellent\") +\n  coord_flip() +\n  facet_wrap(~fac_qual, scales = \"free\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nSo the first shot at making a visual for the two years looks a little cluttered because of using geom_col(). My first decision was to remove the columns and change those to points to make it a little less cluttered and clearer. I already enjoyed the way this looked better. I also decided to clean some things up by changing the names of the variables to better describe what the variables were assessing. I also decided to go back and change the programs to be title case and with spaces rather than underscores.\n\n\nCode\nexit |&gt; \n  pivot_longer(\n    matches(\n      \"^fac_qual\"\n    ),\n    names_to = \"fac_qual\",\n    values_to = \"fac_values\"\n  ) |&gt; \n  filter(fac_qual == \"fac_qual_ex_good\") |&gt;\n  mutate(fac_qual = recode(fac_qual, \"fac_qual_ex_good\" = \"Excellent/Good Faculty Quality\",\n                           \"fac_qual_fair_poor\" = \"Fair/Poor Faculty Quality\")) |&gt; \n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  facet_wrap(~fac_qual, scales = \"free\") +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\nexit |&gt; \n  pivot_longer(\n    matches(\n      \"^fac_qual\"\n    ),\n    names_to = \"fac_qual\",\n    values_to = \"fac_values\"\n  ) |&gt; \n  filter(fac_qual == \"fac_qual_fair_poor\") |&gt;\n  mutate(fac_qual = recode(fac_qual, \"fac_qual_ex_good\" = \"Excellent/Good Faculty Quality\",\n                           \"fac_qual_fair_poor\" = \"Fair/Poor Faculty Quality\")) |&gt; \n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  facet_wrap(~fac_qual, scales = \"free\") +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nJust in case anyone else is interested in this data, I also created a quick function to see how this visual looked like for other variables in the dataset. For instance, I’ll look at a couple of different variables.\n\n\nCode\nprogram_experience_agree &lt;- function(name){\n  exit |&gt; \n    pivot_longer(\n      matches(\n          {{name}}\n      )\n    ) |&gt;\n    # filter(name != paste0({{name}}, \"_ex\") &\n    #          name != paste0({{name}}, \"_strong\")) |&gt; \n  filter(str_detect(name, \"_agree\")) |&gt;\n  ggplot(aes(fct_reorder(program, value), value)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"\") +\n  coord_flip() +\n  # facet_wrap(~name, scales = \"free\") +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n}\n\nprogram_experience_disagree &lt;- function(name){\n  exit |&gt; \n    pivot_longer(\n      matches(\n          {{name}}\n      )\n    ) |&gt;\n    # filter(name != paste0({{name}}, \"_ex\") &\n    #          name != paste0({{name}}, \"_strong\")) |&gt; \n  filter(str_detect(name, \"_disagree\")) |&gt;\n  ggplot(aes(fct_reorder(program, value), value)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"\") +\n  coord_flip() +\n  # facet_wrap(~name, scales = \"free\") +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n}\n\n\nBelow are all the variables from the dataset.\n\n\n  [1] \"year\"                          \"program\"                      \n  [3] \"number_respondents\"            \"fac_qual_ex\"                  \n  [5] \"pro_qual_ex\"                   \"money_sup_ex\"                 \n  [7] \"field_dev_pace_ex\"             \"advising_qual_ex\"             \n  [9] \"smart_community_ex\"            \"prof_dev_ex\"                  \n [11] \"equipment_ex\"                  \"grad_involve_ex\"              \n [13] \"research_opp_ex\"               \"grad_fair_assess_ex\"          \n [15] \"promote_inclu_ex\"              \"grant_train_ex\"               \n [17] \"teach_prep_ex\"                 \"grad_clear_assess_ex\"         \n [19] \"inter_sup_ex\"                  \"prof_ethic_train_ex\"          \n [21] \"fac_qual_ex_good\"              \"pro_qual_ex_good\"             \n [23] \"money_sup_ex_good\"             \"field_dev_pace_ex_good\"       \n [25] \"advising_qual_ex_good\"         \"smart_community_ex_good\"      \n [27] \"prof_dev_ex_good\"              \"equipment_ex_good\"            \n [29] \"grad_involve_ex_good\"          \"research_opp_ex_good\"         \n [31] \"grad_fair_assess_ex_good\"      \"promote_inclu_ex_good\"        \n [33] \"grant_train_ex_good\"           \"teach_prep_ex_good\"           \n [35] \"grad_clear_assess_ex_good\"     \"inter_sup_ex_good\"            \n [37] \"prof_ethic_train_ex_good\"      \"fac_qual_fair_poor\"           \n [39] \"pro_qual_fair_poor\"            \"money_sup_fair_poor\"          \n [41] \"field_dev_pace_fair_poor\"      \"advising_qual_fair_poor\"      \n [43] \"smart_community_fair_poor\"     \"prof_dev_fair_poor\"           \n [45] \"equipment_fair_poor\"           \"grad_involve_fair_poor\"       \n [47] \"research_opp_fair_poor\"        \"grad_fair_assess_fair_poor\"   \n [49] \"promote_inclu_fair_poor\"       \"grant_train_fair_poor\"        \n [51] \"teach_prep_fair_poor\"          \"grad_clear_assess_fair_poor\"  \n [53] \"inter_sup_fair_poor\"           \"prof_ethic_train_fair_poor\"   \n [55] \"encourage_agree\"               \"idea_resp_agree\"              \n [57] \"construct_feed_agree\"          \"time_feed_agree\"              \n [59] \"avail_agree\"                   \"career_sup_agree\"             \n [61] \"stu_equit_agree\"               \"ethic_emp_agree\"              \n [63] \"help_secure_fund_agree\"        \"help_prof_dev_agree\"          \n [65] \"publish_help_agree\"            \"encourage_intel_diff_agree\"   \n [67] \"comfort_talk_issue_agree\"      \"encourage_disagree\"           \n [69] \"idea_resp_disagree\"            \"construct_feed_disagree\"      \n [71] \"time_feed_disagree\"            \"avail_disagree\"               \n [73] \"career_sup_disagree\"           \"stu_equit_disagree\"           \n [75] \"ethic_emp_disagree\"            \"help_secure_fund_disagree\"    \n [77] \"help_prof_dev_disagree\"        \"publish_help_disagree\"        \n [79] \"encourage_intel_diff_disagree\" \"comfort_talk_issue_disagree\"  \n [81] \"collegial_strong\"              \"encouraging_strong\"           \n [83] \"supportive_strong\"             \"intel_open_strong\"            \n [85] \"inter_open_strong\"             \"inclu_stu_color_strong\"       \n [87] \"inclu_gender_strong\"           \"inclu_intern_stu_strong\"      \n [89] \"inclu_stu_disab_strong\"        \"inclu_first_gen_strong\"       \n [91] \"inclu_stu_sex_orient_strong\"   \"collegial_agree\"              \n [93] \"encouraging_agree\"             \"supportive_agree\"             \n [95] \"intel_open_agree\"              \"inter_open_agree\"             \n [97] \"inclu_stu_color_agree\"         \"inclu_gender_agree\"           \n [99] \"inclu_intern_stu_agree\"        \"inclu_stu_disab_agree\"        \n[101] \"inclu_first_gen_agree\"         \"inclu_stu_sex_orient_agree\"   \n[103] \"collegial_disagree\"            \"encouraging_disagree\"         \n[105] \"supportive_disagree\"           \"intel_open_disagree\"          \n[107] \"inter_open_disagree\"           \"inclu_stu_color_disagree\"     \n[109] \"inclu_gender_disagree\"         \"inclu_intern_stu_disagree\"    \n[111] \"inclu_stu_disab_disagree\"      \"inclu_first_gen_disagree\"     \n[113] \"inclu_stu_sex_orient_disagree\"\n\n\n\n\nCode\n# student equitable treatment\nprogram_experience_agree(name = \"stu_equit\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"stu_equit\")\n\n\n\n\n\n\n\n\n\nCode\n# inclusive of students of color\nprogram_experience_agree(name = \"inclu_stu_color\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"inclu_stu_color\")\n\n\n\n\n\n\n\n\n\nCode\n# inclusive of gender\nprogram_experience_agree(name = \"inclu_gender\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"inclu_gender\")\n\n\n\n\n\n\n\n\n\nCode\n# inclusive of international students\nprogram_experience_agree(name = \"inclu_intern_stu\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"inclu_intern_stu\")\n\n\n\n\n\n\n\n\n\nCode\n# inclusive of students with disabilities\nprogram_experience_agree(name = \"inclu_stu_disab\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"inclu_stu_disab\")\n\n\n\n\n\n\n\n\n\nCode\n# inclusive of first generation students\nprogram_experience_agree(name = \"inclu_first_gen\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"inclu_first_gen\")\n\n\n\n\n\n\n\n\n\nCode\n# inclusive of students of all sexual orientations\nprogram_experience_agree(name = \"inclu_stu_sex_orient\")\n\n\n\n\n\n\n\n\n\nCode\nprogram_experience_disagree(name = \"inclu_stu_sex_orient\")\n\n\n\n\n\n\n\n\n\nLastly, I decided to look into the difference between the variables I’m most interested in. First, I wanted to look at how graduate students perceive inclusiveness of students of color within their departments. Another variable I was interested in was inclusiveness of first-generation graduate students. Thanks to the plotly package I was able to include some interactive components to the visuals. Specifically zooming in to specific departments give a better idea of the difference between agreeing and disagreeing on these topics. With plotly, you can also click on an option in the legend to only see those values. I also removed the strongly agree option since the agree applied to students that strongly agreed or agreed with the statement.\n\n\nCode\nlibrary(plotly)\n\nstu_color &lt;- exit |&gt; \n  pivot_longer(\n    matches(\n      \"^inclu_stu_color\"\n    ),\n    names_to = \"stu_color\",\n    values_to = \"stu_color_values\"\n  ) |&gt;\n  filter(stu_color != \"inclu_stu_color_strong\") |&gt; \n  mutate(stu_color = recode(stu_color, \"inclu_stu_color_agree\" = \"Agree with Inclusive Environment for Students of Color\",\n                           \"inclu_stu_color_disagree\" = \"Disagree with Inclusive Environment for Students of Color\")) |&gt; \n  ggplot(aes(fct_reorder(program, stu_color_values), stu_color_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(stu_color)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\"))\n\nstu_plot &lt;- ggplotly(stu_color)\n  # layout(legend = list(orientation = \"h\",\n                       # xanchor = \"center\",\n                       # x = 0,\n                       # y = -60)) \nstu_plot\n\n\n\n\n\n\nCode\nfirstgen &lt;- exit |&gt; \n  pivot_longer(\n    matches(\n      \"^inclu_first_gen\"\n    ),\n    names_to = \"first_gen\",\n    values_to = \"first_gen_values\"\n  ) |&gt;\n  filter(first_gen != \"inclu_first_gen_strong\") |&gt; \n  mutate(first_gen = recode(first_gen, \"inclu_first_gen_agree\" = \"Agree with Inclusive Environment for First Gen\",\n                           \"inclu_first_gen_disagree\" = \"Disagree with Inclusive Environment for First Gen\")) |&gt; \n  ggplot(aes(fct_reorder(program, first_gen_values), first_gen_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(first_gen)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\"))\n\nfirst_plot &lt;- ggplotly(firstgen) \n  # layout(legend = list(orientation = \"h\",\n  #                      xanchor = \"center\",\n  #                      x = 0,\n  #                      y = -60)) \nfirst_plot"
  },
  {
    "objectID": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html",
    "href": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html",
    "title": "Tidy Tuesday Coffee Ratings",
    "section": "",
    "text": "With coffee being a hobby of mine, I was scrolling through past Tidy Tuesdays and found one on coffee ratings. Originally I thought looking at predictions of total cup points, but I assumed with all the coffee tasting characteristics that it wouldn’t really tell me anything. Instead, I decided to look into the processing method, as there are different taste characteristics between washed and other processing methods.\n\n\nCode\ncoffee &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv') %&gt;% \n  mutate(species = as.factor(species),\n         process = recode(processing_method, \"Washed / Wet\" = \"washed\",\n                          \"Semi-washed / Semi-pulped\" = \"not_washed\",\n                          \"Pulped natural / honey\" = \"not_washed\",\n                          \"Other\" = \"not_washed\",\n                          \"Natural / Dry\" = \"not_washed\",\n                          \"NA\" = NA_character_),\n         process = as.factor(process),\n         process = relevel(process, ref = \"washed\"),\n         country_of_origin = as.factor(country_of_origin)) %&gt;% \n  drop_na(process) %&gt;% \n  filter(country_of_origin != \"Cote d?Ivoire\")\n\n\nRows: 1339 Columns: 43\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter looking at the distributions of procssing methods, I also decided to make the processing method binary with washed and not washed. This worked out better for the prediction models. There are also some descriptives of each variable.\n\n\nCode\ncoffee %&gt;% \n  ggplot(aes(processing_method)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\ncoffee %&gt;% \n  ggplot(aes(process)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\npsych::describe(coffee, na.rm = TRUE)[c(\"n\", \"mean\", \"sd\", \"min\", \"max\", \"skew\", \"kurtosis\")]\n\n\n                          n    mean      sd   min       max  skew kurtosis\ntotal_cup_points       1168   82.06    2.71 59.83     90.58 -1.95     9.26\nspecies*               1168    1.01    0.09  1.00      2.00 10.65   111.61\nowner*                 1161  134.40   77.04  1.00    287.00  0.11    -0.99\ncountry_of_origin*     1168   14.79   10.08  1.00     36.00  0.31    -1.12\nfarm_name*              883  264.34  153.73  1.00    524.00 -0.01    -1.28\nlot_number*             239   93.18   59.86  1.00    202.00  0.18    -1.27\nmill*                   931  188.25  123.79  1.00    419.00  0.23    -1.29\nico_number*            1057  360.89  242.06  1.00    753.00  0.08    -1.27\ncompany*               1077  142.68   76.32  1.00    266.00 -0.10    -1.24\naltitude*              1014  154.49   98.46  1.00    351.00  0.32    -1.04\nregion*                1137  167.49   87.31  1.00    325.00 -0.08    -1.10\nproducer*               995  307.70  175.31  1.00    624.00 -0.02    -1.11\nnumber_of_bags         1168  153.80  130.08  1.00   1062.00  0.37     0.50\nbag_weight*            1168   23.50   16.67  1.00     45.00 -0.23    -1.71\nin_country_partner*    1168    9.68    7.04  1.00     25.00  0.41    -1.31\nharvest_year*          1161    5.82    2.95  1.00     14.00  0.76    -0.46\ngrading_date*          1168  242.39  144.52  1.00    495.00  0.11    -1.21\nowner_1*               1161  136.29   77.93  1.00    290.00  0.10    -0.99\nvariety*               1089   12.61    9.77  1.00     29.00  0.63    -1.24\nprocessing_method*     1168    3.98    1.67  1.00      5.00 -1.13    -0.62\naroma                  1168    7.56    0.31  5.08      8.75 -0.55     4.46\nflavor                 1168    7.51    0.34  6.08      8.83 -0.34     1.73\naftertaste             1168    7.39    0.34  6.17      8.67 -0.45     1.36\nacidity                1168    7.53    0.31  5.25      8.75 -0.30     3.31\nbody                   1168    7.52    0.28  6.33      8.50 -0.10     0.89\nbalance                1168    7.51    0.34  6.08      8.58 -0.10     1.17\nuniformity             1168    9.84    0.50  6.00     10.00 -4.21    20.83\nclean_cup              1168    9.84    0.75  0.00     10.00 -6.98    62.29\nsweetness              1168    9.89    0.52  1.33     10.00 -7.53    80.78\ncupper_points          1168    7.48    0.40  5.17      8.75 -0.64     2.79\nmoisture               1168    0.09    0.05  0.00      0.17 -1.41     0.35\ncategory_one_defects   1168    0.51    2.70  0.00     63.00 14.43   279.42\nquakers                1167    0.17    0.82  0.00     11.00  6.87    57.30\ncolor*                 1070    2.80    0.64  1.00      4.00 -1.51     2.57\ncategory_two_defects   1168    3.79    5.54  0.00     55.00  3.54    18.53\nexpiration*            1168  241.61  144.11  1.00    494.00  0.12    -1.21\ncertification_body*    1168    9.38    6.65  1.00     24.00  0.37    -1.31\ncertification_address* 1168   16.58    7.33  1.00     29.00 -0.19    -1.06\ncertification_contact* 1168    9.39    7.26  1.00     26.00  0.36    -0.96\nunit_of_measurement*   1168    1.87    0.34  1.00      2.00 -2.21     2.87\naltitude_low_meters    1012 1796.86 9073.21  1.00 190164.00 19.36   384.12\naltitude_high_meters   1012 1834.27 9071.86  1.00 190164.00 19.36   384.03\naltitude_mean_meters   1012 1815.56 9072.31  1.00 190164.00 19.36   384.12\nprocess*               1168    1.30    0.46  1.00      2.00  0.86    -1.27\n\n\nNow, its time to split the data into training and testing data. I also included the function of strata to stratify sampling based on process.\n\n\nCode\nset.seed(05132021)\n\ncoffee_split &lt;- initial_split(coffee, strata = \"process\")\n\ncoffee_train &lt;- training(coffee_split)\ncoffee_test &lt;- testing(coffee_split)\n\n\nI also did some cross validation for the training dataset and used the metrics I was most interested in.\n\n\nCode\nset.seed(05132021)\n\ncoffee_fold &lt;- vfold_cv(coffee_train, strata = \"process\", v = 10)\n\nmetric_measure &lt;- metric_set(accuracy, mn_log_loss, roc_auc)\n\n\nFrom the beginning I was interested in the tasting characteristics and how they would predict whether the green coffee was washed or not washed. I also included the total cup points because I wanted to see the importance of that predictor on the processing method. The only feature engineering I did was to remove any zero variance in the predictors of the model.\n\n\nCode\nset.seed(05132021)\n\nchar_recipe &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points,\n                      data = coffee_train) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\nchar_recipe %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;%\n  head()\n\n\n# A tibble: 6 × 11\n  aroma flavor aftertaste acidity  body balance uniformity clean_cup sweetness\n  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  8.17   8.58       8.42    8.42  8.5     8.25         10        10        10\n2  8.08   8.58       8.5     8.5   7.67    8.42         10        10        10\n3  8.17   8.17       8       8.17  8.08    8.33         10        10        10\n4  8.42   8.17       7.92    8.17  8.33    8            10        10        10\n5  8.5    8.5        8       8     8       8            10        10        10\n6  8      8          8       8.25  8       8.17         10        10        10\n# ℹ 2 more variables: total_cup_points &lt;dbl&gt;, process &lt;fct&gt;\n\n\n\n\nThe first model I wanted to test with the current recipe was logistic regression. The accuracy and roc auc were alright for a starting model.\n\n\nWarning: package 'glmnet' was built under R version 4.3.2\n\n\n\n\nCode\ncollect_metrics(lr_fit)\n\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.698    10 0.00342 Preprocessor1_Model1\n2 mn_log_loss binary     0.589    10 0.00775 Preprocessor1_Model1\n3 roc_auc     binary     0.648    10 0.0188  Preprocessor1_Model1\n\n\n\n\n\nNow for the first penalized regression. The lasso regression did not improve in either metric. Let’s try the next penalized regression.\n\n\nCode\ncollect_metrics(lasso_fit)\n\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.702    10 0.00397 Preprocessor1_Model1\n2 mn_log_loss binary     0.592    10 0.00850 Preprocessor1_Model1\n3 roc_auc     binary     0.645    10 0.0191  Preprocessor1_Model1\n\n\n\n\n\nThe ridge regression was shown to not be a good fitting model. So I tested an additional penalized regression while tuning hyper-parameters.\n\n\nCode\ncollect_metrics(ridge_fit)\n\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.697    10 0.00133 Preprocessor1_Model1\n2 mn_log_loss binary     0.603    10 0.00212 Preprocessor1_Model1\n3 roc_auc     binary     0.612    10 0.0186  Preprocessor1_Model1\n\n\n\n\n\nThe elastic net regression had slightly better accuracy than the non-penalized logistic regression but the ROC AUC was exactly the same. While the elastic net regression did not take long computationally due to the small amount of data, this model would not be chosen over the logistic regression.\n\n\nCode\ncollect_metrics(elastic_fit)\n\n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n# ℹ 290 more rows\n\n\nCode\nshow_best(elastic_fit, metric = \"accuracy\", n = 5)\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric  .estimator  mean     n std_err .config         \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1 0.0774          0.111 accuracy binary     0.703    10 0.00434 Preprocessor1_M…\n2 0.0000000001    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n3 0.00000000129   0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n4 0.0000000167    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n5 0.000000215     0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n\n\nCode\nshow_best(elastic_fit, metric = \"roc_auc\", n = 5)\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n2 0.00000000129       0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n3 0.0000000167        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n4 0.000000215         0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n5 0.00000278          0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n\n\nCode\nselect_best(elastic_fit, metric = \"accuracy\")\n\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1  0.0774   0.111 Preprocessor1_Model019\n\n\nCode\nselect_best(elastic_fit, metric = \"roc_auc\")\n\n\n# A tibble: 1 × 3\n       penalty mixture .config               \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001       0 Preprocessor1_Model001\n\n\n\n\n\nEven though the elastic net regression was only slightly better, I decided to update the workflow using that model. This time I decided to update the recipe by including additional predictors like if there were any defects in the green coffee beans, the species of the coffee (e.g., Robusta and Arabica), and the country of origin. I also included additional steps in my recipe by transforming the category predictors and working with the factor predictors, like species, and country of origin. The inclusion of additional steps and the predictors created a better fitting model with the elastic net regression.\n\n\nCode\nset.seed(05132021)\n\nbal_rec &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points + category_one_defects + category_two_defects + species +\n                        country_of_origin,\n                      data = coffee_train) %&gt;% \n  step_BoxCox(category_two_defects, category_one_defects) %&gt;% \n  step_novel(species, country_of_origin) %&gt;% \n  step_other(species, country_of_origin, threshold = .01) %&gt;%\n  step_unknown(species, country_of_origin) %&gt;% \n  step_dummy(species, country_of_origin) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\n\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n\n\n\nCode\ncollect_metrics(elastic_bal_fit) \n\n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n# ℹ 290 more rows\n\n\nCode\nshow_best(elastic_bal_fit, metric = \"accuracy\", n = 5)\n\n\n# A tibble: 5 × 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000464   0.111 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n2 0.000464   0.222 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n3 0.00599    0.111 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n4 0.00599    0.222 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n5 0.00599    0.333 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n\n\nCode\nshow_best(elastic_bal_fit, metric = \"mn_log_loss\", n = 5)\n\n\n# A tibble: 5 × 8\n   penalty mixture .metric     .estimator  mean     n std_err .config           \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;             \n1 0.000464   1     mn_log_loss binary     0.420    10  0.0179 Preprocessor1_Mod…\n2 0.000464   0.889 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n3 0.000464   0.778 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n4 0.000464   0.667 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n5 0.000464   0.556 mn_log_loss binary     0.420    10  0.0177 Preprocessor1_Mod…\n\n\nCode\nshow_best(elastic_bal_fit, metric = \"roc_auc\", n = 5)\n\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599    0.778 roc_auc binary     0.843    10  0.0150 Preprocessor1_Model078\n2 0.000464   0.667 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model067\n3 0.00599    0.889 roc_auc binary     0.843    10  0.0148 Preprocessor1_Model088\n4 0.000464   0.444 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model047\n5 0.000464   0.556 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model057\n\n\nCode\nselect_best(elastic_bal_fit, metric = \"accuracy\")\n\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464   0.111 Preprocessor1_Model017\n\n\nCode\nselect_best(elastic_bal_fit, metric = \"mn_log_loss\")\n\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464       1 Preprocessor1_Model097\n\n\nCode\nselect_best(elastic_bal_fit, metric = \"roc_auc\")\n\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599   0.778 Preprocessor1_Model078\n\n\nNow using the testing dataset, we can see how well the final model fit the testing data. While not the best at predicting washed green coffee beans, this was a good test to show that the penalized regressions are not always the best fitting models compared to regular logistic regression. In the end, it seemed like the recipe was the most important component to predicting washed green coffee beans.\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\n\nCode\nfinal_results %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.823 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html#predicting-process-of-green-coffee-beans",
    "href": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html#predicting-process-of-green-coffee-beans",
    "title": "Tidy Tuesday Coffee Ratings",
    "section": "",
    "text": "With coffee being a hobby of mine, I was scrolling through past Tidy Tuesdays and found one on coffee ratings. Originally I thought looking at predictions of total cup points, but I assumed with all the coffee tasting characteristics that it wouldn’t really tell me anything. Instead, I decided to look into the processing method, as there are different taste characteristics between washed and other processing methods.\n\n\nCode\ncoffee &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv') %&gt;% \n  mutate(species = as.factor(species),\n         process = recode(processing_method, \"Washed / Wet\" = \"washed\",\n                          \"Semi-washed / Semi-pulped\" = \"not_washed\",\n                          \"Pulped natural / honey\" = \"not_washed\",\n                          \"Other\" = \"not_washed\",\n                          \"Natural / Dry\" = \"not_washed\",\n                          \"NA\" = NA_character_),\n         process = as.factor(process),\n         process = relevel(process, ref = \"washed\"),\n         country_of_origin = as.factor(country_of_origin)) %&gt;% \n  drop_na(process) %&gt;% \n  filter(country_of_origin != \"Cote d?Ivoire\")\n\n\nRows: 1339 Columns: 43\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter looking at the distributions of procssing methods, I also decided to make the processing method binary with washed and not washed. This worked out better for the prediction models. There are also some descriptives of each variable.\n\n\nCode\ncoffee %&gt;% \n  ggplot(aes(processing_method)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\ncoffee %&gt;% \n  ggplot(aes(process)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\npsych::describe(coffee, na.rm = TRUE)[c(\"n\", \"mean\", \"sd\", \"min\", \"max\", \"skew\", \"kurtosis\")]\n\n\n                          n    mean      sd   min       max  skew kurtosis\ntotal_cup_points       1168   82.06    2.71 59.83     90.58 -1.95     9.26\nspecies*               1168    1.01    0.09  1.00      2.00 10.65   111.61\nowner*                 1161  134.40   77.04  1.00    287.00  0.11    -0.99\ncountry_of_origin*     1168   14.79   10.08  1.00     36.00  0.31    -1.12\nfarm_name*              883  264.34  153.73  1.00    524.00 -0.01    -1.28\nlot_number*             239   93.18   59.86  1.00    202.00  0.18    -1.27\nmill*                   931  188.25  123.79  1.00    419.00  0.23    -1.29\nico_number*            1057  360.89  242.06  1.00    753.00  0.08    -1.27\ncompany*               1077  142.68   76.32  1.00    266.00 -0.10    -1.24\naltitude*              1014  154.49   98.46  1.00    351.00  0.32    -1.04\nregion*                1137  167.49   87.31  1.00    325.00 -0.08    -1.10\nproducer*               995  307.70  175.31  1.00    624.00 -0.02    -1.11\nnumber_of_bags         1168  153.80  130.08  1.00   1062.00  0.37     0.50\nbag_weight*            1168   23.50   16.67  1.00     45.00 -0.23    -1.71\nin_country_partner*    1168    9.68    7.04  1.00     25.00  0.41    -1.31\nharvest_year*          1161    5.82    2.95  1.00     14.00  0.76    -0.46\ngrading_date*          1168  242.39  144.52  1.00    495.00  0.11    -1.21\nowner_1*               1161  136.29   77.93  1.00    290.00  0.10    -0.99\nvariety*               1089   12.61    9.77  1.00     29.00  0.63    -1.24\nprocessing_method*     1168    3.98    1.67  1.00      5.00 -1.13    -0.62\naroma                  1168    7.56    0.31  5.08      8.75 -0.55     4.46\nflavor                 1168    7.51    0.34  6.08      8.83 -0.34     1.73\naftertaste             1168    7.39    0.34  6.17      8.67 -0.45     1.36\nacidity                1168    7.53    0.31  5.25      8.75 -0.30     3.31\nbody                   1168    7.52    0.28  6.33      8.50 -0.10     0.89\nbalance                1168    7.51    0.34  6.08      8.58 -0.10     1.17\nuniformity             1168    9.84    0.50  6.00     10.00 -4.21    20.83\nclean_cup              1168    9.84    0.75  0.00     10.00 -6.98    62.29\nsweetness              1168    9.89    0.52  1.33     10.00 -7.53    80.78\ncupper_points          1168    7.48    0.40  5.17      8.75 -0.64     2.79\nmoisture               1168    0.09    0.05  0.00      0.17 -1.41     0.35\ncategory_one_defects   1168    0.51    2.70  0.00     63.00 14.43   279.42\nquakers                1167    0.17    0.82  0.00     11.00  6.87    57.30\ncolor*                 1070    2.80    0.64  1.00      4.00 -1.51     2.57\ncategory_two_defects   1168    3.79    5.54  0.00     55.00  3.54    18.53\nexpiration*            1168  241.61  144.11  1.00    494.00  0.12    -1.21\ncertification_body*    1168    9.38    6.65  1.00     24.00  0.37    -1.31\ncertification_address* 1168   16.58    7.33  1.00     29.00 -0.19    -1.06\ncertification_contact* 1168    9.39    7.26  1.00     26.00  0.36    -0.96\nunit_of_measurement*   1168    1.87    0.34  1.00      2.00 -2.21     2.87\naltitude_low_meters    1012 1796.86 9073.21  1.00 190164.00 19.36   384.12\naltitude_high_meters   1012 1834.27 9071.86  1.00 190164.00 19.36   384.03\naltitude_mean_meters   1012 1815.56 9072.31  1.00 190164.00 19.36   384.12\nprocess*               1168    1.30    0.46  1.00      2.00  0.86    -1.27\n\n\nNow, its time to split the data into training and testing data. I also included the function of strata to stratify sampling based on process.\n\n\nCode\nset.seed(05132021)\n\ncoffee_split &lt;- initial_split(coffee, strata = \"process\")\n\ncoffee_train &lt;- training(coffee_split)\ncoffee_test &lt;- testing(coffee_split)\n\n\nI also did some cross validation for the training dataset and used the metrics I was most interested in.\n\n\nCode\nset.seed(05132021)\n\ncoffee_fold &lt;- vfold_cv(coffee_train, strata = \"process\", v = 10)\n\nmetric_measure &lt;- metric_set(accuracy, mn_log_loss, roc_auc)\n\n\nFrom the beginning I was interested in the tasting characteristics and how they would predict whether the green coffee was washed or not washed. I also included the total cup points because I wanted to see the importance of that predictor on the processing method. The only feature engineering I did was to remove any zero variance in the predictors of the model.\n\n\nCode\nset.seed(05132021)\n\nchar_recipe &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points,\n                      data = coffee_train) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\nchar_recipe %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL) %&gt;%\n  head()\n\n\n# A tibble: 6 × 11\n  aroma flavor aftertaste acidity  body balance uniformity clean_cup sweetness\n  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  8.17   8.58       8.42    8.42  8.5     8.25         10        10        10\n2  8.08   8.58       8.5     8.5   7.67    8.42         10        10        10\n3  8.17   8.17       8       8.17  8.08    8.33         10        10        10\n4  8.42   8.17       7.92    8.17  8.33    8            10        10        10\n5  8.5    8.5        8       8     8       8            10        10        10\n6  8      8          8       8.25  8       8.17         10        10        10\n# ℹ 2 more variables: total_cup_points &lt;dbl&gt;, process &lt;fct&gt;\n\n\n\n\nThe first model I wanted to test with the current recipe was logistic regression. The accuracy and roc auc were alright for a starting model.\n\n\nWarning: package 'glmnet' was built under R version 4.3.2\n\n\n\n\nCode\ncollect_metrics(lr_fit)\n\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.698    10 0.00342 Preprocessor1_Model1\n2 mn_log_loss binary     0.589    10 0.00775 Preprocessor1_Model1\n3 roc_auc     binary     0.648    10 0.0188  Preprocessor1_Model1\n\n\n\n\n\nNow for the first penalized regression. The lasso regression did not improve in either metric. Let’s try the next penalized regression.\n\n\nCode\ncollect_metrics(lasso_fit)\n\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.702    10 0.00397 Preprocessor1_Model1\n2 mn_log_loss binary     0.592    10 0.00850 Preprocessor1_Model1\n3 roc_auc     binary     0.645    10 0.0191  Preprocessor1_Model1\n\n\n\n\n\nThe ridge regression was shown to not be a good fitting model. So I tested an additional penalized regression while tuning hyper-parameters.\n\n\nCode\ncollect_metrics(ridge_fit)\n\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.697    10 0.00133 Preprocessor1_Model1\n2 mn_log_loss binary     0.603    10 0.00212 Preprocessor1_Model1\n3 roc_auc     binary     0.612    10 0.0186  Preprocessor1_Model1\n\n\n\n\n\nThe elastic net regression had slightly better accuracy than the non-penalized logistic regression but the ROC AUC was exactly the same. While the elastic net regression did not take long computationally due to the small amount of data, this model would not be chosen over the logistic regression.\n\n\nCode\ncollect_metrics(elastic_fit)\n\n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.648    10 0.0188  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.698    10 0.00342 Preprocesso…\n# ℹ 290 more rows\n\n\nCode\nshow_best(elastic_fit, metric = \"accuracy\", n = 5)\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric  .estimator  mean     n std_err .config         \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1 0.0774          0.111 accuracy binary     0.703    10 0.00434 Preprocessor1_M…\n2 0.0000000001    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n3 0.00000000129   0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n4 0.0000000167    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n5 0.000000215     0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M…\n\n\nCode\nshow_best(elastic_fit, metric = \"roc_auc\", n = 5)\n\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.0000000001        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n2 0.00000000129       0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n3 0.0000000167        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n4 0.000000215         0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n5 0.00000278          0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo…\n\n\nCode\nselect_best(elastic_fit, metric = \"accuracy\")\n\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1  0.0774   0.111 Preprocessor1_Model019\n\n\nCode\nselect_best(elastic_fit, metric = \"roc_auc\")\n\n\n# A tibble: 1 × 3\n       penalty mixture .config               \n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001       0 Preprocessor1_Model001\n\n\n\n\n\nEven though the elastic net regression was only slightly better, I decided to update the workflow using that model. This time I decided to update the recipe by including additional predictors like if there were any defects in the green coffee beans, the species of the coffee (e.g., Robusta and Arabica), and the country of origin. I also included additional steps in my recipe by transforming the category predictors and working with the factor predictors, like species, and country of origin. The inclusion of additional steps and the predictors created a better fitting model with the elastic net regression.\n\n\nCode\nset.seed(05132021)\n\nbal_rec &lt;- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points + category_one_defects + category_two_defects + species +\n                        country_of_origin,\n                      data = coffee_train) %&gt;% \n  step_BoxCox(category_two_defects, category_one_defects) %&gt;% \n  step_novel(species, country_of_origin) %&gt;% \n  step_other(species, country_of_origin, threshold = .01) %&gt;%\n  step_unknown(species, country_of_origin) %&gt;% \n  step_dummy(species, country_of_origin) %&gt;% \n  step_zv(all_predictors(), -all_outcomes())\n\n\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n\n\n\nCode\ncollect_metrics(elastic_bal_fit) \n\n\n# A tibble: 300 × 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 0.0000000001        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 2 0.0000000001        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 3 0.0000000001        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 4 0.00000000129       0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 5 0.00000000129       0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 6 0.00000000129       0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n 7 0.0000000167        0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n 8 0.0000000167        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso…\n 9 0.0000000167        0 roc_auc     binary     0.840    10 0.0163  Preprocesso…\n10 0.000000215         0 accuracy    binary     0.839    10 0.00923 Preprocesso…\n# ℹ 290 more rows\n\n\nCode\nshow_best(elastic_bal_fit, metric = \"accuracy\", n = 5)\n\n\n# A tibble: 5 × 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000464   0.111 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n2 0.000464   0.222 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0…\n3 0.00599    0.111 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n4 0.00599    0.222 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n5 0.00599    0.333 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0…\n\n\nCode\nshow_best(elastic_bal_fit, metric = \"mn_log_loss\", n = 5)\n\n\n# A tibble: 5 × 8\n   penalty mixture .metric     .estimator  mean     n std_err .config           \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;             \n1 0.000464   1     mn_log_loss binary     0.420    10  0.0179 Preprocessor1_Mod…\n2 0.000464   0.889 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n3 0.000464   0.778 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n4 0.000464   0.667 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod…\n5 0.000464   0.556 mn_log_loss binary     0.420    10  0.0177 Preprocessor1_Mod…\n\n\nCode\nshow_best(elastic_bal_fit, metric = \"roc_auc\", n = 5)\n\n\n# A tibble: 5 × 8\n   penalty mixture .metric .estimator  mean     n std_err .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599    0.778 roc_auc binary     0.843    10  0.0150 Preprocessor1_Model078\n2 0.000464   0.667 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model067\n3 0.00599    0.889 roc_auc binary     0.843    10  0.0148 Preprocessor1_Model088\n4 0.000464   0.444 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model047\n5 0.000464   0.556 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model057\n\n\nCode\nselect_best(elastic_bal_fit, metric = \"accuracy\")\n\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464   0.111 Preprocessor1_Model017\n\n\nCode\nselect_best(elastic_bal_fit, metric = \"mn_log_loss\")\n\n\n# A tibble: 1 × 3\n   penalty mixture .config               \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000464       1 Preprocessor1_Model097\n\n\nCode\nselect_best(elastic_bal_fit, metric = \"roc_auc\")\n\n\n# A tibble: 1 × 3\n  penalty mixture .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00599   0.778 Preprocessor1_Model078\n\n\nNow using the testing dataset, we can see how well the final model fit the testing data. While not the best at predicting washed green coffee beans, this was a good test to show that the penalized regressions are not always the best fitting models compared to regular logistic regression. In the end, it seemed like the recipe was the most important component to predicting washed green coffee beans.\n\n\n→ A | warning: Non-positive values in selected variable., No Box-Cox transformation could be estimated for: `category_two_defects`, `category_one_defects`\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\n\nCode\nfinal_results %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.823 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2024-11-08-bayes-net-loan-approval/index.html",
    "href": "posts/2024-11-08-bayes-net-loan-approval/index.html",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "",
    "text": "For this tutorial, I wanted to create a more interesting and maybe more realistic use of a Bayesian Network (bayes net). That is why I am walking through how to use a bayes net in a machine learning application while also seeing what the likelihood of getting a loan approved would be based off some Kaggle data (data can be found here). First, I’ll load in the packages I’ll need and the data."
  },
  {
    "objectID": "posts/2024-11-08-bayes-net-loan-approval/index.html#structure-learning",
    "href": "posts/2024-11-08-bayes-net-loan-approval/index.html#structure-learning",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "Structure Learning",
    "text": "Structure Learning\nWhile I’m not going to go into a lot of detail about it, structure learning is the process of learning the structure of the DAG from the data. For this example, I’ll be using different algorithms for structure learning to see which algorithm has the best network score, including the log likelihood and the Bayesian Dirichlet Equivalent (BDE) score. There are two arguments for each of the learning algorithms, a blacklist and a whitelist, which are edges (relationships) between nodes (variables) that we either don’t want the algorithm to make or want to make sure are included in the model respectively. Bnlearn has links to articles of interest for all of the structure learning algorithms here if interested about how the algorithms work.\nI only included a blacklist and made sure that there were no edges from the outcome of interest (loan_status) as well as no edges that end at the demographic nodes of gender, age, and education.\n\n\nCode\nbl &lt;- matrix(\n  c(\n    \"loan_status\", \"person_gender\",\n    \"loan_status\", \"person_age_brack\",\n    \"loan_status\", \"credit_score_brack\",\n    \"loan_status\", \"person_education\",\n    \"loan_status\", \"previous_loan_defaults_on_file\",\n    \"loan_status\", \"loan_percent_income_brack\",\n    \"loan_status\", \"person_income_brack\",\n    \"loan_status\", \"loan_amnt_brack\",\n    \"loan_status\", \"person_home_ownership\",\n    \"loan_status\", \"loan_int_rate_brack\",\n    \"loan_status\", \"loan_intent\",\n\n    \"person_age_brack\", \"person_gender\",\n    \"credit_score_brack\", \"person_gender\",\n    \"person_education\", \"person_gender\",\n    \"previous_loan_defaults_on_file\", \"person_gender\",\n    \"loan_percent_income_brack\", \"person_gender\",\n    \"person_income_brack\", \"person_gender\",\n    \"loan_amnt_brack\", \"person_gender\",\n    \"person_home_ownership\", \"person_gender\",\n    \"loan_int_rate_brack\", \"person_gender\",\n    \"loan_intent\", \"person_gender\",\n    \"person_gender\", \"person_age_brack\",\n    \"credit_score_brack\", \"person_age_brack\",\n    \"person_education\", \"person_age_brack\",\n    \"previous_loan_defaults_on_file\", \"person_age_brack\",\n    \"loan_percent_income_brack\", \"person_age_brack\",\n    \"person_income_brack\", \"person_age_brack\",\n    \"loan_amnt_brack\", \"person_age_brack\",\n    \"person_home_ownership\", \"person_age_brack\",\n    \"loan_int_rate_brack\", \"person_age_brack\",\n    \"loan_intent\", \"person_age_brack\",\n    \"person_age_brack\", \"person_education\",\n    \"person_gender\", \"person_education\",\n    \"credit_score_brack\", \"person_education\",\n    \"previous_loan_defaults_on_file\", \"person_education\",\n    \"loan_percent_income_brack\", \"person_education\",\n    \"person_income_brack\", \"person_education\",\n    \"loan_amnt_brack\", \"person_education\",\n    \"person_home_ownership\", \"person_education\",\n    \"loan_int_rate_brack\", \"person_education\",\n    \"loan_intent\", \"person_education\"\n  ),\n  ncol = 2,\n  byrow = TRUE,\n  dimnames = list(\n    NULL,\n    c(\"from\", \"to\")\n    )\n  )\n\n\n\n\nCode\nset.seed(12345)\nhc_bn &lt;- hc(\n  loan_train,\n  blacklist = bl\n  )\n\ngraphviz.plot(hc_bn)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(12345)\nmmhc_bn &lt;- mmhc(\n  loan_train,\n  blacklist = bl\n  )\n\ngraphviz.plot(mmhc_bn)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(12345)\niamb_bn &lt;- iamb(\n  loan_train,\n  blacklist = bl\n  )\n\ngraphviz.plot(iamb_bn)\n\n\n\n\n\n\n\n\n\nFrom the DAGs, apparently gender is not that important of a node in our model. We can also see that when using the Incremental Association (IAMB) algorithm that some of the edges between nodes are not directional. We will have to do some extra work by setting the arcs between the non-directional edges a certain way. This takes some domain knowledge to see what makes the most sense. This is where creating a bayes net becomes more art than science. Below are the decisions that were made to complete the DAG.\nThe DAG needs to be fully directional so that network scores can be computed.\n\n\nCode\narcs(iamb_dag) &lt;- arcs(iamb_bn)\n\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_int_rate_brack\", to = \"loan_intent\")\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_percent_income_brack\", to = \"loan_amnt_brack\")\n\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_percent_income_brack\", to = \"person_income_brack\")\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_amnt_brack\", to = \"person_income_brack\")\n\ngraphviz.plot(iamb_dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\narcs(hc_dag) &lt;- hc_bn$arcs\narcs(mmhc_dag) &lt;- mmhc_bn$arcs"
  },
  {
    "objectID": "posts/2024-11-08-bayes-net-loan-approval/index.html#domain-knowledge-dag",
    "href": "posts/2024-11-08-bayes-net-loan-approval/index.html#domain-knowledge-dag",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "Domain Knowledge DAG",
    "text": "Domain Knowledge DAG\nThe final DAG I created was something that I thought of without any structural learning. This uses demographic variables as the starting nodes and then setting arcs that made sense to me. This defined DAG will also be compared to the learned DAGs to see what model has the lowest log likelihood and BDE.\n\n\nCode\narcs &lt;- matrix(\n  c(\n    \"person_gender\", \"person_income_brack\",\n    \"person_gender\", \"person_home_ownership\",\n    \"person_gender\", \"previous_loan_defaults_on_file\",\n\n    \"person_age_brack\", \"person_income_brack\",\n    \"person_age_brack\", \"person_home_ownership\",\n    \"person_age_brack\", \"previous_loan_defaults_on_file\",\n\n    \"person_education\", \"person_income_brack\",\n    \"person_education\", \"person_home_ownership\",\n    \"person_education\", \"previous_loan_defaults_on_file\",\n\n    \"person_income_brack\", \"credit_score_brack\", \n    \"person_home_ownership\", \"credit_score_brack\", \n    \"previous_loan_defaults_on_file\", \"credit_score_brack\",\n\n    \"person_income_brack\", \"loan_percent_income_brack\",\n    \"person_home_ownership\", \"loan_percent_income_brack\",\n\n    \"person_gender\", \"loan_int_rate_brack\",\n    \"person_education\", \"loan_int_rate_brack\",\n    \"person_age_brack\", \"loan_int_rate_brack\",\n    \"person_income_brack\", \"loan_int_rate_brack\",\n    \"person_home_ownership\", \"loan_int_rate_brack\",\n    \"previous_loan_defaults_on_file\", \"loan_int_rate_brack\",\n\n    \"person_income_brack\", \"loan_amnt_brack\",\n    \"person_home_ownership\", \"loan_amnt_brack\",\n\n    \"loan_percent_income_brack\", \"loan_intent\",\n    \"loan_int_rate_brack\", \"loan_intent\",\n    \"loan_amnt_brack\", \"loan_intent\",\n    \"credit_score_brack\", \"loan_intent\",\n\n    \"loan_int_rate_brack\", \"loan_status\",\n    \"credit_score_brack\", \"loan_status\",\n    \"loan_percent_income_brack\", \"loan_status\",\n    \"loan_amnt_brack\", \"loan_status\",\n    \"previous_loan_defaults_on_file\", \"loan_status\",\n    \"loan_intent\", \"loan_status\"\n    ),\n  byrow = TRUE,\n  ncol = 2,\n  dimnames = list(NULL, c(\"from\", \"to\"))\n)\n\narcs(jp_dag) &lt;- arcs\n\ngraphviz.plot(jp_dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"loglik\")\n)\n\n\n[[1]]\n[1] -429780\n\n[[2]]\n[1] -410700.2\n\n[[3]]\n[1] -386319.3\n\n[[4]]\n[1] -389004.6\n\n[[5]]\n[1] -387806\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"aic\")\n)\n\n\n[[1]]\n[1] -429813\n\n[[2]]\n[1] -418042.2\n\n[[3]]\n[1] -386717.3\n\n[[4]]\n[1] -389278.6\n\n[[5]]\n[1] -389074\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bic\")\n)\n\n\n[[1]]\n[1] -429952\n\n[[2]]\n[1] -448967.2\n\n[[3]]\n[1] -388393.7\n\n[[4]]\n[1] -390432.7\n\n[[5]]\n[1] -394414.8\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bde\", iss = 5)\n)\n\n\n[[1]]\n[1] -429941\n\n[[2]]\n[1] -430616.4\n\n[[3]]\n[1] -387687.8\n\n[[4]]\n[1] -389857.5\n\n[[5]]\n[1] -393009.6\n\n\nAfter calculating all of the scores, it seems like the DAG created from the Hill Climb algorithm is the best fitting model, so I am going to fit that DAG for the model.\n\n\nCode\nset.seed(12345)\nhc_fit &lt;- bn.fit(hc_dag, data = loan_train, method = \"bayes\", iss = 5)\n\n\nWhile this model used the training data, I’m still interested in looking at the likelihood of being accepted for a loan and the probabilities of getting a a low interest rate based on the loan amount, home ownership, and if the person has defaulted on a previous loan.\nI printed out the DAG again to make it easier to see what nodes I am particularly interested in. I also included the str() function to see the breakdown of the table of probabilities when using bnlearn.\nLooking at the output, the table is broken down into the first index showing the levels of loan status (1 = “0”/“Decline”, 2 = “1”/“Accepted”), the second index showing the levels of the loan amount as a percentage of annual income variable, and so on following the values of the list. For the conditional probabilities printed for the model (hc_fit$loan_status$prob[2, 1:4, 1:3, 1:3, 1:2]), I am only interested in looking at the combinations of each parent node for those that are accepted for a loan.\nIt seems that not having a previous default on file leads to a higher probability of being accepted for a loan, which makes sense.\n\n\nCode\ngraphviz.plot(hc_dag)\n\n\n\n\n\n\n\n\n\nCode\nhc_fit$loan_status$prob |&gt; str()\n\n\n 'table' num [1:2, 1:4, 1:3, 1:3, 1:2] 0.908 0.092 0.888 0.112 0.858 ...\n - attr(*, \"dimnames\")=List of 5\n  ..$ loan_status                   : chr [1:2] \"0\" \"1\"\n  ..$ loan_percent_income_brack     : chr [1:4] \"0-.09\" \".1 - .19\" \".2 - .29\" \".3+\"\n  ..$ loan_int_rate_brack           : chr [1:3] \"&lt;10\" \"10 - 15\" \"15+\"\n  ..$ person_home_ownership         : chr [1:3] \"MORTGAGE\" \"OWN\" \"RENT\"\n  ..$ previous_loan_defaults_on_file: chr [1:2] \"No\" \"Yes\"\n\n\nCode\n# loan status - yes\nhc_fit$loan_status$prob[2, 1:4, 1:3, 1:3, 1:2]\n\n\n, , person_home_ownership = MORTGAGE, previous_loan_defaults_on_file = No\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.09204538045 0.23441689989 0.68234908528\n                 .1 - .19 0.11181890282 0.27953722168 0.69203251344\n                 .2 - .29 0.14229782194 0.35013823128 0.82235274788\n                 .3+      0.25833519803 0.50364778600 0.79207013871\n\n, , person_home_ownership = OWN, previous_loan_defaults_on_file = No\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00036142836 0.00025706941 0.37554019015\n                 .1 - .19 0.13415540959 0.17275431782 0.31612550164\n                 .2 - .29 0.29006635834 0.28331900258 0.40937696665\n                 .3+      0.43912749408 0.31612550164 0.62445980985\n\n, , person_home_ownership = RENT, previous_loan_defaults_on_file = No\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.14568259065 0.34125578284 0.85128541940\n                 .1 - .19 0.20362270164 0.39789409908 0.85317350774\n                 .2 - .29 0.70486047665 0.77799227799 0.93997469113\n                 .3+      0.99989151660 0.99995745188 0.99980609633\n\n, , person_home_ownership = MORTGAGE, previous_loan_defaults_on_file = Yes\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00001960523 0.00002052057 0.00025148375\n                 .1 - .19 0.00002556734 0.00002182320 0.00023934897\n                 .2 - .29 0.00010091632 0.00006323831 0.00044476072\n                 .3+      0.00037713079 0.00020416497 0.00246791708\n\n, , person_home_ownership = OWN, previous_loan_defaults_on_file = Yes\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00018761726 0.00015706477 0.00099009901\n                 .1 - .19 0.00018561140 0.00012266326 0.00072233459\n                 .2 - .29 0.00032129546 0.00022984279 0.00133191263\n                 .3+      0.00070761393 0.00057803468 0.00491159136\n\n, , person_home_ownership = RENT, previous_loan_defaults_on_file = Yes\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00003029679 0.00001976144 0.00025706941\n                 .1 - .19 0.00002644355 0.00001597107 0.00019282684\n                 .2 - .29 0.00010616162 0.00005181884 0.00075369310\n                 .3+      0.50000000000 0.50000000000 0.50000000000\n\n\nAnother node I was interested in was the interest rate levels. Below I show the probability of each interest rate level. Interesting that the probability of getting an interest rate between 10% and 15% seemed similar if the person has or has not defaulted on a previous loan.\n\n\nCode\n# loan interest rates\n# &lt; 10 percent interest rate\nhc_fit$loan_int_rate_brack$prob[1, 1:5, 1:3, 1:2]\n\n\n, , previous_loan_defaults_on_file = No\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4319922 0.3372990 0.2485702\n   5k - 9.99k   0.4289390 0.4413874 0.2817689\n   10k - 14.99k 0.3818242 0.3749606 0.2521690\n   15k - 19.99k 0.2261606 0.2045198 0.1497653\n   20k+         0.1632122 0.1856410 0.1316751\n\n, , previous_loan_defaults_on_file = Yes\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4996333 0.4057123 0.3626205\n   5k - 9.99k   0.5184934 0.4311780 0.4009119\n   10k - 14.99k 0.4553838 0.3736940 0.3217806\n   15k - 19.99k 0.3118322 0.3258567 0.2633289\n   20k+         0.2222483 0.2171009 0.1872255\n\n\nCode\n# 10-15 percent interest rate\nhc_fit$loan_int_rate_brack$prob[2, 1:5, 1:3, 1:2]\n\n\n, , previous_loan_defaults_on_file = No\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4828632 0.5792025 0.6103174\n   5k - 9.99k   0.4720931 0.4653994 0.5596162\n   10k - 14.99k 0.4904833 0.5168716 0.5987237\n   15k - 19.99k 0.6113127 0.6723164 0.6480722\n   20k+         0.5282189 0.5364103 0.5793884\n\n, , previous_loan_defaults_on_file = Yes\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4669755 0.5270534 0.5972150\n   5k - 9.99k   0.4503999 0.4948113 0.5578687\n   10k - 14.99k 0.5006534 0.5205381 0.6228410\n   15k - 19.99k 0.6224036 0.6062305 0.6886723\n   20k+         0.6525271 0.6379426 0.6686627\n\n\nCode\n# 15+ percent interest rate\nhc_fit$loan_int_rate_brack$prob[3, 1:5, 1:3, 1:2]\n\n\n, , previous_loan_defaults_on_file = No\n\n               person_home_ownership\nloan_amnt_brack   MORTGAGE        OWN       RENT\n   &lt;5k          0.08514465 0.08349857 0.14111240\n   5k - 9.99k   0.09896793 0.09321327 0.15861494\n   10k - 14.99k 0.12769256 0.10816777 0.14910728\n   15k - 19.99k 0.16252675 0.12316384 0.20216242\n   20k+         0.30856886 0.27794872 0.28893644\n\n, , previous_loan_defaults_on_file = Yes\n\n               person_home_ownership\nloan_amnt_brack   MORTGAGE        OWN       RENT\n   &lt;5k          0.03339124 0.06723434 0.04016452\n   5k - 9.99k   0.03110673 0.07401072 0.04121932\n   10k - 14.99k 0.04396282 0.10576785 0.05537834\n   15k - 19.99k 0.06576415 0.06791277 0.04799882\n   20k+         0.12522459 0.14495658 0.14411178"
  },
  {
    "objectID": "posts/2024-11-08-bayes-net-loan-approval/index.html#cross-validation",
    "href": "posts/2024-11-08-bayes-net-loan-approval/index.html#cross-validation",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "Cross Validation",
    "text": "Cross Validation\nI also wanted to include the code for cross validation. You can either conduct cross validation with the data and start with a structure learning algorithm or you can include the DAG that was created and then make predictions. Here, since I am using the DAG I created with the training dataset, it will separate the data into a training and validation datasets. I am going to focus on the reduction of the classification error. I am also going to use the parent nodes to predict whether people get approved for loans and have 10 folds. I also checked to see the confusion matrix and see the proportions of true and false positives and true and false negatives. Overall, it seems like the model is doing okay with false positives and false negatives.\n\n\nCode\n# set.seed(12345)\n# hc_cv_fit &lt;- bn.cv(\n#   loan_train, \n#   bn = \"hc\", \n#   algorithm.args = list(\n#     blacklist = bl\n#     ),\n#   loss = \"pred\",\n#   loss.args = list(\n#     predict = \"parents\",\n#     target = \"loan_status\"\n#     ),\n#   runs = 10\n#   )\n\nset.seed(12345)\nhc_cv_fit &lt;- bn.cv(\n  data = loan_train,\n  hc_dag,\n  loss = \"pred\",\n  loss.args = list(\n    predict = \"parents\",\n    target = \"loan_status\"\n    ),\n  runs = 10\n)\n\n# hc_cv_fit[[1]][[1]] |&gt; str()\n\nhc_cv_fit\n\n\n\n  k-fold cross-validation for Bayesian networks\n\n  target network structure:\n   [loan_percent_income_brack][person_age_brack][person_gender]\n   [person_education][credit_score_brack|person_age_brack:person_education]\n   [person_income_brack|loan_percent_income_brack]\n   [loan_amnt_brack|loan_percent_income_brack:person_income_brack]\n   [person_home_ownership|person_income_brack]\n   [previous_loan_defaults_on_file|credit_score_brack:loan_percent_income_brack:person_home_ownership]\n   [loan_int_rate_brack|loan_amnt_brack:person_home_ownership:previous_loan_defaults_on_file]\n   [loan_intent|person_home_ownership:previous_loan_defaults_on_file]\n   [loan_status|loan_percent_income_brack:loan_int_rate_brack:person_home_ownership:previous_loan_defaults_on_file]\n  number of folds:                       10 \n  loss function:                         Classification Error \n  training node:                         loan_status \n  number of runs:                        10 \n  average loss over the runs:            0.1106233 \n  standard deviation of the loss:        0.0001862066 \n\n\nCode\nmap(\n  1:10,\n  ~round(\n  prop.table(\n    table(\n      hc_cv_fit[[.x]][[1]]$observed,\n      hc_cv_fit[[.x]][[1]]$predicted\n    )\n  ),\n  2\n)\n)\n\n\n[[1]]\n   \n       0    1\n  0 0.74 0.03\n  1 0.08 0.15\n\n[[2]]\n   \n       0    1\n  0 0.75 0.02\n  1 0.09 0.13\n\n[[3]]\n   \n       0    1\n  0 0.76 0.03\n  1 0.09 0.13\n\n[[4]]\n   \n       0    1\n  0 0.76 0.03\n  1 0.08 0.13\n\n[[5]]\n   \n       0    1\n  0 0.75 0.03\n  1 0.09 0.14\n\n[[6]]\n   \n       0    1\n  0 0.76 0.02\n  1 0.08 0.14\n\n[[7]]\n   \n       0    1\n  0 0.74 0.03\n  1 0.09 0.14\n\n[[8]]\n   \n       0    1\n  0 0.74 0.03\n  1 0.08 0.15\n\n[[9]]\n   \n       0    1\n  0 0.74 0.02\n  1 0.09 0.14\n\n[[10]]\n   \n       0    1\n  0 0.75 0.03\n  1 0.09 0.14\n\n\nThe last thing I’ll do when working with the training data and the original DAG is to predict if a person is approved for a loan based on likelihood weighting. Predicting can be done by using the parents, similar to what was done for the cross validation, but the bayes-lw method is often a better method. It does take longer to run the code though. I’m using the training set to be able to compare the confusion matrix for this model and the updated model with additonal arcs.\n\n\nCode\n# Use predict to infer the target variable on the test set\nset.seed(12345)\nhc_pred &lt;- predict(\n  hc_fit,\n  node = \"loan_status\",\n  data = loan_train,\n  method = \"bayes-lw\" # \"parents\"\n  )\n\nround(\n  prop.table(\n    table(\n      loan_train$loan_status,\n      hc_pred\n    )\n  ),\n  2\n)\n\n\n   hc_pred\n       0    1\n  0 0.75 0.03\n  1 0.08 0.14\n\n\nI decided to try and include the gender node and made a small change to include an edge between credit score and home ownership.\n\n\nCode\nhc_up_dag &lt;- hc_dag\n\nhc_up_dag &lt;- set.arc(hc_up_dag, from = \"person_gender\", to = \"credit_score_brack\")\nhc_up_dag &lt;- set.arc(hc_up_dag, from = \"credit_score_brack\", to = \"person_home_ownership\")\n\ngraphviz.plot(hc_up_dag)\n\n\n\n\n\n\n\n\n\nFirst, I’ll check the network scores between the two models. Overall, it does not seem like much has changed but the updated model has a negligible improvement so I decided to use that model.\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"loglik\")\n)\n\n\n[[1]]\n[1] -386319.3\n\n[[2]]\n[1] -386286.4\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"aic\")\n)\n\n\n[[1]]\n[1] -386717.3\n\n[[2]]\n[1] -386784.4\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bic\")\n)\n\n\n[[1]]\n[1] -388393.7\n\n[[2]]\n[1] -388882\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bde\", iss = 5000, prior = \"vsp\")\n)\n\n\n[[1]]\n[1] -406968.7\n\n[[2]]\n[1] -406442.9\n\n\nLooking at the confusion matrix, nothing has changed so I’ll now just move on to using the testing dataset.\n\n\nCode\nset.seed(12345)\nhc_up_fit &lt;- bn.fit(hc_up_dag, data = loan_train, method = \"bayes\", iss = 5)\n\nset.seed(12345)\nhc_up_predict &lt;- predict(\n  hc_up_fit,\n  node = \"loan_status\",\n  data = loan_train,\n  method = \"bayes-lw\" # \"parents\"\n  )\n\nround(\n  prop.table(\n    table(\n      loan_train$loan_status,\n      hc_up_predict\n    )\n  ),\n  2\n)\n\n\n   hc_up_predict\n       0    1\n  0 0.75 0.03\n  1 0.08 0.14"
  },
  {
    "objectID": "posts/2024-11-14-prophet-model/index.html",
    "href": "posts/2024-11-14-prophet-model/index.html",
    "title": "Prophet Model",
    "section": "",
    "text": "Photo by Annie Spratt on Unsplash\nNOTE: This is for practicing forecasting skills and should not be seen as a guide for predicting stock prices.\nAfter some reading, I finally have a good understanding of how to utilize forecasting for time-series analyses. This post started because even after a BA, 2 masters degrees, and a doctorate, my brother still has no clue what I do. He, along with most of my family think I am a Clinical Psychologist.\nSo for me to try and make my brother understand what I do, I thought I would show him with something that he has become interested in recently; stocks. So for this post, I’ll\nBelow are all the sites for the packages I used.\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(prophet)\nlibrary(lubridate)\nlibrary(modeltime)\nlibrary(timetk)"
  },
  {
    "objectID": "posts/2024-11-14-prophet-model/index.html#loading-data",
    "href": "posts/2024-11-14-prophet-model/index.html#loading-data",
    "title": "Prophet Model",
    "section": "Loading Data",
    "text": "Loading Data\nTo load the Google Finance data, I decided to pick a stock that my brother had, which in this case was JetBlue. A cool feature about Google Finance and Google Sheets is that you can use the following formula in a Google Sheet on the first cell of the first column =GOOGLEFINANCE(\"JBLU\", \"price\", DATE(2000,1,1), DATE(2024, 1, 1), \"DAILY\") and it will give you the date and stock closing values for whatever period you’d like. The example above provides Google financial data for JBLU or the abbreviation for JetBlue stock. It also provides the price of the stock from the first day that there is data on JetBlue stocks, which in this case is April 12th 2002. You can also choose the interval of time for the stock prices. I decided to look at daily data because I have several years of data.\nJetBlue Sheet\nHere I have a copy of my Google Sheet for JetBlue that I will use to train and test my Prophet model. Instead of having a .csv file on my local machine, I decided to keep this on Google Drive so that it constantly updates with the Google Finance function. This meant that I had to use the googlesheets4 package to load the data from a Google Sheet. I also changed the name and class of the date variable to make it a date variable instead of a date and time variable.\n\n\nCode\ngooglesheets4::gs4_deauth()\n\ntheme_set(theme_light())\n\njet &lt;- googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1SpRXsC3kXDaQLUfC6cPIOvsqxDF6updhgHRJeT8PTog/edit#gid=0\", sheet = 1) |&gt; \n  janitor::clean_names() |&gt;\n  mutate(ds = as_date(date))\n\n\n\nCleaning Up the Data\nBased on some visualizations below, I also decided to create some additional variables from the date variable. Specifically, I used lubridate's wday() function to create a new variable that gives you the actual day from the corresponding cell’s date. I also used the ts_clean_vec function from time_tk to clean for outliers in the stock price values. There are additional arguments for the function, like applying a Box-Cox transformation but that is for a multiplicative trend, which this model does not appear to fit since the variation in the outcome does not grow exponentially. I’ll also include 2002 as the reference year for the year variable and make sure that my data is arranged by date.\n\n\nCode\njetblue &lt;- jet |&gt; \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) |&gt; \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') |&gt; \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002')) |&gt; \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') |&gt;\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) |&gt; \n  select(-drop) |&gt; \n  arrange(ds)"
  },
  {
    "objectID": "posts/2024-11-14-prophet-model/index.html#visualizing-data",
    "href": "posts/2024-11-14-prophet-model/index.html#visualizing-data",
    "title": "Prophet Model",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nStarting with some quick visualizations, we can see that the only area that there is a difference in the variation of the stock prices is in the beginning of 2020. I wonder what that could have been.\n\n\nCode\njetblue |&gt; \n  group_by(year_num, month_num) |&gt; \n  summarize(var_value = sd(close)^2) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(month_num, var_value)) + \n  geom_point() + \n  facet_wrap(vars(year_num))\n\n\n\n\n\n\n\n\n\nNext, we can look at the histograms for the outcome of interest. If we look at the histograms, we can see that there are potential outliers in the original stock prices data. We can also see that cleaning the variable removed the potential outliers.\n\n\nCode\nonly_numeric &lt;- jetblue |&gt; \n  select(close, clean)\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~ggplot(data = only_numeric,\n             aes(.x)) + \n       geom_histogram(color = 'white',\n                      fill = 'dodgerblue') +\n       geom_vline(xintercept = mean(.x) +\n                    sd(.x) +\n                    sd(.x) +\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       geom_vline(xintercept = mean(.x) -\n                    sd(.x) -\n                    sd(.x) -\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       labs(title = .y))\n\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nThere will also be a lot of use of the purrr package and the map functions, which are part of the tidyverse. We can also see that in the plot series visualization using modeltime's plot_time_series function, that the cleaned stock prices remove the outliers. So from here on out, I’ll be using the cleaned stock prices.\n\n\nCode\nmap2(only_numeric,\n     names(only_numeric),\n     ~only_numeric |&gt; \n       plot_time_series(jetblue$ds,\n                        .x,\n                        .interactive = FALSE) + \n       labs(title = .y))\n\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nWe can also look for anomalies, or points that deviate from the trend. Using the plot_anomaly_diagnostics function from the modeltime package, I can see all the anomalies in the data. I also used ggplot to create my own visualization using the same data. Lastly, we’ll deal with those anomalies by removing them from the dataset. This is not too much of a problem because the Prophet model should be able to handle this fairly easy.\n\n\nCode\njetblue |&gt; \n  plot_anomaly_diagnostics(ds,\n                           clean,\n                           .facet_ncol = 1,\n                           .interactive = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nCode\njetblue |&gt; \n  tk_anomaly_diagnostics(ds,\n                         clean) |&gt; \n  ggplot(aes(ds, observed)) + \n  geom_line() + \n  geom_point(aes(color = anomaly)) +\n  viridis::scale_color_viridis(option = 'D',\n                               discrete = TRUE,\n                               begin = .5,\n                               end = 0)\n\n\n\n\n\n\n\n\n\nCode\nanomaly &lt;- jetblue |&gt;\n  tk_anomaly_diagnostics(ds,\n                         clean)\n\njetblue &lt;- left_join(jetblue, anomaly) |&gt;\n  filter(anomaly != 'Yes')\n\n\nWe can also look into additional regressors to include in the model by looking into seasonality. We can see some fluctuation in stock prices across the years. We’ll include the year variable as another regressor on the stock prices.\n\n\nCode\njetblue |&gt; \n  plot_seasonal_diagnostics(ds,\n                            clean,\n                            .interactive = FALSE)"
  },
  {
    "objectID": "posts/2024-11-14-prophet-model/index.html#training-the-prophet-model",
    "href": "posts/2024-11-14-prophet-model/index.html#training-the-prophet-model",
    "title": "Prophet Model",
    "section": "Training the Prophet Model",
    "text": "Training the Prophet Model\nBefore we begin, I’m going to designate 10 cores to process any models run.\n\n\nCode\nset.seed(05262022)\n\nparallel::detectCores()\n\n\n[1] 12\n\n\nCode\nparallel_start(10,\n               .method = 'parallel')\n\n\nFirst, instead of the normal initial_split used for training and testing splits, we’ll use the initial_time_split function from tidymodels to separate the first 80% of the data into training set and the last 20% into the testing set.\n\n\nCode\nset.seed(05262022)\njet_split &lt;- initial_time_split(jetblue)\n\n\n\nProphet Model Function\nI decided to create my own Prophet function to be able to use for both training the model and testing it. In this function, I’ve also included parameters that can be changed to see if the model performs better or worse. Lastly, the train = TRUE allows us to practice with the training dataset and then when we’re happy with the model, we can use it to test our model. For our model, we’ll be predicting stock prices with date and comparing each year to the reference year (2002).\n\n\nCode\nprophet_mod &lt;- function(splits,\n                        changepoints = .05,\n                        seasonality = .01,\n                        holiday = .01,\n                        season_type = 'additive',\n                        day_season = 'auto',\n                        week_season = 'auto',\n                        year_season = 'auto',\n                        train = TRUE){\n  library(tidyverse)\n  library(tidymodels)\n  library(modeltime)\n  library(prophet)\n  \n  analy_data &lt;- analysis(splits)\n  assess_data &lt;- assessment(splits)\n  \n  model &lt;- prophet_reg() |&gt; \n    set_engine(engine = 'prophet',\n               verbose = TRUE) |&gt; \n    set_args(prior_scale_changepoints = changepoints,\n             prior_scale_seasonality = seasonality,\n             prior_scale_holidays = holiday,\n             season = season_type,\n             seasonality_daily = day_season,\n             seasonality_weekly = week_season,\n             seasonality_yearly = year_season) |&gt; \n    fit(clean ~ ds + year_num, \n        data = analy_data)\n  \n  if(train == TRUE){\n    train_cali &lt;- model |&gt; \n      modeltime_calibrate(new_data = analy_data)\n    \n    train_acc &lt;- train_cali |&gt; \n      modeltime_accuracy()\n    \n    return(list(train_cali, train_acc))\n  }\n  \n  else{\n    test_cali &lt;- model |&gt; \n      modeltime_calibrate(new_data = assess_data)\n    \n    test_acc &lt;- test_cali |&gt; \n      modeltime_accuracy()\n    \n    return(list(test_cali, test_acc))\n  }\n}\n\n\nIt is worth noting that I’m using the modeltime package to run the prophet model because I believe it is easier to use (especially for later steps) than from Prophet, but both can be implemented in this function. Let’s try running this model with the some random parameters I chose from the Prophet website until realizing that the modeltime parameters are log transformed.\n\n\nCode\nset.seed(05262022)\nbaseline &lt;- prophet_mod(jet_split,\n                 train = TRUE) |&gt; \n  pluck(2)\n\nbaseline\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted  1.02  9.14  4.71  8.94  1.38 0.950\n\n\nSo with the model, we can see that the Mean Absolute Scaled Error (MASE) is 4.7105317 and the Root Mean Square Error (RMSE) is 1.3773241. Not bad for an initial run. Let’s look at how the model fits the training data.\n\n\nCode\nprophet_mod(jet_split,\n                 train = TRUE) |&gt;  \n  pluck(1) |&gt; \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Prophet Baseline Model')\n\n\n\n\n\n\n\n\n\nSo the model appears to follow the trend line. We’ll try to tune some of these parameters to see if we can make the model better.\n\n\nTuning the Model\nNow, I’ll tune the prior scale values for the model. I’ll use the grid_latin_hypercube from the dials package in tidymodels to choose 5 sets of parameter values to run. I’m also using the rolling_origin from the rsample package in tidymodels because we are working with time series data. This does not create random samples but instead has samples with data points with consecutive values.\n\n\nCode\nset.seed(05262022)\n\nproph_model &lt;- prophet_reg() |&gt;\n  set_engine(engine = 'prophet',\n             verbose = TRUE) |&gt;\n  set_args(prior_scale_changepoints = tune(),\n           prior_scale_seasonality = tune(),\n           prior_scale_holidays = tune(),\n           season = 'additive',\n           seasonality_daily = 'auto',\n           seasonality_weekly = 'auto',\n           seasonality_yearly = 'auto')\n\nproph_rec &lt;-\n  recipe(clean ~ ds + year_num,\n         data = training(jet_split))\n\n\nset.seed(05262022)\ntrain_fold &lt;-\n  rolling_origin(training(jet_split),\n                 initial = 270,  \n                 assess = 90, \n                 skip = 30,\n                 cumulative = TRUE)\n\nset.seed(05262022)\ngrid_values &lt;-\n  grid_latin_hypercube(prior_scale_changepoints(),\n                       prior_scale_seasonality(),\n                       prior_scale_holidays(),\n                       size = 5)\n\nset.seed(05262022)\nproph_fit &lt;- tune_grid(object = proph_model,\n                       preprocessor = proph_rec,\n                       resamples = train_fold,\n                       grid = grid_values,\n                       control = control_grid(verbose = TRUE,\n                                              save_pred = TRUE,\n                                              allow_par = TRUE))\n\n\ntuned_metrics &lt;- collect_metrics(proph_fit)\ntuned_metrics |&gt;\n  filter(.metric == 'rmse') |&gt; \n  arrange(mean)\n\n# saveRDS(tuned_metrics, file = 'tuned_metrics.rds')\n\n\n\n\nCode\nmetrics &lt;-\n  readr::read_rds(here::here('posts/2024-11-14-prophet-model/tuned_metrics.rds'))\n\nmetrics |&gt; \n  filter(.metric == 'rmse') |&gt; \n  arrange(mean)\n\n\n# A tibble: 5 × 9\n  prior_scale_changepoints prior_scale_seasonality prior_scale_holidays .metric\n                     &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;  \n1                  3.53                    0.0170               1.12    rmse   \n2                  0.00139                 0.00166              0.00172 rmse   \n3                  0.884                  36.4                  0.0131  rmse   \n4                  0.0549                  0.261                0.231   rmse   \n5                 43.0                     3.80                12.2     rmse   \n# ℹ 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;,\n#   .config &lt;chr&gt;\n\n\nFor the sake of not waiting for this to render, I decided to make a RDS file of the metrics gathered from the tuned Prophet model. We can see that the RMSE value was 2.52 and the prior scale changepoint value was 3.53, the prior scale seasonality value was 0.02, and the prior scale holiday value was 1.\n\n\nFinal Training Model\nI then decided to run the prophet model on the training dataset with the new parameter values.\n\n\nCode\nfinal_train &lt;- prophet_mod(jet_split,\n                 changepoints = 3.53,\n                 seasonality = .017,\n                 holiday = 1.12,\n                 train = TRUE) |&gt;  \n  pluck(2)\n\nfinal_train\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted 0.864  7.59  4.00  7.49  1.20 0.962\n\n\n\n\nCode\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = TRUE) |&gt;  \n  pluck(1) |&gt; \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Training Model')\n\n\n\n\n\n\n\n\n\nWe can see that when using the whole training set, we have a RMSE of 1.2 and a MASE of 4 so both metrics reduced slightly."
  },
  {
    "objectID": "posts/2024-11-14-prophet-model/index.html#testing-the-model",
    "href": "posts/2024-11-14-prophet-model/index.html#testing-the-model",
    "title": "Prophet Model",
    "section": "Testing the Model",
    "text": "Testing the Model\nFinally, let’s test our Prophet model to see how well the model fits.\n\n\nCode\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) |&gt;\n  pluck(1) |&gt; \n  modeltime_forecast(new_data = testing(jet_split),\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Testing Model')\n\n\n\n\n\n\n\n\n\n\n\nCode\ntest_model &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) |&gt;\n  pluck(2)\n\ntest_model\n\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type   mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Test   10.5  119.  41.7  60.9  12.0 0.480\n\n\nBased on the visualization and the model fit metrics, we can see that our model fits much worse when used on the testing set. The MASE has gotten significantly worse (41.6831491) and so has the RMSE (11.9781056) ."
  },
  {
    "objectID": "posts/2024-11-14-prophet-model/index.html#forecasting-ahead-a-year",
    "href": "posts/2024-11-14-prophet-model/index.html#forecasting-ahead-a-year",
    "title": "Prophet Model",
    "section": "Forecasting Ahead a Year",
    "text": "Forecasting Ahead a Year\nWell our model fit could be better, but let’s see how the model looks when refit to the full data and forecast ahead a year. So in a year, it seems that JetBlue stock may experience an uptick in prices to almost $10 a stock at the beginning of 2024. Then it seems like the value may drop again as the year goes on. The confidence intervals for the forecast do not appear to change much from the rest of the model as well.\n\n\nCode\nfuture &lt;- jetblue |&gt; \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture &lt;-\n  future |&gt;\n  select(-year_num, -month_num, -day_num) |&gt;\n  mutate(date2 = ds) |&gt;\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') |&gt;\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num)) |&gt; \n  arrange(ds)\n\nglimpse(future)\n\n\nRows: 5,757\nColumns: 17\n$ close         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ ds            &lt;date&gt; 2002-04-12, 2002-04-15, 2002-04-16, 2002-04-17, 2002-04…\n$ actual_day    &lt;ord&gt; Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, M…\n$ clean         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ observed      &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ season        &lt;dbl&gt; -0.0036928764, 0.0015302364, -0.0071242584, 0.0004207782…\n$ trend         &lt;dbl&gt; 13.40549, 13.41502, 13.42456, 13.43409, 13.44363, 13.453…\n$ remainder     &lt;dbl&gt; -0.071797724, -0.016554889, 0.152565554, -0.074513535, -…\n$ seasadj       &lt;dbl&gt; 13.33369, 13.39847, 13.57712, 13.35958, 13.09113, 12.933…\n$ remainder_l1  &lt;dbl&gt; -2.211971, -2.211971, -2.211971, -2.211971, -2.211971, -…\n$ remainder_l2  &lt;dbl&gt; 2.223683, 2.223683, 2.223683, 2.223683, 2.223683, 2.2236…\n$ anomaly       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ recomposed_l1 &lt;dbl&gt; 11.18983, 11.20458, 11.20546, 11.22254, 11.24052, 11.237…\n$ recomposed_l2 &lt;dbl&gt; 15.62548, 15.64024, 15.64112, 15.65820, 15.67618, 15.673…\n$ year_num      &lt;fct&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 20…\n$ month_num     &lt;fct&gt; 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 05, …\n$ day_num       &lt;dbl&gt; 12, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 29, 30, 1, 2…\n\n\nCode\ntest_model1 &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) |&gt;\n  pluck(1)\n\ntest_model1 |&gt; \n  modeltime_refit(data = future) |&gt; \n  modeltime_forecast(new_data = future,\n                     actual_data = jetblue) |&gt; \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Forecasted JetBlue Stock Prices')"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html",
    "title": "Data Manipulation in R & Python",
    "section": "",
    "text": "Photo by Joseph Malone on Unsplash\nOne of my favorite posts is the comparison between data.table and the tidyverse’s dplyr packages. Here is the link to that post. I have used that when trying to build my competence in using data.table. Now I’m going to try and expand on that by creating this post that compares cases of using dplyr, data.table, pandas, and polars. Hopefully this can be as useful as the comparison between dplyr and data.table post was for me. This is not an extensive way of comparing them but just to get started for anyone that wants to use python more."
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#filtering-integers",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#filtering-integers",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Integers)",
    "text": "Filtering (Integers)\n\n\nCode\nr_data |&gt;\n  filter(\n    x &gt; 1\n  ) |&gt;\n  head()\n\n\n\n\nCode\nhead(\n  r_table[x &gt; 1]\n)\n\n\n\n\nCode\npy_data[py_data[\"x\"] &gt; 1].head()\n\n\n\n\nCode\npl_data.filter(pl.col('x') &gt; 1).head()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#filtering-categorical",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#filtering-categorical",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Categorical)",
    "text": "Filtering (Categorical)\n\n\nCode\nr_data |&gt;\n  filter(\n    y == 1\n  ) |&gt;\n  head()\n\n\n\n\nCode\nhead(\n  r_table[y == 1]\n)\n\n\n\n\nCode\npy_data[py_data[\"y\"] == 1].head()\n\n\n\n\nCode\npl_data.filter(pl.col('y') == 1).head()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering Multiple Columns",
    "text": "Filtering Multiple Columns\n\n\nCode\nr_data |&gt;\n  filter(\n    y == 1 &\n    x2 &lt; 0\n  ) |&gt;\n  head()\n\n\n\n\nCode\nhead(\n  r_table[\n    y == 1 &\n    x2 &gt; 0\n  ]\n)\n\n\n\n\nCode\npy_data[\n  (py_data[\"y\"] == 1) & \n  (py_data[\"x2\"] &gt; 0)\n    ].head()\n\n\n\n\nCode\npl_data.filter(pl.col('y') == 1, pl.col('x2') &gt; 0).head()\n# uses a comma instead of using &"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#sorting-rows",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#sorting-rows",
    "title": "Data Manipulation in R & Python",
    "section": "Sorting Rows",
    "text": "Sorting Rows\n\n\nCode\nr_data |&gt; \n  arrange(y) |&gt;\n  head()\n\n\n\n\nCode\nhead(\n  r_table[order(y)]\n)\n\n\n\n\nCode\npy_data.sort_values(by = \"y\").head()\n\n\n\n\nCode\npl_data.sort(pl.col('y')).head()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#selecting-specific-columns",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#selecting-specific-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Specific Columns",
    "text": "Selecting Specific Columns\n\n\nCode\nr_data |&gt;\n  select(\n    y\n  ) |&gt;\n  head()\n\n\n\n\nCode\nhead(\n  r_table[,\"y\"]\n)\n\n\n\n\nCode\npy_data[\"y\"].head()\n\n# py_data.filter(items = \"y\").head()\n\n\n\n\nCode\npl_data.select(pl.col('y')).head()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Multiple Columns",
    "text": "Selecting Multiple Columns\n\n\nCode\nr_data |&gt; \n  select(x, x2) |&gt; \n  head()\n\n\n\n\nCode\nhead(\n  r_table[,list(x, x2)]\n)\n\n\n\n\nCode\npy_data[[\"x\", \"x2\"]].head()\n# or\npy_data.filter(items = [\"x\", \"x2\"]).head()\n\n\n\n\nCode\npl_data.select(pl.col('x'), pl.col('x2')).head()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#selecting-using-regex",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#selecting-using-regex",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Using Regex",
    "text": "Selecting Using Regex\n\n\nCode\nr_data |&gt;\n  select(\n    matches(\"x\")\n  ) |&gt;\n  head()\n\n\n\n\nCode\ncols &lt;- grep(\"^x\", names(r_table))\n\nhead(\n  r_table[, ..cols]\n)\n\n\n\n\nCode\npy_data.filter(regex = \"x\").head()\n\n\n\n\nCode\nimport polars.selectors as cs\n\npl_data.select(cs.starts_with('x')).head()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#summarize-data",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#summarize-data",
    "title": "Data Manipulation in R & Python",
    "section": "Summarize Data",
    "text": "Summarize Data\n\n\nCode\nr_data |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n  r_data |&gt;\n  summarize(\n    total = sum(x)\n  )\n\n\n\n\nCode\nr_table[, .(avg = mean(x))]\n\nr_table[, .(total = sum(x))]\n\n\n\n\nCode\npy_data[\"x\"].mean()\n\npy_data[\"x\"].sum()\n\n\n\n\nCode\npl_data.select(pl.mean('x'))\n\npl_data.select(pl.sum('x'))"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Adding/Updating/Deleting Columns",
    "text": "Adding/Updating/Deleting Columns\n\n\nCode\nr_data &lt;- r_data |&gt;\n  mutate(\n    x_mult = x*x2\n  )\nhead(r_data)\n\n\n\n\nCode\nr_table[, x_mult := x*x2]\nhead(r_table[, \"x_mult\"])\n\n\n\n\nCode\npy_data[\"x_mult\"] = py_data[\"x\"] * py_data[\"x2\"]\npy_data[\"x_mult\"].head()\n\n\n\n\nCode\npl_data.with_columns((pl.col('x') * pl.col('x2')).alias('x_mult'))"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#counting",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#counting",
    "title": "Data Manipulation in R & Python",
    "section": "Counting",
    "text": "Counting\n\n\nCode\nr_data |&gt; count(y)\n\n\n\n\nCode\nr_table[, .N, by = (y)]\n\n\n\n\nCode\npy_data[\"y\"].value_counts()\n\n\n\n\nCode\npl.Series(pl_data.select(pl.col('y'))).value_counts()"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#group-by",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#group-by",
    "title": "Data Manipulation in R & Python",
    "section": "Group By",
    "text": "Group By\n\n\nCode\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n\n\n\nCode\nr_table[, .(avg = mean(x)), by = \"y\"]\n\n\n\n\nCode\npy_data.groupby(\"y\")[\"x\"].mean()\n\n\n\n\nCode\npl_data.group_by('y').agg(pl.col('x').mean())"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#chain-expressions",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#chain-expressions",
    "title": "Data Manipulation in R & Python",
    "section": "Chain Expressions",
    "text": "Chain Expressions\n\n\nCode\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  ) |&gt;\n  filter(\n    y == 1\n  )\n\n\n\n\nCode\nr_table[, \n  by = y,\n  .(avg = mean(x))\n  ][\n    y == 1\n  ]\n\n\n\n\nCode\npy_group = py_data.groupby(\"y\")[\"x\"].mean().reset_index()\n\npy_group.iloc[1:, ]\n\n\n\n\nCode\npl_group = pl_data.group_by('y').agg(pl.col('x').mean())\n\npl_group.filter(pl.col('y') == 1)"
  },
  {
    "objectID": "posts/2024-11-15-dplyr-datatable-python/index.html#pivot-data",
    "href": "posts/2024-11-15-dplyr-datatable-python/index.html#pivot-data",
    "title": "Data Manipulation in R & Python",
    "section": "Pivot Data",
    "text": "Pivot Data\n\n\nCode\nr_data |&gt;\n  pivot_longer(\n    -y\n  ) |&gt;\n  head()\n\n\n\n\nCode\nhead(melt(r_table, id.vars = \"y\"))\n\n\n\n\nCode\npy_data.melt(id_vars = ['y'], value_vars = ['x', 'x2', 'x_mult']).head()\n\n\n\n\nCode\npl_data.unpivot(index = 'y').head()"
  },
  {
    "objectID": "posts/2024-11-15-bayes-net-loan-approval/index.html",
    "href": "posts/2024-11-15-bayes-net-loan-approval/index.html",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "",
    "text": "Photo by Drahomír Hugo Posteby-Mach on Unsplash\nFor this tutorial, I wanted to create an interesting and maybe more realistic use of a Bayesian Network (bayes net). That is why I am walking through how to use a bayes net in a machine learning application while also seeing what the likelihood of getting a loan approved would be based off some Kaggle data (data can be found here). First, I’ll load in the packages I’ll need and the data."
  },
  {
    "objectID": "posts/2024-11-15-bayes-net-loan-approval/index.html#structure-learning",
    "href": "posts/2024-11-15-bayes-net-loan-approval/index.html#structure-learning",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "Structure Learning",
    "text": "Structure Learning\nWhile I’m not going to go into a lot of detail about it, structure learning is the process of learning the structure of the DAG from the data. For this example, I’ll be using different algorithms for structure learning to see which algorithm has the best network score, including the log likelihood and the Bayesian Dirichlet Equivalent (BDE) score. There are two arguments for each of the learning algorithms, a blacklist and a whitelist, which are edges (relationships) between nodes (variables) that we either don’t want the algorithm to make or want to make sure are included in the model respectively. Bnlearn has links to articles of interest for all of the structure learning algorithms here if interested about how the algorithms work.\nI only included a blacklist and made sure that there were no edges from the outcome of interest (loan_status) as well as no edges that end at the demographic nodes of gender, age, and education.\n\n\nCode\nbl &lt;- matrix(\n  c(\n    \"loan_status\", \"person_gender\",\n    \"loan_status\", \"person_age_brack\",\n    \"loan_status\", \"credit_score_brack\",\n    \"loan_status\", \"person_education\",\n    \"loan_status\", \"previous_loan_defaults_on_file\",\n    \"loan_status\", \"loan_percent_income_brack\",\n    \"loan_status\", \"person_income_brack\",\n    \"loan_status\", \"loan_amnt_brack\",\n    \"loan_status\", \"person_home_ownership\",\n    \"loan_status\", \"loan_int_rate_brack\",\n    \"loan_status\", \"loan_intent\",\n\n    \"person_age_brack\", \"person_gender\",\n    \"credit_score_brack\", \"person_gender\",\n    \"person_education\", \"person_gender\",\n    \"previous_loan_defaults_on_file\", \"person_gender\",\n    \"loan_percent_income_brack\", \"person_gender\",\n    \"person_income_brack\", \"person_gender\",\n    \"loan_amnt_brack\", \"person_gender\",\n    \"person_home_ownership\", \"person_gender\",\n    \"loan_int_rate_brack\", \"person_gender\",\n    \"loan_intent\", \"person_gender\",\n    \"person_gender\", \"person_age_brack\",\n    \"credit_score_brack\", \"person_age_brack\",\n    \"person_education\", \"person_age_brack\",\n    \"previous_loan_defaults_on_file\", \"person_age_brack\",\n    \"loan_percent_income_brack\", \"person_age_brack\",\n    \"person_income_brack\", \"person_age_brack\",\n    \"loan_amnt_brack\", \"person_age_brack\",\n    \"person_home_ownership\", \"person_age_brack\",\n    \"loan_int_rate_brack\", \"person_age_brack\",\n    \"loan_intent\", \"person_age_brack\",\n    \"person_age_brack\", \"person_education\",\n    \"person_gender\", \"person_education\",\n    \"credit_score_brack\", \"person_education\",\n    \"previous_loan_defaults_on_file\", \"person_education\",\n    \"loan_percent_income_brack\", \"person_education\",\n    \"person_income_brack\", \"person_education\",\n    \"loan_amnt_brack\", \"person_education\",\n    \"person_home_ownership\", \"person_education\",\n    \"loan_int_rate_brack\", \"person_education\",\n    \"loan_intent\", \"person_education\"\n  ),\n  ncol = 2,\n  byrow = TRUE,\n  dimnames = list(\n    NULL,\n    c(\"from\", \"to\")\n    )\n  )\n\n\n\n\nCode\nset.seed(12345)\nhc_bn &lt;- hc(\n  loan_train,\n  blacklist = bl\n  )\n\ngraphviz.plot(hc_bn)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(12345)\nmmhc_bn &lt;- mmhc(\n  loan_train,\n  blacklist = bl\n  )\n\ngraphviz.plot(mmhc_bn)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(12345)\niamb_bn &lt;- iamb(\n  loan_train,\n  blacklist = bl\n  )\n\ngraphviz.plot(iamb_bn)\n\n\n\n\n\n\n\n\n\nFrom the DAGs, apparently gender is not that important of a node in our model. We can also see that when using the Incremental Association (IAMB) algorithm that some of the edges between nodes are not directional. We will have to do some extra work by setting the arcs between the non-directional edges a certain way. This takes some domain knowledge to see what makes the most sense. This is where creating a bayes net becomes more art than science. Below are the decisions that were made to complete the DAG.\nThe DAG needs to be fully directional so that network scores can be computed.\n\n\nCode\narcs(iamb_dag) &lt;- arcs(iamb_bn)\n\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_int_rate_brack\", to = \"loan_intent\")\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_percent_income_brack\", to = \"loan_amnt_brack\")\n\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_percent_income_brack\", to = \"person_income_brack\")\niamb_dag &lt;- set.arc(iamb_dag, from = \"loan_amnt_brack\", to = \"person_income_brack\")\n\ngraphviz.plot(iamb_dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\narcs(hc_dag) &lt;- hc_bn$arcs\narcs(mmhc_dag) &lt;- mmhc_bn$arcs"
  },
  {
    "objectID": "posts/2024-11-15-bayes-net-loan-approval/index.html#domain-knowledge-dag",
    "href": "posts/2024-11-15-bayes-net-loan-approval/index.html#domain-knowledge-dag",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "Domain Knowledge DAG",
    "text": "Domain Knowledge DAG\nThe final DAG I created was something that I thought of without any structural learning. This uses demographic variables as the starting nodes and then setting arcs that made sense to me. This defined DAG will also be compared to the learned DAGs to see what model has the lowest log likelihood and BDE.\n\n\nCode\narcs &lt;- matrix(\n  c(\n    \"person_gender\", \"person_income_brack\",\n    \"person_gender\", \"person_home_ownership\",\n    \"person_gender\", \"previous_loan_defaults_on_file\",\n\n    \"person_age_brack\", \"person_income_brack\",\n    \"person_age_brack\", \"person_home_ownership\",\n    \"person_age_brack\", \"previous_loan_defaults_on_file\",\n\n    \"person_education\", \"person_income_brack\",\n    \"person_education\", \"person_home_ownership\",\n    \"person_education\", \"previous_loan_defaults_on_file\",\n\n    \"person_income_brack\", \"credit_score_brack\", \n    \"person_home_ownership\", \"credit_score_brack\", \n    \"previous_loan_defaults_on_file\", \"credit_score_brack\",\n\n    \"person_income_brack\", \"loan_percent_income_brack\",\n    \"person_home_ownership\", \"loan_percent_income_brack\",\n\n    \"person_gender\", \"loan_int_rate_brack\",\n    \"person_education\", \"loan_int_rate_brack\",\n    \"person_age_brack\", \"loan_int_rate_brack\",\n    \"person_income_brack\", \"loan_int_rate_brack\",\n    \"person_home_ownership\", \"loan_int_rate_brack\",\n    \"previous_loan_defaults_on_file\", \"loan_int_rate_brack\",\n\n    \"person_income_brack\", \"loan_amnt_brack\",\n    \"person_home_ownership\", \"loan_amnt_brack\",\n\n    \"loan_percent_income_brack\", \"loan_intent\",\n    \"loan_int_rate_brack\", \"loan_intent\",\n    \"loan_amnt_brack\", \"loan_intent\",\n    \"credit_score_brack\", \"loan_intent\",\n\n    \"loan_int_rate_brack\", \"loan_status\",\n    \"credit_score_brack\", \"loan_status\",\n    \"loan_percent_income_brack\", \"loan_status\",\n    \"loan_amnt_brack\", \"loan_status\",\n    \"previous_loan_defaults_on_file\", \"loan_status\",\n    \"loan_intent\", \"loan_status\"\n    ),\n  byrow = TRUE,\n  ncol = 2,\n  dimnames = list(NULL, c(\"from\", \"to\"))\n)\n\narcs(jp_dag) &lt;- arcs\n\ngraphviz.plot(jp_dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"loglik\")\n)\n\n\n[[1]]\n[1] -429780\n\n[[2]]\n[1] -410700.2\n\n[[3]]\n[1] -386319.3\n\n[[4]]\n[1] -389004.6\n\n[[5]]\n[1] -387806\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"aic\")\n)\n\n\n[[1]]\n[1] -429813\n\n[[2]]\n[1] -418042.2\n\n[[3]]\n[1] -386717.3\n\n[[4]]\n[1] -389278.6\n\n[[5]]\n[1] -389074\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bic\")\n)\n\n\n[[1]]\n[1] -429952\n\n[[2]]\n[1] -448967.2\n\n[[3]]\n[1] -388393.7\n\n[[4]]\n[1] -390432.7\n\n[[5]]\n[1] -394414.8\n\n\nCode\nmap(\n  list(\n    dag,\n    jp_dag,\n    hc_dag,\n    mmhc_dag,\n    iamb_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bde\", iss = 5)\n)\n\n\n[[1]]\n[1] -429941\n\n[[2]]\n[1] -430616.4\n\n[[3]]\n[1] -387687.8\n\n[[4]]\n[1] -389857.5\n\n[[5]]\n[1] -393009.6\n\n\nAfter calculating all of the scores, it seems like the DAG created from the Hill Climb algorithm is the best fitting model, so I am going to fit that DAG for the model.\n\n\nCode\nset.seed(12345)\nhc_fit &lt;- bn.fit(hc_dag, data = loan_train, method = \"bayes\", iss = 5)\n\n\nWhile this model used the training data, I’m still interested in looking at the likelihood of being accepted for a loan and the probabilities of getting a a low interest rate based on the loan amount, home ownership, and if the person has defaulted on a previous loan.\nI printed out the DAG again to make it easier to see what nodes I am particularly interested in. I also included the str() function to see the breakdown of the table of probabilities when using bnlearn.\nLooking at the output, the table is broken down into the first index showing the levels of loan status (1 = “0”/“Decline”, 2 = “1”/“Accepted”), the second index showing the levels of the loan amount as a percentage of annual income variable, and so on following the values of the list. For the conditional probabilities printed for the model (hc_fit$loan_status$prob[2, 1:4, 1:3, 1:3, 1:2]), I am only interested in looking at the combinations of each parent node for those that are accepted for a loan.\nIt seems that not having a previous default on file leads to a higher probability of being accepted for a loan, which makes sense.\n\n\nCode\ngraphviz.plot(hc_dag)\n\n\n\n\n\n\n\n\n\nCode\nhc_fit$loan_status$prob |&gt; str()\n\n\n 'table' num [1:2, 1:4, 1:3, 1:3, 1:2] 0.908 0.092 0.888 0.112 0.858 ...\n - attr(*, \"dimnames\")=List of 5\n  ..$ loan_status                   : chr [1:2] \"0\" \"1\"\n  ..$ loan_percent_income_brack     : chr [1:4] \"0-.09\" \".1 - .19\" \".2 - .29\" \".3+\"\n  ..$ loan_int_rate_brack           : chr [1:3] \"&lt;10\" \"10 - 15\" \"15+\"\n  ..$ person_home_ownership         : chr [1:3] \"MORTGAGE\" \"OWN\" \"RENT\"\n  ..$ previous_loan_defaults_on_file: chr [1:2] \"No\" \"Yes\"\n\n\nCode\n# loan status - yes\nhc_fit$loan_status$prob[2, 1:4, 1:3, 1:3, 1:2]\n\n\n, , person_home_ownership = MORTGAGE, previous_loan_defaults_on_file = No\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.09204538045 0.23441689989 0.68234908528\n                 .1 - .19 0.11181890282 0.27953722168 0.69203251344\n                 .2 - .29 0.14229782194 0.35013823128 0.82235274788\n                 .3+      0.25833519803 0.50364778600 0.79207013871\n\n, , person_home_ownership = OWN, previous_loan_defaults_on_file = No\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00036142836 0.00025706941 0.37554019015\n                 .1 - .19 0.13415540959 0.17275431782 0.31612550164\n                 .2 - .29 0.29006635834 0.28331900258 0.40937696665\n                 .3+      0.43912749408 0.31612550164 0.62445980985\n\n, , person_home_ownership = RENT, previous_loan_defaults_on_file = No\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.14568259065 0.34125578284 0.85128541940\n                 .1 - .19 0.20362270164 0.39789409908 0.85317350774\n                 .2 - .29 0.70486047665 0.77799227799 0.93997469113\n                 .3+      0.99989151660 0.99995745188 0.99980609633\n\n, , person_home_ownership = MORTGAGE, previous_loan_defaults_on_file = Yes\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00001960523 0.00002052057 0.00025148375\n                 .1 - .19 0.00002556734 0.00002182320 0.00023934897\n                 .2 - .29 0.00010091632 0.00006323831 0.00044476072\n                 .3+      0.00037713079 0.00020416497 0.00246791708\n\n, , person_home_ownership = OWN, previous_loan_defaults_on_file = Yes\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00018761726 0.00015706477 0.00099009901\n                 .1 - .19 0.00018561140 0.00012266326 0.00072233459\n                 .2 - .29 0.00032129546 0.00022984279 0.00133191263\n                 .3+      0.00070761393 0.00057803468 0.00491159136\n\n, , person_home_ownership = RENT, previous_loan_defaults_on_file = Yes\n\n                         loan_int_rate_brack\nloan_percent_income_brack           &lt;10       10 - 15           15+\n                 0-.09    0.00003029679 0.00001976144 0.00025706941\n                 .1 - .19 0.00002644355 0.00001597107 0.00019282684\n                 .2 - .29 0.00010616162 0.00005181884 0.00075369310\n                 .3+      0.50000000000 0.50000000000 0.50000000000\n\n\nAnother node I was interested in was the interest rate levels. Below I show the probability of each interest rate level. Interesting that the probability of getting an interest rate between 10% and 15% seemed similar if the person has or has not defaulted on a previous loan.\n\n\nCode\n# loan interest rates\n# &lt; 10 percent interest rate\nhc_fit$loan_int_rate_brack$prob[1, 1:5, 1:3, 1:2]\n\n\n, , previous_loan_defaults_on_file = No\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4319922 0.3372990 0.2485702\n   5k - 9.99k   0.4289390 0.4413874 0.2817689\n   10k - 14.99k 0.3818242 0.3749606 0.2521690\n   15k - 19.99k 0.2261606 0.2045198 0.1497653\n   20k+         0.1632122 0.1856410 0.1316751\n\n, , previous_loan_defaults_on_file = Yes\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4996333 0.4057123 0.3626205\n   5k - 9.99k   0.5184934 0.4311780 0.4009119\n   10k - 14.99k 0.4553838 0.3736940 0.3217806\n   15k - 19.99k 0.3118322 0.3258567 0.2633289\n   20k+         0.2222483 0.2171009 0.1872255\n\n\nCode\n# 10-15 percent interest rate\nhc_fit$loan_int_rate_brack$prob[2, 1:5, 1:3, 1:2]\n\n\n, , previous_loan_defaults_on_file = No\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4828632 0.5792025 0.6103174\n   5k - 9.99k   0.4720931 0.4653994 0.5596162\n   10k - 14.99k 0.4904833 0.5168716 0.5987237\n   15k - 19.99k 0.6113127 0.6723164 0.6480722\n   20k+         0.5282189 0.5364103 0.5793884\n\n, , previous_loan_defaults_on_file = Yes\n\n               person_home_ownership\nloan_amnt_brack  MORTGAGE       OWN      RENT\n   &lt;5k          0.4669755 0.5270534 0.5972150\n   5k - 9.99k   0.4503999 0.4948113 0.5578687\n   10k - 14.99k 0.5006534 0.5205381 0.6228410\n   15k - 19.99k 0.6224036 0.6062305 0.6886723\n   20k+         0.6525271 0.6379426 0.6686627\n\n\nCode\n# 15+ percent interest rate\nhc_fit$loan_int_rate_brack$prob[3, 1:5, 1:3, 1:2]\n\n\n, , previous_loan_defaults_on_file = No\n\n               person_home_ownership\nloan_amnt_brack   MORTGAGE        OWN       RENT\n   &lt;5k          0.08514465 0.08349857 0.14111240\n   5k - 9.99k   0.09896793 0.09321327 0.15861494\n   10k - 14.99k 0.12769256 0.10816777 0.14910728\n   15k - 19.99k 0.16252675 0.12316384 0.20216242\n   20k+         0.30856886 0.27794872 0.28893644\n\n, , previous_loan_defaults_on_file = Yes\n\n               person_home_ownership\nloan_amnt_brack   MORTGAGE        OWN       RENT\n   &lt;5k          0.03339124 0.06723434 0.04016452\n   5k - 9.99k   0.03110673 0.07401072 0.04121932\n   10k - 14.99k 0.04396282 0.10576785 0.05537834\n   15k - 19.99k 0.06576415 0.06791277 0.04799882\n   20k+         0.12522459 0.14495658 0.14411178"
  },
  {
    "objectID": "posts/2024-11-15-bayes-net-loan-approval/index.html#cross-validation",
    "href": "posts/2024-11-15-bayes-net-loan-approval/index.html#cross-validation",
    "title": "Using a Bayesian Network For Machine Learning",
    "section": "Cross Validation",
    "text": "Cross Validation\nI also wanted to include the code for cross validation. You can either conduct cross validation with the data and start with a structure learning algorithm or you can include the DAG that was created and then make predictions. Here, since I am using the DAG I created with the training dataset, it will separate the data into a training and validation datasets. I am going to focus on the reduction of the classification error. I am also going to use the parent nodes to predict whether people get approved for loans and have 10 folds. I also checked to see the confusion matrix and see the proportions of true and false positives and true and false negatives. Overall, it seems like the model is doing okay with false positives and false negatives.\n\n\nCode\n# set.seed(12345)\n# hc_cv_fit &lt;- bn.cv(\n#   loan_train, \n#   bn = \"hc\", \n#   algorithm.args = list(\n#     blacklist = bl\n#     ),\n#   loss = \"pred\",\n#   loss.args = list(\n#     predict = \"parents\",\n#     target = \"loan_status\"\n#     ),\n#   runs = 10\n#   )\n\nset.seed(12345)\nhc_cv_fit &lt;- bn.cv(\n  data = loan_train,\n  hc_dag,\n  loss = \"pred\",\n  loss.args = list(\n    predict = \"parents\",\n    target = \"loan_status\"\n    ),\n  runs = 10\n)\n\n# hc_cv_fit[[1]][[1]] |&gt; str()\n\nhc_cv_fit\n\n\n\n  k-fold cross-validation for Bayesian networks\n\n  target network structure:\n   [loan_percent_income_brack][person_age_brack][person_gender]\n   [person_education][credit_score_brack|person_age_brack:person_education]\n   [person_income_brack|loan_percent_income_brack]\n   [loan_amnt_brack|loan_percent_income_brack:person_income_brack]\n   [person_home_ownership|person_income_brack]\n   [previous_loan_defaults_on_file|credit_score_brack:loan_percent_income_brack:person_home_ownership]\n   [loan_int_rate_brack|loan_amnt_brack:person_home_ownership:previous_loan_defaults_on_file]\n   [loan_intent|person_home_ownership:previous_loan_defaults_on_file]\n   [loan_status|loan_percent_income_brack:loan_int_rate_brack:person_home_ownership:previous_loan_defaults_on_file]\n  number of folds:                       10 \n  loss function:                         Classification Error \n  training node:                         loan_status \n  number of runs:                        10 \n  average loss over the runs:            0.1106233 \n  standard deviation of the loss:        0.0001862066 \n\n\nCode\nmap(\n  1:10,\n  ~round(\n  prop.table(\n    table(\n      hc_cv_fit[[.x]][[1]]$observed,\n      hc_cv_fit[[.x]][[1]]$predicted\n    )\n  ),\n  2\n)\n)\n\n\n[[1]]\n   \n       0    1\n  0 0.74 0.03\n  1 0.08 0.15\n\n[[2]]\n   \n       0    1\n  0 0.75 0.02\n  1 0.09 0.13\n\n[[3]]\n   \n       0    1\n  0 0.76 0.03\n  1 0.09 0.13\n\n[[4]]\n   \n       0    1\n  0 0.76 0.03\n  1 0.08 0.13\n\n[[5]]\n   \n       0    1\n  0 0.75 0.03\n  1 0.09 0.14\n\n[[6]]\n   \n       0    1\n  0 0.76 0.02\n  1 0.08 0.14\n\n[[7]]\n   \n       0    1\n  0 0.74 0.03\n  1 0.09 0.14\n\n[[8]]\n   \n       0    1\n  0 0.74 0.03\n  1 0.08 0.15\n\n[[9]]\n   \n       0    1\n  0 0.74 0.02\n  1 0.09 0.14\n\n[[10]]\n   \n       0    1\n  0 0.75 0.03\n  1 0.09 0.14\n\n\nThe last thing I’ll do when working with the training data and the original DAG is to predict if a person is approved for a loan based on likelihood weighting. Predicting can be done by using the parents, similar to what was done for the cross validation, but the bayes-lw method is often a better method. It does take longer to run the code though. I’m using the training set to be able to compare the confusion matrix for this model and the updated model with additonal arcs.\n\n\nCode\n# Use predict to infer the target variable on the test set\nset.seed(12345)\nhc_pred &lt;- predict(\n  hc_fit,\n  node = \"loan_status\",\n  data = loan_train,\n  method = \"bayes-lw\" # \"parents\"\n  )\n\nround(\n  prop.table(\n    table(\n      loan_train$loan_status,\n      hc_pred\n    )\n  ),\n  2\n)\n\n\n   hc_pred\n       0    1\n  0 0.75 0.03\n  1 0.08 0.14\n\n\nI decided to try and include the gender node and made a small change to include an edge between credit score and home ownership.\n\n\nCode\nhc_up_dag &lt;- hc_dag\n\nhc_up_dag &lt;- set.arc(hc_up_dag, from = \"person_gender\", to = \"credit_score_brack\")\nhc_up_dag &lt;- set.arc(hc_up_dag, from = \"credit_score_brack\", to = \"person_home_ownership\")\n\ngraphviz.plot(hc_up_dag)\n\n\n\n\n\n\n\n\n\nFirst, I’ll check the network scores between the two models. Overall, it does not seem like much has changed but the updated model has a negligible improvement so I decided to use that model.\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"loglik\")\n)\n\n\n[[1]]\n[1] -386319.3\n\n[[2]]\n[1] -386286.4\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"aic\")\n)\n\n\n[[1]]\n[1] -386717.3\n\n[[2]]\n[1] -386784.4\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bic\")\n)\n\n\n[[1]]\n[1] -388393.7\n\n[[2]]\n[1] -388882\n\n\nCode\nmap(\n  list(\n    hc_dag,\n    hc_up_dag\n  ),\n  ~bn_score(.x, loan_train, type = \"bde\", iss = 5000, prior = \"vsp\")\n)\n\n\n[[1]]\n[1] -406968.7\n\n[[2]]\n[1] -406442.9\n\n\nLooking at the confusion matrix, nothing has changed so I’ll now just move on to using the testing dataset.\n\n\nCode\nset.seed(12345)\nhc_up_fit &lt;- bn.fit(hc_up_dag, data = loan_train, method = \"bayes\", iss = 5)\n\nset.seed(12345)\nhc_up_predict &lt;- predict(\n  hc_up_fit,\n  node = \"loan_status\",\n  data = loan_train,\n  method = \"bayes-lw\" # \"parents\"\n  )\n\nround(\n  prop.table(\n    table(\n      loan_train$loan_status,\n      hc_up_predict\n    )\n  ),\n  2\n)\n\n\n   hc_up_predict\n       0    1\n  0 0.75 0.03\n  1 0.08 0.14"
  },
  {
    "objectID": "posts/2024-11-06-closeread-temp-blanket/index.html",
    "href": "posts/2024-11-06-closeread-temp-blanket/index.html",
    "title": "My submission to Posit’s Closeread Competition",
    "section": "",
    "text": "When I saw that Posit had posted a blog post about storytelling with Quarto (see blog post here) using Closeread I became interested in trying out Closeread for this competition. This is my first time reading about Closeread and it sparked my interest in trying to tell a story using a quarto document. For my submission, I thought I would do something silly about a topic that is serious. I wanted to show the weather differences in Los Angeles from 2003 to 2023 by creating a story of global warming while showing visualizations of the temperature differences using “temperature blankets”. If you know anything about me, the first thing you recognize is art != JP so instead I thought I would create a temperature blanket using ggplot2.\nFor the competition I decided to do everything in R because I knew I was going to be mess around a lot using the theme() function in ggplot2. Below is going to be a walkthough of my thought process for creating the Closeread story. Then I will probably create a Closeread specific folder within a GitHub repo or its own repo.\n\nRetrieving the Data\nI originally was going to use this site to create the year of data for my location. However, I found that the website uses an API that has an R package and a Python package. Shout out for those that made the API and the packages. This post will follow the R package.\nI will be using the weather_history() function, but there are some other interesting functions for forecasting in that package as well. From the documentation, the location can either be a latitutde x longitude vector or a string of a place. Figuring that LA is well known I used that for the location and then included the dates for the year of data I am interested in. The dates must follow the format of YYYY-MM-DD and when I ran the function orignally, I realized my temperatures were in Celsius. I decided to simply add a calculation to change it to Fahrenheit. To do the calculation, I used the formula below. Since all the data matched up, I decided to bind the two dataframes and check to make sure it was correct by viewing the first and last 5 rows.\n\\[\nF = (C * 1.8) + 32\n\\]\n\n\nCode\n# install.packages(\"openmeteo\")\n\nlibrary(tidyverse)\nlibrary(openmeteo)\n\nla23 &lt;- weather_history(\n  location = \"Los Angeles\",\n  start = \"2023-01-01\",\n  end = \"2023-12-31\",\n  daily = \"temperature_2m_max\"\n) |&gt;\n  mutate(\n    avg_temp = (daily_temperature_2m_max * 1.8) + 32\n  )\n\n\n`geocode()` has matched \"Los Angeles\" to:\nLos Angeles in California, United States\nPopulation: 3971883\nCo-ordinates: c(34.05223, -118.24368)\n\n\nCode\nla03 &lt;- weather_history(\n  location = \"Los Angeles\",\n  start = \"2003-01-01\",\n  end = \"2003-12-31\",\n  daily = \"temperature_2m_max\"\n) |&gt;\n  mutate(\n    avg_temp = (daily_temperature_2m_max * 1.8) + 32\n  )\n\n\n`geocode()` has matched \"Los Angeles\" to:\nLos Angeles in California, United States\nPopulation: 3971883\nCo-ordinates: c(34.05223, -118.24368)\n\n\nCode\ntemp &lt;- rbind(la03, la23)\n\ntemp |&gt; head()\n\n\n# A tibble: 6 × 3\n  date       daily_temperature_2m_max avg_temp\n  &lt;date&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n1 2003-01-01                     19.4     66.9\n2 2003-01-02                     22.5     72.5\n3 2003-01-03                     25.1     77.2\n4 2003-01-04                     26.1     79.0\n5 2003-01-05                     25.9     78.6\n6 2003-01-06                     22.8     73.0\n\n\nCode\ntemp |&gt; tail()\n\n\n# A tibble: 6 × 3\n  date       daily_temperature_2m_max avg_temp\n  &lt;date&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-12-26                     17.9     64.2\n2 2023-12-27                     17.1     62.8\n3 2023-12-28                     17.3     63.1\n4 2023-12-29                     17.6     63.7\n5 2023-12-30                     15.3     59.5\n6 2023-12-31                     15.4     59.7\n\n\nOkay, everything looks okay to me. I am going to focus on the average temperature to create my temperature blankets. I also just want to point out that all temperatures will be in Farenheit. Before making my visualization, I want to break down my date category into years, months, and days. This will be easier to use facet_wrap() to separate my years.\n\n\nCode\ntemp &lt;- temp |&gt;\n  mutate(date2 = date) |&gt;\n  separate(\n    date2,\n    into = c(\n      \"year\", \"month\", \"day\"\n    ),\n    sep = \"-\"\n  )\n\n\nNow, we can focus on creating our plot. Interestingly, I have never had to make one axis on my plots be set to an amount that would range across the entirety of the axis. This was definitely one of those times where I just tried something and BAM! it worked. I set my y-axis to 1 and it worked. We will not focus on the values for the y-axis because they don’t make any sense, but I did try some other values. The value that you choose on the y-axis does not matter, especially for this visualization because we are going to remove the axis titles and text.\n\n\nTemperature Blanket Creation\n\nFirst Attempt\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = avg_temp\n    )\n  ) +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nFor some reason, I am not a fan of the blankets being horizontal, so I’m going to change the orientation of them.\n\n\nVertical Alignment\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = avg_temp\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nWe need a color scale. This will be a manual scale to try and make a cool blanket style so after some googling I found this scale project sheet. This was the easiest scale to follow (for me) and the large balls of yarn allowed me to find color codes that matched the yarn fairly well.\nSome quick coding of the temperature ranges and the months should set up the template for my blanket design.\n\n\nColor Scale\n\n\nCode\ntemp &lt;- temp |&gt;\n  mutate(\n    temp_color = case_when(\n      avg_temp &gt; 96 ~ \"96+\", #cherry red\n      avg_temp &gt;= 89 & avg_temp &lt; 96 ~ \"89-95\", #really red\n      avg_temp &gt;= 82 & avg_temp &lt; 89 ~ \"82-88\", #carrot\n      avg_temp &gt;= 75 & avg_temp &lt; 82 ~ \"75-81\", #canary\n      avg_temp &gt;= 68 & avg_temp &lt; 75 ~ \"68-74\", #yellow\n      avg_temp &gt;= 61 & avg_temp &lt; 68 ~ \"61-67\", #green apple\n      avg_temp &gt;= 54 & avg_temp &lt; 61 ~ \"54-60\", #porcelain blue\n      avg_temp &gt;= 47 & avg_temp &lt; 54 ~ \"47-53\", #teal\n      avg_temp &gt;= 40 & avg_temp &lt; 47 ~ \"40-46\", #alaskan blue\n      avg_temp &gt;= 33 & avg_temp &lt; 40 ~ \"33-39\", #cobalt\n      avg_temp &gt;= 26 & avg_temp &lt; 33 ~ \"26-32\", #thistle\n      avg_temp &lt; 26 ~ \"Below 26\" #purple\n    ),\n    month_name = case_when(\n      month == \"01\" ~ \"Jan\",\n      month == \"02\" ~ \"Feb\",\n      month == \"03\" ~ \"Mar\",\n      month == \"04\" ~ \"Apr\",\n      month == \"05\" ~ \"May\",\n      month == \"06\" ~ \"Jun\",\n      month == \"07\" ~ \"Jul\",\n      month == \"08\" ~ \"Aug\",\n      month == \"09\" ~ \"Sept\",\n      month == \"10\" ~ \"Oct\",\n      month == \"11\" ~ \"Nov\",\n      month == \"12\" ~ \"Dec\"\n    ),\n    across(\n      c(\n        temp_color,\n        month_name\n      ),\n      ~as.factor(.x)\n    ),\n    temp_color = fct_relevel(\n      temp_color,\n      \"96+\",\n      \"89-95\",\n      \"82-88\",\n      \"75-81\",\n      \"68-74\",\n      \"61-67\",\n      \"54-60\",\n      \"47-53\",\n      \"40-46\",\n      \"33-39\",\n      \"26-32\",\n      \"Below 26\"\n    )\n  ) |&gt;\n  rowid_to_column()\n\n\n\n\nBlanket With New Color Scale\nThis is now how the full blanket for both 2003 and 2023. I still need to clean up the axis labels, the legend, and the facet titles.\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\n\n\nDay One Only\nI was thinking about how to show the progress of the blanket. I decided to use the dplyr::first() function, which was a first for me. Here I have the first day of the year with the temperature range values for 2003 and 2023.\n\n\nCode\ntemp |&gt;\n  group_by(year) |&gt;\n  mutate(\n    first = first(rowid)\n  ) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\n\n\nPrepping the Loop\nNext, I decided to do a quick loop where I used two different seq() functions. The first sequence was to creatte a row for each day of the year from 1 to 365 by each day. I was able to create rows called year_row for both years. There is probably a better way of doing this but I decided on using a filter() where I looped through each day and put that each day was less than the year_row column created. For instance, in the code chunk below you can see it as filter(year_row &lt; .x). So when the loop starts at 2 it will filter for values that are lower than 2, with the only value being 1. This will be important later on because I am not sure if I want to present a story of the weather and a creation of my blanket for every single day or if there is another metric to go by.\n\n\nCode\nmap(\n  seq(2, 10, 1),\n  ~temp |&gt;\n  select(\n    rowid,\n    date,\n    year,\n    temp_color,\n    avg_temp\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n\nLooping Through Every 30 Days\nI ended up deciding on showing my blanket every week. I’m still not sure about it since there are are 52 plots to look through. Also, now that I think about it, maybe LA was not the best place to showcase temperature for a blanket. Looking at the final blanket plot, it screams “Sorry its always nice here!” and that might not be the most interesting for a temperature blanket. I’ll have to start looking at other locations for better blanket designs. At least now I have the basic design of what I’ll do for the competition.\n\n\nCode\nmap(\n  seq(2, 365, 30),\n  ~temp |&gt;\n  select(\n    rowid,\n    date,\n    year,\n    temp_color,\n    avg_temp\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]"
  },
  {
    "objectID": "posts/2024-11-15-closeread-temp-blanket/index.html",
    "href": "posts/2024-11-15-closeread-temp-blanket/index.html",
    "title": "My submission to Posit’s Closeread Competition",
    "section": "",
    "text": "Photo by Karen Penroz on Unsplash\n\n\nWhen I saw that Posit had posted a blog post about storytelling with Quarto (see blog post here) using Closeread I became interested in trying out Closeread for this competition. This is my first time reading about Closeread and it sparked my interest in trying to tell a story using a quarto document. For my submission, I thought I would do something silly about a topic that is serious. I wanted to show the weather differences in Los Angeles from 2003 to 2023 by creating a story of global warming while showing visualizations of the temperature differences using “temperature blankets”. If you know anything about me, the first thing you recognize is art != JP so instead I thought I would create a temperature blanket using ggplot2.\nFor the competition I decided to do everything in R because I knew I was going to be mess around a lot using the theme() function in ggplot2. Below is going to be a walkthough of my thought process for creating the Closeread story. Then I will probably create a Closeread specific folder within a GitHub repo or its own repo.\n\nRetrieving the Data\nI originally was going to use this site to create the year of data for my location. However, I found that the website uses an API that has an R package and a Python package. Shout out for those that made the API and the packages. This post will follow the R package.\nI will be using the weather_history() function, but there are some other interesting functions for forecasting in that package as well. From the documentation, the location can either be a latitutde x longitude vector or a string of a place. Figuring that LA is well known I used that for the location and then included the dates for the year of data I am interested in. The dates must follow the format of YYYY-MM-DD and when I ran the function orignally, I realized my temperatures were in Celsius. I decided to simply add a calculation to change it to Fahrenheit. To do the calculation, I used the formula below. Since all the data matched up, I decided to bind the two dataframes and check to make sure it was correct by viewing the first and last 5 rows.\n\\[\nF = (C * 1.8) + 32\n\\]\n\n\nCode\n# install.packages(\"openmeteo\")\n\nlibrary(tidyverse)\nlibrary(openmeteo)\n\nla23 &lt;- weather_history(\n  location = \"Los Angeles\",\n  start = \"2023-01-01\",\n  end = \"2023-12-31\",\n  daily = \"temperature_2m_max\"\n) |&gt;\n  mutate(\n    avg_temp = (daily_temperature_2m_max * 1.8) + 32\n  )\n\n\n`geocode()` has matched \"Los Angeles\" to:\nLos Angeles in California, United States\nPopulation: 3971883\nCo-ordinates: c(34.05223, -118.24368)\n\n\nCode\nla03 &lt;- weather_history(\n  location = \"Los Angeles\",\n  start = \"2003-01-01\",\n  end = \"2003-12-31\",\n  daily = \"temperature_2m_max\"\n) |&gt;\n  mutate(\n    avg_temp = (daily_temperature_2m_max * 1.8) + 32\n  )\n\n\n`geocode()` has matched \"Los Angeles\" to:\nLos Angeles in California, United States\nPopulation: 3971883\nCo-ordinates: c(34.05223, -118.24368)\n\n\nCode\ntemp &lt;- rbind(la03, la23)\n\ntemp |&gt; head()\n\n\n# A tibble: 6 × 3\n  date       daily_temperature_2m_max avg_temp\n  &lt;date&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n1 2003-01-01                     19.4     66.9\n2 2003-01-02                     22.5     72.5\n3 2003-01-03                     25.1     77.2\n4 2003-01-04                     26.1     79.0\n5 2003-01-05                     25.9     78.6\n6 2003-01-06                     22.8     73.0\n\n\nCode\ntemp |&gt; tail()\n\n\n# A tibble: 6 × 3\n  date       daily_temperature_2m_max avg_temp\n  &lt;date&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-12-26                     17.9     64.2\n2 2023-12-27                     17.1     62.8\n3 2023-12-28                     17.3     63.1\n4 2023-12-29                     17.6     63.7\n5 2023-12-30                     15.3     59.5\n6 2023-12-31                     15.4     59.7\n\n\nOkay, everything looks okay to me. I am going to focus on the average temperature to create my temperature blankets. I also just want to point out that all temperatures will be in Farenheit. Before making my visualization, I want to break down my date category into years, months, and days. This will be easier to use facet_wrap() to separate my years.\n\n\nCode\ntemp &lt;- temp |&gt;\n  mutate(date2 = date) |&gt;\n  separate(\n    date2,\n    into = c(\n      \"year\", \"month\", \"day\"\n    ),\n    sep = \"-\"\n  )\n\n\nNow, we can focus on creating our plot. Interestingly, I have never had to make one axis on my plots be set to an amount that would range across the entirety of the axis. This was definitely one of those times where I just tried something and BAM! it worked. I set my y-axis to 1 and it worked. We will not focus on the values for the y-axis because they don’t make any sense, but I did try some other values. The value that you choose on the y-axis does not matter, especially for this visualization because we are going to remove the axis titles and text.\n\n\nTemperature Blanket Creation\n\nFirst Attempt\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = avg_temp\n    )\n  ) +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nFor some reason, I am not a fan of the blankets being horizontal, so I’m going to change the orientation of them.\n\n\nVertical Alignment\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = avg_temp\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\nWe need a color scale. This will be a manual scale to try and make a cool blanket style so after some googling I found this scale project sheet. This was the easiest scale to follow (for me) and the large balls of yarn allowed me to find color codes that matched the yarn fairly well.\nSome quick coding of the temperature ranges and the months should set up the template for my blanket design.\n\n\nColor Scale\n\n\nCode\ntemp &lt;- temp |&gt;\n  mutate(\n    temp_color = case_when(\n      avg_temp &gt; 96 ~ \"96+\", #cherry red\n      avg_temp &gt;= 89 & avg_temp &lt; 96 ~ \"89-95\", #really red\n      avg_temp &gt;= 82 & avg_temp &lt; 89 ~ \"82-88\", #carrot\n      avg_temp &gt;= 75 & avg_temp &lt; 82 ~ \"75-81\", #canary\n      avg_temp &gt;= 68 & avg_temp &lt; 75 ~ \"68-74\", #yellow\n      avg_temp &gt;= 61 & avg_temp &lt; 68 ~ \"61-67\", #green apple\n      avg_temp &gt;= 54 & avg_temp &lt; 61 ~ \"54-60\", #porcelain blue\n      avg_temp &gt;= 47 & avg_temp &lt; 54 ~ \"47-53\", #teal\n      avg_temp &gt;= 40 & avg_temp &lt; 47 ~ \"40-46\", #alaskan blue\n      avg_temp &gt;= 33 & avg_temp &lt; 40 ~ \"33-39\", #cobalt\n      avg_temp &gt;= 26 & avg_temp &lt; 33 ~ \"26-32\", #thistle\n      avg_temp &lt; 26 ~ \"Below 26\" #purple\n    ),\n    month_name = case_when(\n      month == \"01\" ~ \"Jan\",\n      month == \"02\" ~ \"Feb\",\n      month == \"03\" ~ \"Mar\",\n      month == \"04\" ~ \"Apr\",\n      month == \"05\" ~ \"May\",\n      month == \"06\" ~ \"Jun\",\n      month == \"07\" ~ \"Jul\",\n      month == \"08\" ~ \"Aug\",\n      month == \"09\" ~ \"Sept\",\n      month == \"10\" ~ \"Oct\",\n      month == \"11\" ~ \"Nov\",\n      month == \"12\" ~ \"Dec\"\n    ),\n    across(\n      c(\n        temp_color,\n        month_name\n      ),\n      ~as.factor(.x)\n    ),\n    temp_color = fct_relevel(\n      temp_color,\n      \"96+\",\n      \"89-95\",\n      \"82-88\",\n      \"75-81\",\n      \"68-74\",\n      \"61-67\",\n      \"54-60\",\n      \"47-53\",\n      \"40-46\",\n      \"33-39\",\n      \"26-32\",\n      \"Below 26\"\n    )\n  ) |&gt;\n  rowid_to_column()\n\n\n\n\nBlanket With New Color Scale\nThis is now how the full blanket for both 2003 and 2023. I still need to clean up the axis labels, the legend, and the facet titles.\n\n\nCode\ntemp |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\n\n\nDay One Only\nI was thinking about how to show the progress of the blanket. I decided to use the dplyr::first() function, which was a first for me. Here I have the first day of the year with the temperature range values for 2003 and 2023.\n\n\nCode\ntemp |&gt;\n  group_by(year) |&gt;\n  mutate(\n    first = first(rowid)\n  ) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    )\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  NULL\n\n\n\n\n\n\n\n\n\n\n\nPrepping the Loop\nNext, I decided to do a quick loop where I used two different seq() functions. The first sequence was to creatte a row for each day of the year from 1 to 365 by each day. I was able to create rows called year_row for both years. There is probably a better way of doing this but I decided on using a filter() where I looped through each day and put that each day was less than the year_row column created. For instance, in the code chunk below you can see it as filter(year_row &lt; .x). So when the loop starts at 2 it will filter for values that are lower than 2, with the only value being 1. This will be important later on because I am not sure if I want to present a story of the weather and a creation of my blanket for every single day or if there is another metric to go by.\n\n\nCode\nmap(\n  seq(2, 10, 1),\n  ~temp |&gt;\n  select(\n    rowid,\n    date,\n    year,\n    temp_color,\n    avg_temp\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n\nLooping Through Every 30 Days\nI ended up deciding on showing my blanket every week. I’m still not sure about it since there are are 52 plots to look through. Also, now that I think about it, maybe LA was not the best place to showcase temperature for a blanket. Looking at the final blanket plot, it screams “Sorry its always nice here!” and that might not be the most interesting for a temperature blanket. I’ll have to start looking at other locations for better blanket designs. At least now I have the basic design of what I’ll do for the competition.\n\n\nCode\nmap(\n  seq(2, 365, 30),\n  ~temp |&gt;\n  select(\n    rowid,\n    date,\n    year,\n    temp_color,\n    avg_temp\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    year_row = seq(1, 365, 1)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year_row &lt; .x) |&gt;\n  ggplot(\n    aes(\n      date,\n      1\n    )\n  ) +\n  geom_tile(\n    aes(\n      fill = temp_color\n    ),\n    color = \"white\"\n  ) +\n  coord_flip() +\n  facet_wrap(\n    ~year,\n    scales = \"free\"\n  ) +\n  labs(\n    fill = \"Temperatures\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"96+\" = \"#D2042D\",\n      \"89-95\" = \"#C41E3A\",\n      \"82-88\" = \"#ED9121\",\n      \"75-81\" = \"#FFFF99\",\n      \"68-74\" = \"#FFD700\",\n      \"61-67\" = \"#7CFC00\",\n      \"54-60\" = \"#AFDBF5\",\n      \"47-53\" = \"#008080\",\n      \"40-46\" = \"#A2C2E0\",\n      \"33-39\" = \"#0047AB\",\n      \"26-32\" = \"#D8BFD8\",\n      \"Below 26\" = \"#800080\"\n    )\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0)\n  ) +\n  scale_x_date(\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title = element_blank(),\n    strip.background = element_rect(fill = NA),\n    strip.text = element_text(size = 18),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(color = \"black\"),\n    legend.position = \"bottom\"\n  ) +\n  NULL\n)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]"
  },
  {
    "objectID": "posts/2024-11-14-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-11-14-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Photo by Nan Zhou on Unsplash\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table &lt;- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n\n\nAs mentioned in the previous post, the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the cmdstanr package to call on Stan for the Bayesian analyses, I am using the posterior package to manipulate the chains, iterations, and draws from the analyses and the bayesplot package to visualize the convergence of each parameter included in the bayes net model. I’m also using the reactable package to showcase the parameters for the model.\n\nData Creation\n\n\nCode\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\n\nCode\nstan_file &lt;- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The J, I, K, and C list values are all important for looping through:\n\nJ = The number of rows of data; in this case there are 30 “students”\nI = The number of columns in the dataset; which is 15 excluding the first column\nK = The number of latent attributes/skills\nC = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from y for the actual data and then X in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\n\nCode\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"posts/2024-11-14-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n\nfit &lt;- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n\n# fit$save_object(\"simple_bayes_net.RDS\")\n\n\nThis next part will be different depending on whether or not you are using RStan or like in this case cmdstanR. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For cmdstanR, you call on your Stan file. Below is the Stan code or if you’d like to see it side-by-side, the Stan file can be found here. I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations.\n\n\nCode\n\"\ndata {\n  int&lt;lower=1&gt; J; // number of examinees\n  int&lt;lower=1&gt; I; // number of items\n  int&lt;lower=1&gt; K; // number of latent variables\n  int&lt;lower=1&gt; C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\nparameters {\n  simplex[C] nu; // class probabilities\n  vector&lt;lower=0, upper=1&gt;[I] false_pos;\n  vector&lt;lower=0, upper=1&gt;[I] true_pos;\n  real&lt;lower=0, upper=1&gt; lambda1;\n  real&lt;lower=0, upper=1&gt; lambda20;\n  real&lt;lower=0, upper=1&gt; lambda21;\n  real&lt;lower=0, upper=1&gt; lambda30;\n  real&lt;lower=0, upper=1&gt; lambda31;\n}\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] &gt; 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] &gt; 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] &gt; 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n\n\nLooking over the Stan code, there is a lot here. I’ll break down each section, but will not be spending an extensive amount of time for each.\n\n\nCode\n\"\ndata {\n  int&lt;lower=1&gt; J; // number of examinees\n  int&lt;lower=1&gt; I; // number of items\n  int&lt;lower=1&gt; K; // number of latent variables\n  int&lt;lower=1&gt; C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\n\"\n\n\nThe data section of stan code is including what you called the components of the stan_filelist object. If you deviate from what you named the components in your list, then your model will show an error. While not entirely necessary, you may want to put constraints on these values. For instance, I know that I have more than 1 student, item, latent variable, and class, so I will put a constraint that the lowest possible value is 1.\n\n\nCode\n\"\nparameters {\n  simplex[C] nu; // class probabilities\n  vector&lt;lower=0, upper=1&gt;[I] false_pos;\n  vector&lt;lower=0, upper=1&gt;[I] true_pos;\n  real&lt;lower=0, upper=1&gt; lambda1;\n  real&lt;lower=0, upper=1&gt; lambda20;\n  real&lt;lower=0, upper=1&gt; lambda21;\n  real&lt;lower=0, upper=1&gt; lambda30;\n  real&lt;lower=0, upper=1&gt; lambda31;\n}\n\"\n\n\nThe parameters section includes any parameters that are being included in your model. For instance, if creating a Bayesian linear regression, you would include the alpha and beta parameters in this section. For these models, I have the class probabilities for each latent class (to read more about the simplex function see here). Then I will have the probabilities of a student being either a true or false positive mastery case for the latent classes. These are vectors due to there being a true and false positive parameter for each item. The last parameters are the lambda parameters, which are the probabilities for mastery of the three latent attributes. These often require expert domain knowledge to specify informative priors.\n\n\nCode\n\"\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] &gt; 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] &gt; 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] &gt; 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\n\"\n\n\nWhile this section is optional, I like to include it because I use this section to do many of my calculations. For instance, in this section I like to use the prior lambda values to get the log probabilities of theta_log values, which are the log probabilities based on the level of mastery from the lambda values. I looped through the latent classes so when a latent class’ value is 1, then it takes the greater log probability, and when the value is 0, then it takes the lower log probability. I also did my delta calculations in this section. The delta calculation takes theta values based on the latent classes values and it uses the Q-matrix for each item. Then by multiplying the theta values raised to the power of the Q-matrix gets the probability of mastery for each item within each latent class. This value indicates whether a given student will have mastery over all of the latent attributes.\n\n\nCode\n\"\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\n\"\n\n\nFor the model section, which is necessary, I always start with declaring any new variables, followed by priors for my lambda values and the true and false positive probabilities for each item. Lastly, this section is always where you will do your calculations for each item and for each latent class. Finally, the target calculation at the end is for the target log density.\n\n\nCode\n\"\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); \n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n\n\nThe last section, the generated quantities, is “generate additional quantities of interest from a fitted model without re-running the sampler” (Stan). For this series, I am using this section to calculate posterior probabilities, such as the probability of a student being in a specific latent class and the probability that students have mastered the attributes.\n\n\nCode\nfit &lt;- read_rds(here::here(\"posts/2024-11-14-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9376389 0.9111992 0.8754096 0.9844359\n\n\nCode\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |&gt; arrange(desc(rhat)) |&gt; head()\n\n\n# A tibble: 6 × 4\n  variable               rhat ess_bulk ess_tail\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 prob_resp_class[12,8]  1.00    3869.    4877.\n2 nu[5]                  1.00    7371.    5381.\n3 log_nu[5]              1.00    7371.    5381.\n4 prob_resp_class[5,8]   1.00    5686.    5902.\n5 prob_resp_class[18,8]  1.00    3686.    4569.\n6 prob_resp_class[27,5]  1.00    5974.    5744.\n\n\nCode\nbn_measure |&gt; mutate(across(-variable, ~round(.x, 3))) |&gt; react_table()\n\n\n\n\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\n\nCode\nbn_measure |&gt; \n  mutate(across(-variable, ~round(.x, 3))) |&gt; \n  filter(str_detect(variable, \"prob_resp_attr\")) |&gt;\n  react_table()\n\n\n\n\n\n\nI decided to filter in on the probabilities for students to have mastery over the attributes. The first index in the square brackets indicates the student and then the second index value indicates the three attributes. Obviously for something more thought out this would line up for meaningful attributes, but for this example, the values align with arbitrary values.\n\n\nCode\ny_rep &lt;- fit$draws(\"x_rep\") |&gt; as_draws_matrix()\nstu_resp_attr &lt;- fit$draws(\"prob_resp_attr\") |&gt; as_draws_matrix()\n\n\nI decided to extract the replicated values for the items and the probabilities of each student’s mastery of each of the three latent attributes.\n\n\nCode\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n\n\n\n\n\n\n\n\n\nCode\ny |&gt; react_table()\n\n\n\n\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the original data, the original responses and the probabilities with a probability threshold of 0.5 match one another.\n\n\nCode\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\n\n\nCode\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\n\n\nCode\nppc_intervals(\n  y = y |&gt; pull(y1) |&gt; as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n\n\n\n\n\n\n\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\n\nCode\nactual_stu_resp_attr &lt;- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |&gt;\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.\n\n\nCode\nstu_resp_attr_mean &lt;- stu_resp_attr |&gt;\n  as_tibble() |&gt;\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_mean |&gt;\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(\n    everything()\n  ) |&gt;\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |&gt;\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |&gt;\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute.\n\n\nCode\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n\n\n$att1\n   .y\n.x   0  1\n  0  6  5\n  1  7 12\n\n$att2\n   .y\n.x   0  1\n  0  6  8\n  1  5 11\n\n$att3\n   .y\n.x   0  1\n  0  4  2\n  1 11 13\n\n\nCode\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n\n\n$att1\n   .y\n.x          0         1\n  0 0.2000000 0.1666667\n  1 0.2333333 0.4000000\n\n$att2\n   .y\n.x          0         1\n  0 0.2000000 0.2666667\n  1 0.1666667 0.3666667\n\n$att3\n   .y\n.x           0          1\n  0 0.13333333 0.06666667\n  1 0.36666667 0.43333333\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model.\n\n\nCode\nstu_resp_attr_long &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long &lt;- actual_stu_resp_attr |&gt;\n  pivot_longer(-studentid)\n\naccuracy_att &lt;- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n\n\n[1] 0.5777778\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 58%. This is a good starting point, but this may indicate that the model needs better defined priors and may require the edges between the attributes to show latent relationships. The low accuracy value may also be indicative of the importance of domain knowledge in building a latent bayes net."
  },
  {
    "objectID": "posts/2024-11-14-bayes-net-introduction/index.html",
    "href": "posts/2024-11-14-bayes-net-introduction/index.html",
    "title": "Bayes Nets",
    "section": "",
    "text": "I will be the first to state that I am not an expert in the field of conducting psychometric models, Bayesian networks, Bayesian analyses, but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses Stan. The purpose of this post is to walk through Stan and some bayes net terminology to get a basic understanding of some psychometric models conducted using Bayesian inference.\nTo get started, make sure you follow the detailed instructions on installing RStan. I know if using Mac, make sure to also download Xcode so that Stan will work correctly. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using PyStan, Cmdstanpy, or PyMC, but there are just more readily available tools using R so I will be using R instead. I’m also creating some data to be used in the following posts on latent bayes nets. For these posts, I’ll be creating binary data that will represent items for an education assessment where a 1 indicates that a student has answered the item correctly and a 0 indicates they did not answer the item correctly. The model will also include three latent attributes/skills/variables where a 1 would indicate that the student has mastered the skill and a 0 would indicate that they do not have mastery of the skill.\nWhile I will be discussing bayes net through an educational measurement lens, bayes net can be used outside of education to show that individuals have skills that are not directly measured. Instead of items on an assessment, tasks that capture each skill can be assessed. Before walking through some bayes net terminology, it is important to note that this model is simply for educational purposes. Components of the psychometric models I will be writing about require expert opinion and domain knowledge. For example, bayes net models require expert opinions on the assignment of items to skills. Additionally, bayes net models require expert opinion on the priors for the lambda (\\(\\lambda\\)) parameters.\nSince there is different opinions on using different terms, I am going to stick to the following terms.\nFor this introductory post into bayes net, I thought it would be best to create some artificial data and show visually the models I will be planning on creating using R and Stan. I will be using cmdstanr instead of rstan for my Stan computations. The main difference between the two packages is that rstan avoids using R6 classes, while cmdstanr uses R6 classes. If you’d like more information on trade-offs of different object-oriented programming classes, you can read more here. Finally, I will state that while this is introductory to a bayes net model, this post assumes that you have a basic understanding of Bayesian inference."
  },
  {
    "objectID": "posts/2024-11-14-bayes-net-introduction/index.html#q-matrix",
    "href": "posts/2024-11-14-bayes-net-introduction/index.html#q-matrix",
    "title": "Bayes Nets",
    "section": "Q Matrix",
    "text": "Q Matrix\n\n\nCode\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  react_table()\n\n\n\n\n\n\nOkay, now on to the Q-matrix. As previously stated, I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column."
  },
  {
    "objectID": "posts/2024-11-14-bayes-net-introduction/index.html#attribute-profile-matrix",
    "href": "posts/2024-11-14-bayes-net-introduction/index.html#attribute-profile-matrix",
    "title": "Bayes Nets",
    "section": "Attribute Profile Matrix",
    "text": "Attribute Profile Matrix\nIf we only wanted to examine how the posterior distributions compare to each student and their responses, then I would only need to have my student data and the Q-matrix. However, I also want to put students into latent classes. Because of this, I also have to create an attribute profile matrix. I am going to create this matrix by creating every possible combination of skills, which will create every potential latent class. Then I will just add each row as a numbered class. Below is the final matrix created for 3 skills.\n\n\nCode\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\nalpha |&gt; react_table()\n\n\n\n\n\n\nNote: Latent classes are different from our latent nodes/attributes/skills. The matrix created above (alpha) is a matrix where each row is a different latent class and each column corresponds to each of the skills.\nSo now we have everything to build our bayes net model. Before we get to that, I do want to visually show the models I will be creating in this series."
  },
  {
    "objectID": "posts/2024-11-14-bayes-net-introduction/index.html#naive-bayes",
    "href": "posts/2024-11-14-bayes-net-introduction/index.html#naive-bayes",
    "title": "Bayes Nets",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\nCode\nnaive_dag &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - L1\" [latent,pos=\"0.175,0.076\"]\n\"Q\" [pos=\"0.874,0.402\"]\nAtt1 [latent,pos=\"0.220,0.209\"]\nAtt2 [latent,pos=\"0.488,0.182\"]\nAtt3 [latent,pos=\"0.709,0.169\"]\nD [latent,pos=\"0.481,0.421\"]\nfp [latent,pos=\"0.572,0.888\"]\nL1 [latent,pos=\"0.252,0.082\"]\nL20 [latent,pos=\"0.450,0.076\"]\nL21 [latent,pos=\"0.522,0.081\"]\nL30 [latent,pos=\"0.679,0.068\"]\nL31 [latent,pos=\"0.741,0.069\"]\ntp [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - L1\" -&gt; Att1\n\"Q\" -&gt; D\nAtt1 -&gt; D\nAtt2 -&gt; D\nAtt3 -&gt; D\nD -&gt; y1\nD -&gt; y10\nD -&gt; y11\nD -&gt; y12\nD -&gt; y13\nD -&gt; y14\nD -&gt; y15\nD -&gt; y2\nD -&gt; y3\nD -&gt; y4\nD -&gt; y5\nD -&gt; y6\nD -&gt; y7\nD -&gt; y8\nD -&gt; y9\nfp -&gt; y1\nfp -&gt; y10\nfp -&gt; y11\nfp -&gt; y12\nfp -&gt; y13\nfp -&gt; y14\nfp -&gt; y15\nfp -&gt; y2\nfp -&gt; y3\nfp -&gt; y4\nfp -&gt; y5\nfp -&gt; y6\nfp -&gt; y7\nfp -&gt; y8\nfp -&gt; y9\nL1 -&gt; Att1\nL20 -&gt; aAtt2\nL21 -&gt; aAtt2\nL30 -&gt; aAtt3\nL31 -&gt; Att3\ntp -&gt; y1\ntp -&gt; y10\ntp -&gt; y11\ntp -&gt; y12\ntp -&gt; y13\ntp -&gt; y14\ntp -&gt; y15\ntp -&gt; y2\ntp -&gt; y3\ntp -&gt; y4\ntp -&gt; y5\ntp -&gt; y6\ntp -&gt; y7\ntp -&gt; y8\ntp -&gt; y9\n}\n')\n\nggdag(naive_dag) + theme_dag()\n\n\n\n\n\n\n\n\n\n\nTP = True Positive\nFP = False Positive\nQ = Q-matrix\nD = Delta\nL = Lambda\nAtt = Latent Attribute\n\nThe first model I will go over is a naive bayes model; however, naive bayes models do not correct for what I have labeled as true positive and false positive probabilities. This model also mimic a deterministic inputs, noisy “and” gate (DINA) model. Essentially, the model assumes that each student has mastered all skills in order to correctly respond to an assessment item. See here for an excellent post about the DINA model."
  },
  {
    "objectID": "posts/2024-11-14-bayes-net-introduction/index.html#bayes-net",
    "href": "posts/2024-11-14-bayes-net-introduction/index.html#bayes-net",
    "title": "Bayes Nets",
    "section": "Bayes Net",
    "text": "Bayes Net\n\n\nCode\nbayes_net &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - L1\" [latent,pos=\"0.175,0.076\"]\n\"Q\" [pos=\"0.874,0.402\"]\nAtt1 [latent,pos=\"0.220,0.209\"]\nAtt2 [latent,pos=\"0.488,0.182\"]\nAtt3 [latent,pos=\"0.709,0.169\"]\nD [latent,pos=\"0.481,0.421\"]\nfp [latent,pos=\"0.572,0.888\"]\nL1 [latent,pos=\"0.252,0.082\"]\nL20 [latent,pos=\"0.450,0.076\"]\nL21 [latent,pos=\"0.522,0.081\"]\nL30 [latent,pos=\"0.679,0.068\"]\nL31 [latent,pos=\"0.741,0.069\"]\nfp [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - L1\" -&gt; Att1\n\"Q\" -&gt; D\nAtt1 -&gt; Att2\nAtt1 -&gt; D\nAtt2 -&gt; Att3\nAtt2 -&gt; D\nAtt3 -&gt; D\nD -&gt; y1\nD -&gt; y10\nD -&gt; y11\nD -&gt; y12\nD -&gt; y13\nD -&gt; y14\nD -&gt; y15\nD -&gt; y2\nD -&gt; y3\nD -&gt; y4\nD -&gt; y5\nD -&gt; y6\nD -&gt; y7\nD -&gt; y8\nD -&gt; y9\nfp -&gt; y1\nfp -&gt; y10\nfp -&gt; y11\nfp -&gt; y12\nfp -&gt; y13\nfp -&gt; y14\nfp -&gt; y15\nfp -&gt; y2\nfp -&gt; y3\nfp -&gt; y4\nfp -&gt; y5\nfp -&gt; y6\nfp -&gt; y7\nfp -&gt; y8\nfp -&gt; y9\nL1 -&gt; Att1\nL20 -&gt; Att2\nL21 -&gt; Att2\nL30 -&gt; Att3\nL31 -&gt; Att3\ntp -&gt; y1\ntp -&gt; y10\ntp -&gt; y11\ntp -&gt; y12\ntp -&gt; y13\ntp -&gt; y14\ntp -&gt; y15\ntp -&gt; y2\ntp -&gt; y3\ntp -&gt; y4\ntp -&gt; y5\ntp -&gt; y6\ntp -&gt; y7\ntp -&gt; y8\ntp -&gt; y9\n}\n')\n\nggdag(bayes_net) + theme_dag()\n\n\n\n\n\n\n\n\n\nThe second model is a bayes net model that looks very similar to the first model. Now there are edges between the three latent nodes, where depending on whether a student has the previous skill, the probability differs for having the following skill. In the next post I will be estimating the first bayes net model and doing some posterior checks to see how the model works."
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-us-coffee-tasting/index.html",
    "href": "posts/2024-11-11-bayes-net-us-coffee-tasting/index.html",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "",
    "text": "Photo by Najib Kalil on Unsplash\nFor this post, I wanted to include some additional analyses to the descriptive analyses from this video. Specifically, I thought this would be a good opportunity to create a bayesian network for inference. The data can be found here. There is no documentation, but each item is coded as the full question so it is easier to follow. This analysis is strictly to walk through using the bnlearn package for a fun dataset. Because the participants are subscribers to the James Hoffmann youtube channel that are located in the United States, the sample could be seen as representative of James’ US subscribers."
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-us-coffee-tasting/index.html#using-bnlearn",
    "href": "posts/2024-11-11-bayes-net-us-coffee-tasting/index.html#using-bnlearn",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Using Bnlearn",
    "text": "Using Bnlearn\n\n\nCode\n# Building Bayesian Network\ndag &lt;- empty.graph(nodes = colnames(no_fact))\n\narcs &lt;- matrix(\n  c(\"gender\", \"cup_per_day\",\n    \"gender\", \"favorite_coffee_drink\",\n    \"gender\", \"home_brew_pour_over\",\n    \"gender\", \"home_brew_french_press\",\n    \"gender\", \"home_brew_espresso\",\n    \"gender\", \"home_brew_mr_coffee\",\n    \"gender\", \"home_brew_pods\",\n    \"gender\", \"home_brew_instant\",\n    \"gender\", \"home_brew_bean2cup\",\n    \"gender\", \"home_brew_cold_brew\",\n    \"gender\", \"home_brew_cometeer\",\n    \"gender\", \"home_brew_other\",\n    \"age\", \"cup_per_day\",\n    \"age\", \"favorite_coffee_drink\",\n    \"age\", \"home_brew_pour_over\",\n    \"age\", \"home_brew_french_press\",\n    \"age\", \"home_brew_espresso\",\n    \"age\", \"home_brew_mr_coffee\",\n    \"age\", \"home_brew_pods\",\n    \"age\", \"home_brew_instant\",\n    \"age\", \"home_brew_bean2cup\",\n    \"age\", \"home_brew_cold_brew\",\n    \"age\", \"home_brew_cometeer\",\n    \"age\", \"home_brew_other\",\n\n    \"cup_per_day\", \"roast_preference\",\n    \"favorite_coffee_drink\", \"roast_preference\",\n    \"home_brew_pour_over\", \"roast_preference\",\n    \"home_brew_french_press\", \"roast_preference\",\n    \"home_brew_espresso\", \"roast_preference\",\n    \"home_brew_mr_coffee\", \"roast_preference\",\n    \"home_brew_pods\", \"roast_preference\",\n    \"home_brew_instant\", \"roast_preference\",\n    \"home_brew_bean2cup\", \"roast_preference\",\n    \"home_brew_cold_brew\", \"roast_preference\",\n    \"home_brew_cometeer\", \"roast_preference\",\n    \"home_brew_other\", \"roast_preference\",\n\n    \"roast_preference\", \"expertise\",\n    \"roast_preference\", \"favorite_abcd\",\n    \"expertise\", \"favorite_abcd\"),\n  byrow = TRUE,\n  ncol = 2,\n  dimnames = list(NULL, c(\"from\", \"to\"))\n)\n\narcs(dag) &lt;- arcs\n\ngraphviz.plot(dag)\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(12345)\ndag_fit &lt;- bn.fit(dag, data = no_fact, method = \"bayes\", iss = 5000)\n\n\n\nGender - Conditional Probability Table (CPT)\n\n\nCode\ntibble(\n  Gender = attributes(dag_fit$gender$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$gender$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\n\nAge - CPT\n\n\nCode\ntibble(\n  Age = attributes(dag_fit$age$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$age$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\n\nCups of Coffee Per Day - CPT\n\n\nCode\ndag_fit$cup_per_day$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = cup_per_day, y = n, fill = gender, facet = age) +\n  labs(\n    title = \"Probability of Cups of Coffee Per Day From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Pourover\n\n\nCode\ndag_fit$home_brew_pour_over$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pour_over, facet = age) +\n  labs(\n    title = \"Probability of Making Pourover at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - French Press\n\n\nCode\ndag_fit$home_brew_french_press$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_french_press, facet = age) +\n  labs(\n    title = \"Probability of Making French Press at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Espresso\n\n\nCode\ndag_fit$home_brew_espresso$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_espresso, facet = age) +\n  labs(\n    title = \"Probability of Making Espresso at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - coffee Brewing Machine\n\n\nCode\ndag_fit$home_brew_mr_coffee$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_mr_coffee, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Coffee Brewing Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Pods\n\n\nCode\ndag_fit$home_brew_pods$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pods, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using Pods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Instant Coffee\n\n\nCode\ndag_fit$home_brew_instant$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_instant, facet = age) +\n  labs(\n    title = \"Probability of Making Instant Coffee at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Bean 2 Cup\n\n\nCode\ndag_fit$home_brew_bean2cup$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_bean2cup, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Bean 2 Cup Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Cold Brew\n\n\nCode\ndag_fit$home_brew_cold_brew$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cold_brew, facet = age) +\n  labs(\n    title = \"Probability of Making Cold Brew at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Cometeer\n\n\nCode\ndag_fit$home_brew_cometeer$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cometeer, facet = age) +\n  labs(\n    title = \"Probability of Making Cometeer Coffees at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nHome Brewing - Other\n\n\nCode\ndag_fit$home_brew_other$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_other, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee From Other Methods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nFavorite Coffee Drink - CPT\n\n\nCode\ndag_fit$favorite_coffee_drink$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(\n    x = favorite_coffee_drink,\n    y = n,\n    fill = gender,\n    facet = age\n  ) +\n  labs(\n    title = \"Probability of Favorite Coffee Drinks From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nRoast Preference - CPT\n\n\nCode\nroast_pref_func &lt;- function(\n  dag_table,\n  x,\n  y = n,\n  fill,\n  facet_x,\n  facet_y\n){\n  {{dag_table}} |&gt;\n  as_tibble() |&gt;\n  mutate(\n    fill = str_to_title({{fill}}),\n    n = round(n, 2),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    x = fct_relevel(\n      {{x}},\n      \"three_or_more\",\n      \"2\",\n      \"one_or_less\"\n    ),\n    fill = fct_relevel(\n      fill,\n      \"Dark\",\n      \"Medium\",\n      \"Light\"\n    )\n  ) |&gt;\n  ggplot(\n    aes(\n      x,\n      {{y}}\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = fill\n      )\n  ) +\n  coord_flip() +\n  facet_grid(\n    vars({{facet_x}}),\n    vars({{facet_y}})\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  guides(\n    fill = guide_legend(\n      reverse = TRUE\n      )\n    )\n}\n\n\nThe CPT here only include the probabilities for whether participants used one home brewer or not and considered all of the other home brewers as not using those at home.\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pour_over\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Pourover\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_french_press\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a French Press\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_espresso\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Espresso at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_mr_coffee\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Coffee Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pods\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Pods\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_instant\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Instant Coffee\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_bean2cup\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Bean 2 Cup Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cold_brew\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Cold Brew at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cometeer\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Cometeer\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_other\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using an Other Method\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nExpertise Level - CPT\n\n\nCode\nexpertise_tbl &lt;- dag_fit$expertise$prob |&gt; as_tibble() |&gt;\n  mutate(\n    roast_preference = str_to_title(roast_preference),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"Light\",\n      \"Medium\",\n      \"Dark\"\n    )\n  ) \n\nexpertise_tbl |&gt;\n  ggplot(\n    aes(\n      roast_preference,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = expertise\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = expertise_tbl |&gt; filter(roast_preference == \"Light\"),\n    aes(\n      label = expertise,\n      group = expertise,\n      color = expertise\n    ),\n    position = position_dodge(width = .9),\n    vjust = -.5\n  ) +\n   labs(\n    title = \"Probability of One's Roast Preference From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level\",\n    x = \"\",\n    y = \"Probability\",\n    caption = \"Note: Probabilities range from 0 to 1. The scale is reduced to visually compare groups.\"\n  ) +\n  viridis::scale_color_viridis(\n    discrete = TRUE\n    ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  scale_x_discrete(\n    expand = c(0, .5)\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text = element_text(\n      color = \"black\"\n    ),\n    axis.title = element_text(\n      color = \"black\"\n    ),\n    plot.title = element_text(\n      color = \"black\"\n    ),\n    plot.subtitle = element_text(\n      color = \"black\"\n    ),\n    plot.caption = element_text(\n      color = \"black\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nFavorite Coffee (A, B, C, D) - CPT\n\n\nCode\nfavorite_abcd_prob &lt;- dag_fit$favorite_abcd$prob |&gt; as_tibble() |&gt;\n  mutate(\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"light\",\n      \"medium\",\n      \"dark\"\n    ),\n    favorite_abcd = fct_relevel(\n      favorite_abcd,\n      \"Coffee A\",\n      \"Coffee B\",\n      \"Coffee C\",\n      \"Coffee D\"\n    )\n  )\n\n# scales::show_col(viridis::viridis_pal(option = \"E\")(3))\n\nfavorite_abcd_prob |&gt;\n  ggplot(\n    aes(\n      favorite_abcd,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = roast_preference\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee D\" &\n      roast_preference == \"light\"\n    ),\n    label = \"Light\\nRoast\",\n    nudge_y = .03,\n    color = \"#00204DFF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee C\" &\n      roast_preference == \"medium\"\n    ),\n    label = \"Medium\\nRoast\",\n    nudge_y = .03,\n    color = \"#7C7B78FF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee B\" &\n      roast_preference == \"dark\"\n    ),\n    label = \"Dark\\nRoast\",\n    nudge_y = .03,\n    color = \"#FFEA46FF\"\n  ) +\n  facet_wrap(\n    ~expertise,\n    ncol = 5\n  ) +\n  scale_y_continuous(\n    breaks = seq(.1, .6, .1)\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE,\n    option = \"cividis\"\n  ) +\n  labs(\n    title = \"Probability of One's Favorite Coffees From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level & Roast Level Preference\",\n    x = \"\",\n    y = \"Probability\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    strip.background = element_rect(\n      fill = \"#7C7B78FF\"\n    ),\n    axis.text = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    axis.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.subtitle = element_text(\n      color = \"#7C7B78FF\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nscore(\n  dag,\n  data = no_fact, \n  type = \"bde\",\n  iss = 5000\n)\n\n\n[1] -49810.62\n\n\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\"),\n  evidence = (gender == \"Male\")\n)\n\n\n[1] 0.2386035\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\"),\n  evidence = (gender == \"Female\")\n)\n\n\n[1] 0.228005\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Male\")\n)\n\n\n[1] 0.302377\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Female\")\n)\n\n\n[1] 0.2890071\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Male\")\n)\n\n\n[1] 0.5395322\n\n\nCode\ncpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (gender == \"Female\")\n)\n\n\n[1] 0.5265312"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "href": "posts/2024-11-11-bayes-net-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Probabilities of Coffee A or Coffee D Based on Expertise Level",
    "text": "Probabilities of Coffee A or Coffee D Based on Expertise Level\n\n\nCode\nexpert1 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"1\")\n)\nexpert2 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"2\")\n)\nexpert3 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"3\")\n)\nexpert4 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"4\")\n)\nexpert5 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"5\")\n)\nexpert6 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"6\")\n)\nexpert7 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"7\")\n)\nexpert8 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"8\")\n)\nexpert9 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"9\")\n)\nexpert10 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"10\")\n)\n\ntibble(\n  expertise_level = seq(1, 10, 1),\n  probability_of_a_or_d = c(expert1, expert2, expert3, expert4, expert5, expert6, expert7, expert8, expert9, expert10)\n) |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise_level),\n      probability_of_a_or_d\n    )\n  ) +\n  geom_col(\n    aes(fill = as.factor(expertise_level)),\n    position = position_dodge()\n  ) +\n  geom_text(\n    aes(\n      label = round(probability_of_a_or_d, 2)\n    ),\n    color = \"black\",\n    vjust = -.3\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, .1)\n  ) +\n  labs(\n    title = \"Probability of Choosing Coffee A or D as Their Favorite Coffee\",\n    subtitle = \"By Level of Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Probability\"\n  ) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n  theme(\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "",
    "text": "Photo by Najib Kalil on Unsplash\nFor this post, I wanted to include some additional analyses to the descriptive analyses from this video. Specifically, I thought this would be a good opportunity to create a bayesian network for inference. The data can be found here. There is no documentation, but each item is coded as the full question so it is easier to follow. This analysis is strictly to walk through using the bnlearn package for a fun dataset. Because the participants are subscribers to the James Hoffmann Youtube channel that are located in the United States, the sample could be seen as representative of James’ US subscribers."
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-counts",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-counts",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Coffee Preference Counts",
    "text": "Coffee Preference Counts\nThe target variable of interest was participants’ favorite coffee out of the four samples that were provided in their testing kits. As stated in the linked video, Coffee D was the chosen as participants’ favorite coffee more than the other three options.\n\n\nCode\ncoffee_drop |&gt;\n  drop_na(favorite_abcd) |&gt;\n  ggplot(\n    aes(\n      favorite_abcd\n    )\n  ) +\n  geom_bar(\n    aes(\n      fill = favorite_abcd\n    )\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  labs(\n    x = \"Coffee Choices\",\n    y = \"Counts\",\n    title = \"Counts of Each Coffee\"\n  ) +\n  theme(\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-by-age-group",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-by-age-group",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Coffee Preference by Age Group",
    "text": "Coffee Preference by Age Group\nThis next visual shows the preference for each of the four coffees based on age brackets. We can see that the number of participants that rated coffee D as a 5 is highest for the 26-34 age bracket. Some basic descriptive statistics are provided in the following table. Tables can be filtered by typing in values in the box under the variable name. You can also click on the variable name and it will sort the values. Clicking the value again will sort in the opposite direction.\n\n\nCode\ncoffee_drop |&gt;\n  select(\n    age,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -age\n  ) |&gt;\n  group_by(\n    age,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    age = as.factor(age),\n    age = fct_relevel(\n      age,\n      \"&lt;18 years old\",\n      \"18-24 years old\",\n      \"25-34 years old\",\n      \"35-44 years old\",\n      \"45-54 years old\",\n      \"55-64 years old\",\n      \"&gt;65 years old\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  ggplot(\n    aes(\n      age,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    ),\n    axis.text.x = element_text(\n      angle = 45,\n      vjust = 0.5\n      )\n    ) +\n  NULL\n\n\n\n\n\n\n\n\n\nCode\ncoffee_drop |&gt;\n  select(\n    age,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -age\n  ) |&gt;\n  group_by(\n    age,\n    name\n  ) |&gt;\n  summarize(\n    across(\n      value,\n      list(\n        mean = ~mean(.x, na.rm = TRUE),\n        median = ~median(.x, na.rm = TRUE)\n      )\n    ),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(\n    name\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-by-gender",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-by-gender",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Coffee Preference by Gender",
    "text": "Coffee Preference by Gender\nWhen looking at the coffee preferences of different genders, males rated coffees A and D highly. It seems visually that Coffees A, B, and C are higher rated than than Coffee D. The other genders did not have a lot of data, so I also decided to get some basic descriptive statistics for each group. You can look further into the data by filtering by name (A, B, C, D) and by gender.\n\n\nCode\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    gender = as.factor(gender),\n    name = as.factor(name),\n    gender = fct_relevel(\n      gender,\n      \"Male\",\n      \"Female\",\n      \"Non-binary\",\n      \"Other (please specify)\",\n      \"Prefer not to say\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  ggplot(\n    aes(\n      gender,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    )\n    ) +\n  NULL\n\n\n\n\n\n\n\n\n\nCode\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  summarize(\n    across(\n      value,\n      list(\n        mean = ~mean(.x, na.rm = TRUE),\n        median = ~median(.x, na.rm = TRUE)\n      )\n    ),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(\n    name\n  ) |&gt;\n  mutate(\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\nIn addition to the table, I also included a plot for participants that identified as Non-binary, Other, or preferred not to say.\n\n\nCode\ncoffee_drop |&gt;\n  select(\n    gender,\n    matches(\n      \"personal_pref\"\n    )\n  ) |&gt;\n  pivot_longer(\n    -gender\n  ) |&gt;\n  group_by(\n    gender,\n    name\n  ) |&gt;\n  count(\n    value\n    ) |&gt;\n  mutate(\n    value = as.factor(value),\n    gender = as.factor(gender),\n    name = as.factor(name),\n    gender = fct_relevel(\n      gender,\n      \"Male\",\n      \"Female\",\n      \"Non-binary\",\n      \"Other (please specify)\",\n      \"Prefer not to say\"\n    ),\n    name = case_when(\n      name == \"coffee_a_personal_preference\" ~ \"Coffee A Preference\",\n      name == \"coffee_b_personal_preference\" ~ \"Coffee B Preference\",\n      name == \"coffee_c_personal_preference\" ~ \"Coffee C Preference\",\n      name == \"coffee_d_personal_preference\" ~ \"Coffee D Preference\"\n    )\n  ) |&gt; \n  drop_na() |&gt;\n  filter(\n    !gender %in% c(\"Male\", \"Female\")\n  ) |&gt;\n  ggplot(\n    aes(\n      gender,\n      n\n    )\n  ) +\n  geom_col(\n    position = position_dodge(),\n    aes(\n      fill = value\n    )\n  ) +\n  facet_wrap(\n    ~name\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  labs(\n    x = \"\",\n    y = \"Counts\",\n    fill = \"Rating\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    strip.background = element_rect(\n      fill = \"white\"\n      ),\n    strip.text = element_text(\n      color = \"black\"\n    )\n    ) +\n  NULL"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-by-expertise",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#coffee-preference-by-expertise",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Coffee Preference by Expertise",
    "text": "Coffee Preference by Expertise\nThis visual is similar to that from the video. I just included some colors to make the coffee preferences stand out a little more.\n\n\nCode\ncoffee_drop |&gt;\n  group_by(\n    favorite_abcd,\n    expertise\n  ) |&gt;\n  count() |&gt; \n  ungroup(\n    favorite_abcd\n    ) |&gt;\n  mutate(\n    percent = n/sum(n),\n    percent = percent*100\n  ) |&gt;\n  drop_na() |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise),\n      percent\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = as.factor(favorite_abcd)\n    )\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  labs(\n    title = \"Favorite Coffees By Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Percentage\",\n    fill = \"\"\n  ) +\n  NULL"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#using-bnlearn",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#using-bnlearn",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Using Bnlearn",
    "text": "Using Bnlearn\n\n\nCode\n# Building Bayesian Network\ndag &lt;- empty.graph(nodes = colnames(no_fact))\n\narcs &lt;- matrix(\n  c(\"gender\", \"cup_per_day\",\n    \"gender\", \"favorite_coffee_drink\",\n    \"gender\", \"home_brew_pour_over\",\n    \"gender\", \"home_brew_french_press\",\n    \"gender\", \"home_brew_espresso\",\n    \"gender\", \"home_brew_mr_coffee\",\n    \"gender\", \"home_brew_pods\",\n    \"gender\", \"home_brew_instant\",\n    \"gender\", \"home_brew_bean2cup\",\n    \"gender\", \"home_brew_cold_brew\",\n    \"gender\", \"home_brew_cometeer\",\n    \"gender\", \"home_brew_other\",\n    \"age\", \"cup_per_day\",\n    \"age\", \"favorite_coffee_drink\",\n    \"age\", \"home_brew_pour_over\",\n    \"age\", \"home_brew_french_press\",\n    \"age\", \"home_brew_espresso\",\n    \"age\", \"home_brew_mr_coffee\",\n    \"age\", \"home_brew_pods\",\n    \"age\", \"home_brew_instant\",\n    \"age\", \"home_brew_bean2cup\",\n    \"age\", \"home_brew_cold_brew\",\n    \"age\", \"home_brew_cometeer\",\n    \"age\", \"home_brew_other\",\n\n    \"cup_per_day\", \"roast_preference\",\n    \"favorite_coffee_drink\", \"roast_preference\",\n    \"home_brew_pour_over\", \"roast_preference\",\n    \"home_brew_french_press\", \"roast_preference\",\n    \"home_brew_espresso\", \"roast_preference\",\n    \"home_brew_mr_coffee\", \"roast_preference\",\n    \"home_brew_pods\", \"roast_preference\",\n    \"home_brew_instant\", \"roast_preference\",\n    \"home_brew_bean2cup\", \"roast_preference\",\n    \"home_brew_cold_brew\", \"roast_preference\",\n    \"home_brew_cometeer\", \"roast_preference\",\n    \"home_brew_other\", \"roast_preference\",\n\n    \"roast_preference\", \"expertise\",\n    \"roast_preference\", \"favorite_abcd\",\n    \"expertise\", \"favorite_abcd\"),\n  byrow = TRUE,\n  ncol = 2,\n  dimnames = list(NULL, c(\"from\", \"to\"))\n)\n\narcs(dag) &lt;- arcs\n\n# graphviz.plot(dag)\n\n\n\n\nCode\nset.seed(12345)\ndag_fit &lt;- bn.fit(dag, data = no_fact, method = \"bayes\", iss = 5)\n\n\n\nGender - Conditional Probability Table (CPT)\n\n\nCode\ntibble(\n  Gender = attributes(dag_fit$gender$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$gender$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\n\nAge - CPT\n\n\nCode\ntibble(\n  Age = attributes(dag_fit$age$prob)$dimnames[[1]],\n  Probability = round(array(dag_fit$age$prob), 2)\n) |&gt;\n  reactable()\n\n\n\n\n\n\n\n\nCups of Coffee Per Day - CPT\nBelow are the conditional probabilities for cups of coffee per day based on genders and age ranges.\n\n\nCode\ndag_fit$cup_per_day$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = cup_per_day, y = n, fill = gender, facet = age) +\n  labs(\n    title = \"Probability of Cups of Coffee Per Day From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$cup_per_day$prob |&gt;\n  as_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Pourover\nFor all of the following home brewing probability tables, the visuals will be based on genders and age ranges.\n\n\nCode\ndag_fit$home_brew_pour_over$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pour_over, facet = age) +\n  labs(\n    title = \"Probability of Making Pourover at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_pour_over$prob |&gt;\n  as_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - French Press\n\n\nCode\ndag_fit$home_brew_french_press$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_french_press, facet = age) +\n  labs(\n    title = \"Probability of Making French Press at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_french_press$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Espresso\n\n\nCode\ndag_fit$home_brew_espresso$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_espresso, facet = age) +\n  labs(\n    title = \"Probability of Making Espresso at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_espresso$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - coffee Brewing Machine\n\n\nCode\ndag_fit$home_brew_mr_coffee$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_mr_coffee, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Coffee Brewing Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_mr_coffee$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Pods\n\n\nCode\ndag_fit$home_brew_pods$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_pods, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using Pods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_pods$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Instant Coffee\n\n\nCode\ndag_fit$home_brew_instant$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_instant, facet = age) +\n  labs(\n    title = \"Probability of Making Instant Coffee at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_instant$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Bean 2 Cup\n\n\nCode\ndag_fit$home_brew_bean2cup$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_bean2cup, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee Using a Bean 2 Cup Machine at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_bean2cup$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Cold Brew\n\n\nCode\ndag_fit$home_brew_cold_brew$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cold_brew, facet = age) +\n  labs(\n    title = \"Probability of Making Cold Brew at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_cold_brew$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Cometeer\n\n\nCode\ndag_fit$home_brew_cometeer$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_cometeer, facet = age) +\n  labs(\n    title = \"Probability of Making Cometeer Coffees at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_cometeer$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nHome Brewing - Other\n\n\nCode\ndag_fit$home_brew_other$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(x = gender, y = n, fill = home_brew_other, facet = age) +\n  labs(\n    title = \"Probability of Making Coffee From Other Methods at Home From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$home_brew_other$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nFavorite Coffee Drink - CPT\nThese probabilities are for participants’ favorite coffee drinks based on their genders and age ranges.\n\n\nCode\ndag_fit$favorite_coffee_drink$prob |&gt;\n  as_tibble() |&gt;\n  mutate(\n    n = round(n, 2)\n  ) |&gt;\n  gg_func(\n    x = favorite_coffee_drink,\n    y = n,\n    fill = gender,\n    facet = age\n  ) +\n  labs(\n    title = \"Probability of Favorite Coffee Drinks From the Great American Tasting\",\n    subtitle = \"Based on Gender and Age Range\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$favorite_coffee_drink$prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nRoast Preference - CPT\n\n\nCode\nroast_pref_func &lt;- function(\n  dag_table,\n  x,\n  y = n,\n  fill,\n  facet_x,\n  facet_y\n){\n  {{dag_table}} |&gt;\n  as_tibble() |&gt;\n  mutate(\n    fill = str_to_title({{fill}}),\n    n = round(n, 2),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    x = fct_relevel(\n      {{x}},\n      \"three_or_more\",\n      \"2\",\n      \"one_or_less\"\n    ),\n    fill = fct_relevel(\n      fill,\n      \"Dark\",\n      \"Medium\",\n      \"Light\"\n    )\n  ) |&gt;\n  ggplot(\n    aes(\n      x,\n      {{y}}\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = fill\n      )\n  ) +\n  coord_flip() +\n  facet_grid(\n    vars({{facet_x}}),\n    vars({{facet_y}})\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n  ) +\n  guides(\n    fill = guide_legend(\n      reverse = TRUE\n      )\n    )\n}\n\n\nThe CPT here only include the probabilities for whether participants used one home brewer or not and considered all of the other home brewers as not using those at home. If I included all of them, these tables would be unbearably long. This post is already pretty long.\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pour_over\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Pourover\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_french_press\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a French Press\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_espresso\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Espresso at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_mr_coffee\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Coffee Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_pods\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Pods\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_instant\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Instant Coffee\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_bean2cup\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using a Bean 2 Cup Machine\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cold_brew\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew Cold Brew at Home\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_cometeer\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using Cometeer\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nCode\nroast_pref_func(\n  dag_table = dag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1:8],\n  x = cup_per_day,\n  y = n,\n  fill = roast_preference,\n  facet_x = favorite_coffee_drink,\n  facet_y = home_brew_other\n) +\n  labs(\n    title = \"Probability of Coffee Roast Preference From the Great American Tasting\",\n    subtitle = \"Based on Gender, Age, Cups Per Day, and Whether Participants Brew at Home Using an Other Method\",\n    x = \"\",\n    y = \"Probability\",\n    fill = \"\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nCode\ndag_fit$roast_preference$prob[1:3, 1:3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1:2, 1:8] |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nExpertise Level - CPT\nHere are the probabilities of expertise level based on coffee roast preference. Explanations are hard to be made for this because the probabilities do not differ by much based on roast level.\n\n\nCode\nexpertise_tbl &lt;- dag_fit$expertise$prob |&gt; as_tibble() |&gt;\n  mutate(\n    roast_preference = str_to_title(roast_preference),\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"Light\",\n      \"Medium\",\n      \"Dark\"\n    )\n  ) \n\nexpertise_tbl |&gt;\n  ggplot(\n    aes(\n      roast_preference,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = expertise\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = expertise_tbl |&gt; filter(roast_preference == \"Light\"),\n    aes(\n      label = expertise,\n      group = expertise,\n      color = expertise\n    ),\n    position = position_dodge(width = .9),\n    vjust = -.5\n  ) +\n   labs(\n    title = \"Probability of Self-Defined Expertise Level From The Great American Tasting\",\n    subtitle = \"Based on Coffee Roast Preference\",\n    x = \"\",\n    y = \"Probability\",\n    caption = \"Note: Probabilities range from 0 to 1. The scale is reduced to visually compare groups.\"\n  ) +\n  viridis::scale_color_viridis(\n    discrete = TRUE\n    ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE\n    ) +\n  scale_x_discrete(\n    expand = c(0, .5)\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text = element_text(\n      color = \"black\"\n    ),\n    axis.title = element_text(\n      color = \"black\"\n    ),\n    plot.title = element_text(\n      color = \"black\"\n    ),\n    plot.subtitle = element_text(\n      color = \"black\"\n    ),\n    plot.caption = element_text(\n      color = \"black\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nCode\nexpertise_tbl |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )\n\n\n\n\n\n\n\n\nFavorite Coffee (A, B, C, D) - CPT\nThere are a lot of probabilities for participants’ favorite coffee based on their self-defined expertise level and coffee roast preference. It may be easier to follow along with the table rather than the visual.\n\n\nCode\nfavorite_abcd_prob &lt;- dag_fit$favorite_abcd$prob |&gt; as_tibble() |&gt;\n  mutate(\n    across(\n      -n,\n      ~as.factor(.x)\n    ),\n    expertise = fct_relevel(\n      expertise,\n      \"1\",\n      \"2\",\n      \"3\",\n      \"4\",\n      \"5\",\n      \"6\",\n      \"7\",\n      \"8\",\n      \"9\",\n      \"10\"\n    ),\n    roast_preference = fct_relevel(\n      roast_preference,\n      \"light\",\n      \"medium\",\n      \"dark\"\n    ),\n    favorite_abcd = fct_relevel(\n      favorite_abcd,\n      \"Coffee A\",\n      \"Coffee B\",\n      \"Coffee C\",\n      \"Coffee D\"\n    )\n  )\n\n# scales::show_col(viridis::viridis_pal(option = \"E\")(3))\n\nfavorite_abcd_prob |&gt;\n  ggplot(\n    aes(\n      favorite_abcd,\n      n\n    )\n  ) +\n  geom_col(\n    aes(\n      fill = roast_preference\n    ),\n    position = position_dodge()\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee D\" &\n      roast_preference == \"light\"\n    ),\n    label = \"Light\\nRoast\",\n    nudge_y = .03,\n    color = \"#00204DFF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee C\" &\n      roast_preference == \"medium\"\n    ),\n    label = \"Medium\\nRoast\",\n    nudge_y = .03,\n    color = \"#7C7B78FF\"\n  ) +\n  geom_text(\n    data = favorite_abcd_prob |&gt;\n    filter(\n        expertise == \"4\" &\n      favorite_abcd == \"Coffee B\" &\n      roast_preference == \"dark\"\n    ),\n    label = \"Dark\\nRoast\",\n    nudge_y = .03,\n    color = \"#FFEA46FF\"\n  ) +\n  facet_wrap(\n    ~expertise,\n    ncol = 5\n  ) +\n  scale_y_continuous(\n    breaks = seq(.1, .6, .1)\n  ) +\n  viridis::scale_fill_viridis(\n    discrete = TRUE,\n    option = \"cividis\"\n  ) +\n  labs(\n    title = \"Probability of One's Favorite Coffees From The Great American Tasting\",\n    subtitle = \"Based on Self-Defined Expertise Level & Roast Level Preference\",\n    x = \"\",\n    y = \"Probability\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    strip.background = element_rect(\n      fill = \"#7C7B78FF\"\n    ),\n    axis.text = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    axis.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.title = element_text(\n      color = \"#7C7B78FF\"\n    ),\n    plot.subtitle = element_text(\n      color = \"#7C7B78FF\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nCode\nfavorite_abcd_prob |&gt;\nas_tibble() |&gt;\n  reactable(\n    filterable = TRUE,\n    searchable = TRUE\n  )"
  },
  {
    "objectID": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "href": "posts/2024-11-11-bayes-net-bnlearn-us-coffee-tasting/index.html#probabilities-of-coffee-a-or-coffee-d-based-on-expertise-level",
    "title": "Bayesian Network for US Coffee Tasting Data",
    "section": "Probabilities of Coffee A or Coffee D Based on Expertise Level",
    "text": "Probabilities of Coffee A or Coffee D Based on Expertise Level\nThe final queries I created were the probability of preferring coffee A or D based on the 10 self-defined expertise levels. These probabilities were extremely interesting because the upper expertise levels were the participants that were more likely to prefer coffees A or D. I’m sure there are additional analyses that could be done with this data, but I think this is where I’ll stop. I’m sure including more evidence from the model, such as how participants brew at home and their favorite coffee drinks would paint a better picture.\n\n\nCode\nexpert1 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"1\")\n)\nexpert2 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"2\")\n)\nexpert3 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"3\")\n)\nexpert4 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"4\")\n)\nexpert5 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"5\")\n)\nexpert6 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"6\")\n)\nexpert7 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"7\")\n)\nexpert8 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"8\")\n)\nexpert9 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"9\")\n)\nexpert10 &lt;- cpquery(\n  dag_fit,\n  event = (favorite_abcd == \"Coffee A\") |\n  (favorite_abcd == \"Coffee D\"),\n  evidence = (expertise == \"10\")\n)\n\ntibble(\n  expertise_level = seq(1, 10, 1),\n  probability_of_a_or_d = c(expert1, expert2, expert3, expert4, expert5, expert6, expert7, expert8, expert9, expert10)\n) |&gt;\n  ggplot(\n    aes(\n      as.factor(expertise_level),\n      probability_of_a_or_d\n    )\n  ) +\n  geom_col(\n    aes(fill = as.factor(expertise_level)),\n    position = position_dodge()\n  ) +\n  geom_text(\n    aes(\n      label = round(probability_of_a_or_d, 2)\n    ),\n    color = \"black\",\n    vjust = -.3\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, .1)\n  ) +\n  labs(\n    title = \"Probability of Choosing Coffee A or D as Their Favorite Coffee\",\n    subtitle = \"By Level of Self-Defined Expertise\",\n    x = \"Expertise\",\n    y = \"Probability\"\n  ) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n  theme(\n    legend.position = \"none\"\n  )"
  },
  {
    "objectID": "posts/2024-11-15-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-11-15-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Photo by Nan Zhou on Unsplash\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table &lt;- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n\n\nAs mentioned in the previous post, the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the cmdstanr package to call on Stan for the Bayesian analyses, I am using the posterior package to manipulate the chains, iterations, and draws from the analyses and the bayesplot package to visualize the convergence of each parameter included in the bayes net model. I’m also using the reactable package to showcase the parameters for the model.\n\nData Creation\n\n\nCode\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\n\nCode\nstan_file &lt;- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The J, I, K, and C list values are all important for looping through:\n\nJ = The number of rows of data; in this case there are 30 “students”\nI = The number of columns in the dataset; which is 15 excluding the first column\nK = The number of latent attributes/skills\nC = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from y for the actual data and then X in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\n\nCode\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"posts/2024-11-15-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n\nfit &lt;- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n\n# fit$save_object(\"simple_bayes_net.RDS\")\n\n\nThis next part will be different depending on whether or not you are using RStan or like in this case cmdstanR. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For cmdstanR, you call on your Stan file. Below is the Stan code or if you’d like to see it side-by-side, the Stan file can be found here. I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations.\n\n\nCode\n\"\ndata {\n  int&lt;lower=1&gt; J; // number of examinees\n  int&lt;lower=1&gt; I; // number of items\n  int&lt;lower=1&gt; K; // number of latent variables\n  int&lt;lower=1&gt; C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\nparameters {\n  simplex[C] nu; // class probabilities\n  vector&lt;lower=0, upper=1&gt;[I] false_pos;\n  vector&lt;lower=0, upper=1&gt;[I] true_pos;\n  real&lt;lower=0, upper=1&gt; lambda1;\n  real&lt;lower=0, upper=1&gt; lambda20;\n  real&lt;lower=0, upper=1&gt; lambda21;\n  real&lt;lower=0, upper=1&gt; lambda30;\n  real&lt;lower=0, upper=1&gt; lambda31;\n}\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] &gt; 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] &gt; 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] &gt; 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n\n\nLooking over the Stan code, there is a lot here. I’ll break down each section, but will not be spending an extensive amount of time for each.\n\n\nCode\n\"\ndata {\n  int&lt;lower=1&gt; J; // number of examinees\n  int&lt;lower=1&gt; I; // number of items\n  int&lt;lower=1&gt; K; // number of latent variables\n  int&lt;lower=1&gt; C; // number of classes\n  matrix[J, I] X; // response matrix\n  matrix[I, K] Q; // Q matrix\n  matrix[C, K] alpha; // attribute profile matrix\n}\n\"\n\n\nThe data section of stan code is including what you called the components of the stan_filelist object. If you deviate from what you named the components in your list, then your model will show an error. While not entirely necessary, you may want to put constraints on these values. For instance, I know that I have more than 1 student, item, latent variable, and class, so I will put a constraint that the lowest possible value is 1.\n\n\nCode\n\"\nparameters {\n  simplex[C] nu; // class probabilities\n  vector&lt;lower=0, upper=1&gt;[I] false_pos;\n  vector&lt;lower=0, upper=1&gt;[I] true_pos;\n  real&lt;lower=0, upper=1&gt; lambda1;\n  real&lt;lower=0, upper=1&gt; lambda20;\n  real&lt;lower=0, upper=1&gt; lambda21;\n  real&lt;lower=0, upper=1&gt; lambda30;\n  real&lt;lower=0, upper=1&gt; lambda31;\n}\n\"\n\n\nThe parameters section includes any parameters that are being included in your model. For instance, if creating a Bayesian linear regression, you would include the alpha and beta parameters in this section. For these models, I have the class probabilities for each latent class (to read more about the simplex function see here). Then I will have the probabilities of a student being either a true or false positive mastery case for the latent classes. These are vectors due to there being a true and false positive parameter for each item. The last parameters are the lambda parameters, which are the probabilities for mastery of the three latent attributes. These often require expert domain knowledge to specify informative priors.\n\n\nCode\n\"\ntransformed parameters{\n  vector[C] log_nu;\n  vector[2] theta_log1;\n  vector[2] theta_log2;\n  vector[2] theta_log3;\n  vector[C] theta1;\n  vector[C] theta2;\n  vector[C] theta3;\n  matrix[I, C] delta;\n\n  log_nu = log(nu);\n\n  theta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1);\n  theta_log1[2] = bernoulli_lpmf(1 | lambda1);\n  \n  theta_log2[1] = bernoulli_lpmf(1 | lambda20);\n  theta_log2[2] = bernoulli_lpmf(1 | lambda21);\n  \n  theta_log3[1] = bernoulli_lpmf(1 | lambda30);\n  theta_log3[2] = bernoulli_lpmf(1 | lambda31);\n  \n  for (c in 1 : C) {\n    if (alpha[c, 1] &gt; 0) {\n      theta1[c] = theta_log1[2];\n    } else {\n      theta1[c] = theta_log1[1];\n    }\n    if (alpha[c, 2] &gt; 0) {\n      theta2[c] = theta_log2[2];\n    } else {\n      theta2[c] = theta_log2[1];\n    }\n    if (alpha[c, 3] &gt; 0) {\n      theta3[c] = theta_log3[2];\n    } else {\n      theta3[c] = theta_log3[1];\n    }\n  }\n\n  for(c in 1:C){\n    for(i in 1:I){\n      delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2])\n                      * pow(exp(theta3[c]), Q[i, 3]);\n    }\n  }\n}\n\"\n\n\nWhile this section is optional, I like to include it because I use this section to do many of my calculations. For instance, in this section I like to use the prior lambda values to get the log probabilities of theta_log values, which are the log probabilities based on the level of mastery from the lambda values. I looped through the latent classes so when a latent class’ value is 1, then it takes the greater log probability, and when the value is 0, then it takes the lower log probability. I also did my delta calculations in this section. The delta calculation takes theta values based on the latent classes values and it uses the Q-matrix for each item. Then by multiplying the theta values raised to the power of the Q-matrix gets the probability of mastery for each item within each latent class. This value indicates whether a given student will have mastery over all of the latent attributes.\n\n\nCode\n\"\nmodel {\n  real pie;\n  vector[I] log_item;\n  vector[C] log_lik;\n  \n  // Priors\n  lambda1 ~ beta(2, 1);\n  lambda20 ~ beta(1, 2);\n  lambda21 ~ beta(2, 1);\n  lambda30 ~ beta(1, 2);\n  lambda31 ~ beta(2, 1);\n  \n  for (i in 1 : I) {\n    false_pos[i] ~ beta(1, 2);\n    true_pos[i] ~ beta(2, 1);\n  }\n  \n  //Likelihood\n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n      log_lik[c] = log_nu[c] + sum(log_item);\n    }\n    target += log_sum_exp(log_lik);\n  }\n}\n\"\n\n\nFor the model section, which is necessary, I always start with declaring any new variables, followed by priors for my lambda values and the true and false positive probabilities for each item. Lastly, this section is always where you will do your calculations for each item and for each latent class. Finally, the target calculation at the end is for the target log density.\n\n\nCode\n\"\ngenerated quantities {\n  real pie;\n  vector[I] log_item;\n  matrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c \n  matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k \n  row_vector[C] prob_joint;\n  vector[C] prob_attr_class;\n  \n  matrix[J, I] x_rep;\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {        \n        pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n        log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n        }\n      prob_joint[c] = nu[c] * exp(sum(log_item)); \n    }\n    prob_resp_class[j] = prob_joint / sum(prob_joint);\n  }\n  \n  for (j in 1 : J) {\n    for (k in 1 : K) {\n      for (c in 1 : C) {\n        // Calculate the probability of mastering attribute k given class c\n        prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k];\n      }\n      // Sum the probabilities to get the posterior probability of mastering attribute k\n      prob_resp_attr[j, k] = sum(prob_attr_class);\n    }\n  }\n  \n  for (j in 1 : J) {\n    for (c in 1 : C) {\n      for (i in 1 : I) {\n        x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n      }\n    }\n  }\n}\n\"\n\n\nThe last section, the generated quantities, is “generate additional quantities of interest from a fitted model without re-running the sampler” (Stan). For this series, I am using this section to calculate posterior probabilities, such as the probability of a student being in a specific latent class and the probability that students have mastered the attributes.\n\n\nCode\nfit &lt;- read_rds(here::here(\"posts/2024-11-15-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n\n\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9376389 0.9111992 0.8754096 0.9844359\n\n\nCode\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |&gt; arrange(desc(rhat)) |&gt; head()\n\n\n# A tibble: 6 × 4\n  variable               rhat ess_bulk ess_tail\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 prob_resp_class[12,8]  1.00    3869.    4877.\n2 nu[5]                  1.00    7371.    5381.\n3 log_nu[5]              1.00    7371.    5381.\n4 prob_resp_class[5,8]   1.00    5686.    5902.\n5 prob_resp_class[18,8]  1.00    3686.    4569.\n6 prob_resp_class[27,5]  1.00    5974.    5744.\n\n\nCode\nbn_measure |&gt; mutate(across(-variable, ~round(.x, 3))) |&gt; react_table()\n\n\n\n\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\n\nCode\nbn_measure |&gt; \n  mutate(across(-variable, ~round(.x, 3))) |&gt; \n  filter(str_detect(variable, \"prob_resp_attr\")) |&gt;\n  react_table()\n\n\n\n\n\n\nI decided to filter in on the probabilities for students to have mastery over the attributes. The first index in the square brackets indicates the student and then the second index value indicates the three attributes. Obviously for something more thought out this would line up for meaningful attributes, but for this example, the values align with arbitrary values.\n\n\nCode\ny_rep &lt;- fit$draws(\"x_rep\") |&gt; as_draws_matrix()\nstu_resp_attr &lt;- fit$draws(\"prob_resp_attr\") |&gt; as_draws_matrix()\n\n\nI decided to extract the replicated values for the items and the probabilities of each student’s mastery of each of the three latent attributes.\n\n\nCode\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n\n\n\n\n\n\n\n\n\nCode\ny |&gt; react_table()\n\n\n\n\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the original data, the original responses and the probabilities with a probability threshold of 0.5 match one another.\n\n\nCode\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\n\n\nCode\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\n\n\nCode\nppc_intervals(\n  y = y |&gt; pull(y1) |&gt; as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n\n\n\n\n\n\n\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\n\nCode\nactual_stu_resp_attr &lt;- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |&gt;\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.\n\n\nCode\nstu_resp_attr_mean &lt;- stu_resp_attr |&gt;\n  as_tibble() |&gt;\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_mean |&gt;\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(\n    everything()\n  ) |&gt;\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |&gt;\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |&gt;\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute.\n\n\nCode\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n\n\n$att1\n   .y\n.x   0  1\n  0  6  5\n  1  7 12\n\n$att2\n   .y\n.x   0  1\n  0  6  8\n  1  5 11\n\n$att3\n   .y\n.x   0  1\n  0  4  2\n  1 11 13\n\n\nCode\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n\n\n$att1\n   .y\n.x          0         1\n  0 0.2000000 0.1666667\n  1 0.2333333 0.4000000\n\n$att2\n   .y\n.x          0         1\n  0 0.2000000 0.2666667\n  1 0.1666667 0.3666667\n\n$att3\n   .y\n.x           0          1\n  0 0.13333333 0.06666667\n  1 0.36666667 0.43333333\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model.\n\n\nCode\nstu_resp_attr_long &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long &lt;- actual_stu_resp_attr |&gt;\n  pivot_longer(-studentid)\n\naccuracy_att &lt;- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n\n\n[1] 0.5777778\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 58%. This is a good starting point, but this may indicate that the model needs better defined priors and may require the edges between the attributes to show latent relationships. The low accuracy value may also be indicative of the importance of domain knowledge in building a latent bayes net."
  },
  {
    "objectID": "index.html#dashboard-example",
    "href": "index.html#dashboard-example",
    "title": "Resources",
    "section": "",
    "text": "University of Oregon Graduate School Dashboard"
  },
  {
    "objectID": "index.html#shiny-examples",
    "href": "index.html#shiny-examples",
    "title": "Resources",
    "section": "",
    "text": "Neuropsychology Assessment Calculations\n\nGitHub Repository: (Link)\n\nProphet Stock Price Parameters Shiny App\n\nGitHub Repository: (Link)"
  },
  {
    "objectID": "index.html#visualizations-with-write-ups",
    "href": "index.html#visualizations-with-write-ups",
    "title": "Resources",
    "section": "",
    "text": "Reductions in Physical Activity in California Counties\nFunding for University of Oregon Teaching Assistants\nThe Importance of Including Variables in Models"
  },
  {
    "objectID": "index.html#current-projectsanalyses",
    "href": "index.html#current-projectsanalyses",
    "title": "Resources",
    "section": "",
    "text": "Suspension Levels for Students with Disabilities in California Districts"
  },
  {
    "objectID": "index.html#docker-image",
    "href": "index.html#docker-image",
    "title": "Resources",
    "section": "",
    "text": "Shiny App for Bayesian Network\n\nGitHub Repository: (Link)"
  },
  {
    "objectID": "posts/2024-12-04-python-parameterized-report/report_template.html",
    "href": "posts/2024-12-04-python-parameterized-report/report_template.html",
    "title": "Creating Parameterized Reports",
    "section": "",
    "text": "Photo by Jonny Gios on Unsplash\nCode\nimport pandas as pd\nimport plotnine as pn\nfrom matplotlib import rcParams\nfrom IPython.display import display, Markdown\nfrom great_tables import GT\nimport plotly.express as px\nfrom palmerpenguins import load_penguins\nCode\npenguins = load_penguins()"
  },
  {
    "objectID": "posts/2024-12-04-python-parameterized-report/report_template.html#eda",
    "href": "posts/2024-12-04-python-parameterized-report/report_template.html#eda",
    "title": "Creating Parameterized Reports",
    "section": "EDA",
    "text": "EDA\n\n\nCode\npenguins.value_counts('year')\n\npenguin = penguins.loc[penguins['year'] == year]\n\n# pen = penguin.melt(id_vars = ['species', 'island', 'sex', 'year'], value_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']).dropna()\n\npen_table = penguin.groupby(['species', 'island', 'sex'])[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].mean().reset_index()\n\npen_table = pen_table.round(2)\n\npen = pen_table.melt(id_vars = ['species', 'island', 'sex'], value_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g'])\n\nprint(pen_table)\nprint(pen)\n\n\n     species     island     sex  bill_length_mm  bill_depth_mm  \\\n0     Adelie     Biscoe  female           37.48          18.58   \n1     Adelie     Biscoe    male           39.16          18.30   \n2     Adelie      Dream  female           37.86          17.84   \n3     Adelie      Dream    male           40.38          19.43   \n4     Adelie  Torgersen  female           38.28          18.16   \n5     Adelie  Torgersen    male           39.90          20.49   \n6  Chinstrap      Dream  female           46.57          17.84   \n7  Chinstrap      Dream    male           50.88          19.13   \n8     Gentoo     Biscoe  female           45.06          13.99   \n9     Gentoo     Biscoe    male           49.00          15.36   \n\n   flipper_length_mm  body_mass_g  \n0             181.80      3470.00  \n1             181.60      3770.00  \n2             185.00      3269.44  \n3             188.60      4102.50  \n4             187.62      3475.00  \n5             192.29      4139.29  \n6             188.69      3569.23  \n7             196.15      3819.23  \n8             211.06      4618.75  \n9             218.88      5552.94  \n      species     island     sex           variable    value\n0      Adelie     Biscoe  female     bill_length_mm    37.48\n1      Adelie     Biscoe    male     bill_length_mm    39.16\n2      Adelie      Dream  female     bill_length_mm    37.86\n3      Adelie      Dream    male     bill_length_mm    40.38\n4      Adelie  Torgersen  female     bill_length_mm    38.28\n5      Adelie  Torgersen    male     bill_length_mm    39.90\n6   Chinstrap      Dream  female     bill_length_mm    46.57\n7   Chinstrap      Dream    male     bill_length_mm    50.88\n8      Gentoo     Biscoe  female     bill_length_mm    45.06\n9      Gentoo     Biscoe    male     bill_length_mm    49.00\n10     Adelie     Biscoe  female      bill_depth_mm    18.58\n11     Adelie     Biscoe    male      bill_depth_mm    18.30\n12     Adelie      Dream  female      bill_depth_mm    17.84\n13     Adelie      Dream    male      bill_depth_mm    19.43\n14     Adelie  Torgersen  female      bill_depth_mm    18.16\n15     Adelie  Torgersen    male      bill_depth_mm    20.49\n16  Chinstrap      Dream  female      bill_depth_mm    17.84\n17  Chinstrap      Dream    male      bill_depth_mm    19.13\n18     Gentoo     Biscoe  female      bill_depth_mm    13.99\n19     Gentoo     Biscoe    male      bill_depth_mm    15.36\n20     Adelie     Biscoe  female  flipper_length_mm   181.80\n21     Adelie     Biscoe    male  flipper_length_mm   181.60\n22     Adelie      Dream  female  flipper_length_mm   185.00\n23     Adelie      Dream    male  flipper_length_mm   188.60\n24     Adelie  Torgersen  female  flipper_length_mm   187.62\n25     Adelie  Torgersen    male  flipper_length_mm   192.29\n26  Chinstrap      Dream  female  flipper_length_mm   188.69\n27  Chinstrap      Dream    male  flipper_length_mm   196.15\n28     Gentoo     Biscoe  female  flipper_length_mm   211.06\n29     Gentoo     Biscoe    male  flipper_length_mm   218.88\n30     Adelie     Biscoe  female        body_mass_g  3470.00\n31     Adelie     Biscoe    male        body_mass_g  3770.00\n32     Adelie      Dream  female        body_mass_g  3269.44\n33     Adelie      Dream    male        body_mass_g  4102.50\n34     Adelie  Torgersen  female        body_mass_g  3475.00\n35     Adelie  Torgersen    male        body_mass_g  4139.29\n36  Chinstrap      Dream  female        body_mass_g  3569.23\n37  Chinstrap      Dream    male        body_mass_g  3819.23\n38     Gentoo     Biscoe  female        body_mass_g  4618.75\n39     Gentoo     Biscoe    male        body_mass_g  5552.94\n\n\n\n\nCode\n(\n  pn.ggplot(pen, pn.aes('factor(species)', 'value'))\n  + pn.geom_col(pn.aes(fill = 'island'), position = pn.position_dodge())\n  + pn.facet_wrap('variable', scales = 'free')\n  + pn.theme(legend_position = 'bottom')\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndisplay(Markdown(f\"\"\"\n## Average Measurements For Different Penguin Species By Island and Sex in the Year {year}\n\"\"\"))\n\n\nAverage Measurements For Different Penguin Species By Island and Sex in the Year 2007\n\n\n\n\nCode\n(\n  GT(pen_table)\n)\n\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nAdelie\nBiscoe\nfemale\n37.48\n18.58\n181.8\n3470.0\n\n\nAdelie\nBiscoe\nmale\n39.16\n18.3\n181.6\n3770.0\n\n\nAdelie\nDream\nfemale\n37.86\n17.84\n185.0\n3269.44\n\n\nAdelie\nDream\nmale\n40.38\n19.43\n188.6\n4102.5\n\n\nAdelie\nTorgersen\nfemale\n38.28\n18.16\n187.62\n3475.0\n\n\nAdelie\nTorgersen\nmale\n39.9\n20.49\n192.29\n4139.29\n\n\nChinstrap\nDream\nfemale\n46.57\n17.84\n188.69\n3569.23\n\n\nChinstrap\nDream\nmale\n50.88\n19.13\n196.15\n3819.23\n\n\nGentoo\nBiscoe\nfemale\n45.06\n13.99\n211.06\n4618.75\n\n\nGentoo\nBiscoe\nmale\n49.0\n15.36\n218.88\n5552.94"
  },
  {
    "objectID": "blog/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "blog/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "Jonathan A. Pedroza",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "blog/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "blog/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Jonathan A. Pedroza",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "blog/Lib/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "href": "blog/Lib/site-packages/httpcore-1.0.7.dist-info/licenses/LICENSE.html",
    "title": "Jonathan A. Pedroza",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "blog/Lib/site-packages/httpx-0.28.0.dist-info/licenses/LICENSE.html",
    "href": "blog/Lib/site-packages/httpx-0.28.0.dist-info/licenses/LICENSE.html",
    "title": "Jonathan A. Pedroza",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "blog/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "blog/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "Jonathan A. Pedroza",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/2024-12-04-python-parameterized-report/report_template.html#average-measurements-for-different-penguin-species-by-island-and-sex-in-the-year-2007",
    "href": "posts/2024-12-04-python-parameterized-report/report_template.html#average-measurements-for-different-penguin-species-by-island-and-sex-in-the-year-2007",
    "title": "Creating Parameterized Reports",
    "section": "Average Measurements For Different Penguin Species By Island and Sex in the Year 2007",
    "text": "Average Measurements For Different Penguin Species By Island and Sex in the Year 2007"
  },
  {
    "objectID": "posts/2024-12-04-python-parameterized-report/index.html",
    "href": "posts/2024-12-04-python-parameterized-report/index.html",
    "title": "Creating Parameterized Reports",
    "section": "",
    "text": "Photo by Jonny Gios on Unsplash\nI wanted to make a quick post about this topic because I could not find much online on this topic and it was something I realized I had not tried in Python. Below is the code for a parameterized report where I am filtering for data from the palmerpenguins package to only look at data from 2007. I decided on the palmerpenguins dataset because it is accessible and can showcase a simple for loop that renders a yearly report on penguin measurements.\nThe for loop script can be found here and the actual files created are showcased here. This example will only show the template that was used for the Quarto document. Also, as I was creating this I came across a wonderful video explaining a more in-depth example of looping through a couple of parameters for Quarto documents in Python (video here, Quarto document code here, for loop code here).\nCode\nlibrary(reticulate)\nuse_python(here::here(\"website_venv/bin/python3\"))\n# use_virtualenv(here::here(\"website_venv\"))\nCode\nimport pandas as pd\nimport plotnine as pn\nfrom matplotlib import rcParams\nfrom IPython.display import display, Markdown\nfrom great_tables import GT\nimport plotly.express as px\nfrom palmerpenguins import load_penguins\nimport PyQt6"
  },
  {
    "objectID": "posts/2024-12-04-python-parameterized-report/index.html#eda",
    "href": "posts/2024-12-04-python-parameterized-report/index.html#eda",
    "title": "Creating Parameterized Reports",
    "section": "EDA",
    "text": "EDA\n\n\nyear\n2009    120\n2008    114\n2007    110\nName: count, dtype: int64\n\n\n     species     island     sex  ...  bill_depth_mm  flipper_length_mm  body_mass_g\n0     Adelie     Biscoe  female  ...      18.580000         181.800000  3470.000000\n1     Adelie     Biscoe    male  ...      18.300000         181.600000  3770.000000\n2     Adelie      Dream  female  ...      17.844444         185.000000  3269.444444\n3     Adelie      Dream    male  ...      19.430000         188.600000  4102.500000\n4     Adelie  Torgersen  female  ...      18.162500         187.625000  3475.000000\n5     Adelie  Torgersen    male  ...      20.485714         192.285714  4139.285714\n6  Chinstrap      Dream  female  ...      17.838462         188.692308  3569.230769\n7  Chinstrap      Dream    male  ...      19.130769         196.153846  3819.230769\n8     Gentoo     Biscoe  female  ...      13.993750         211.062500  4618.750000\n9     Gentoo     Biscoe    male  ...      15.364706         218.882353  5552.941176\n\n[10 rows x 7 columns]\n\n\n     species     island     sex  ...  bill_depth_mm  flipper_length_mm  body_mass_g\n0     Adelie     Biscoe  female  ...          18.58             181.80      3470.00\n1     Adelie     Biscoe    male  ...          18.30             181.60      3770.00\n2     Adelie      Dream  female  ...          17.84             185.00      3269.44\n3     Adelie      Dream    male  ...          19.43             188.60      4102.50\n4     Adelie  Torgersen  female  ...          18.16             187.62      3475.00\n5     Adelie  Torgersen    male  ...          20.49             192.29      4139.29\n6  Chinstrap      Dream  female  ...          17.84             188.69      3569.23\n7  Chinstrap      Dream    male  ...          19.13             196.15      3819.23\n8     Gentoo     Biscoe  female  ...          13.99             211.06      4618.75\n9     Gentoo     Biscoe    male  ...          15.36             218.88      5552.94\n\n[10 rows x 7 columns]\n\n\n      species     island     sex           variable    value\n0      Adelie     Biscoe  female     bill_length_mm    37.48\n1      Adelie     Biscoe    male     bill_length_mm    39.16\n2      Adelie      Dream  female     bill_length_mm    37.86\n3      Adelie      Dream    male     bill_length_mm    40.38\n4      Adelie  Torgersen  female     bill_length_mm    38.28\n5      Adelie  Torgersen    male     bill_length_mm    39.90\n6   Chinstrap      Dream  female     bill_length_mm    46.57\n7   Chinstrap      Dream    male     bill_length_mm    50.88\n8      Gentoo     Biscoe  female     bill_length_mm    45.06\n9      Gentoo     Biscoe    male     bill_length_mm    49.00\n10     Adelie     Biscoe  female      bill_depth_mm    18.58\n11     Adelie     Biscoe    male      bill_depth_mm    18.30\n12     Adelie      Dream  female      bill_depth_mm    17.84\n13     Adelie      Dream    male      bill_depth_mm    19.43\n14     Adelie  Torgersen  female      bill_depth_mm    18.16\n15     Adelie  Torgersen    male      bill_depth_mm    20.49\n16  Chinstrap      Dream  female      bill_depth_mm    17.84\n17  Chinstrap      Dream    male      bill_depth_mm    19.13\n18     Gentoo     Biscoe  female      bill_depth_mm    13.99\n19     Gentoo     Biscoe    male      bill_depth_mm    15.36\n20     Adelie     Biscoe  female  flipper_length_mm   181.80\n21     Adelie     Biscoe    male  flipper_length_mm   181.60\n22     Adelie      Dream  female  flipper_length_mm   185.00\n23     Adelie      Dream    male  flipper_length_mm   188.60\n24     Adelie  Torgersen  female  flipper_length_mm   187.62\n25     Adelie  Torgersen    male  flipper_length_mm   192.29\n26  Chinstrap      Dream  female  flipper_length_mm   188.69\n27  Chinstrap      Dream    male  flipper_length_mm   196.15\n28     Gentoo     Biscoe  female  flipper_length_mm   211.06\n29     Gentoo     Biscoe    male  flipper_length_mm   218.88\n30     Adelie     Biscoe  female        body_mass_g  3470.00\n31     Adelie     Biscoe    male        body_mass_g  3770.00\n32     Adelie      Dream  female        body_mass_g  3269.44\n33     Adelie      Dream    male        body_mass_g  4102.50\n34     Adelie  Torgersen  female        body_mass_g  3475.00\n35     Adelie  Torgersen    male        body_mass_g  4139.29\n36  Chinstrap      Dream  female        body_mass_g  3569.23\n37  Chinstrap      Dream    male        body_mass_g  3819.23\n38     Gentoo     Biscoe  female        body_mass_g  4618.75\n39     Gentoo     Biscoe    male        body_mass_g  5552.94\n\n\n\n\nCode\npn.ggplot.show(\n  pn.ggplot(pen, pn.aes('factor(species)', 'value'))\n  + pn.geom_col(pn.aes(fill = 'island'), position = pn.position_dodge())\n  + pn.facet_wrap('variable', scales = 'free')\n  + pn.theme(legend_position = 'bottom')\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndisplay(Markdown(f\"\"\"\n## Average Measurements For Different Penguin Species By Island and Sex in the Year {year}\n\"\"\"))\n\n\n&lt;IPython.core.display.Markdown object&gt;\n\n\n\n\nCode\n(\n  GT(pen_table)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nAdelie\nBiscoe\nfemale\n37.48\n18.58\n181.8\n3470.0\n\n\nAdelie\nBiscoe\nmale\n39.16\n18.3\n181.6\n3770.0\n\n\nAdelie\nDream\nfemale\n37.86\n17.84\n185.0\n3269.44\n\n\nAdelie\nDream\nmale\n40.38\n19.43\n188.6\n4102.5\n\n\nAdelie\nTorgersen\nfemale\n38.28\n18.16\n187.62\n3475.0\n\n\nAdelie\nTorgersen\nmale\n39.9\n20.49\n192.29\n4139.29\n\n\nChinstrap\nDream\nfemale\n46.57\n17.84\n188.69\n3569.23\n\n\nChinstrap\nDream\nmale\n50.88\n19.13\n196.15\n3819.23\n\n\nGentoo\nBiscoe\nfemale\n45.06\n13.99\n211.06\n4618.75\n\n\nGentoo\nBiscoe\nmale\n49.0\n15.36\n218.88\n5552.94"
  },
  {
    "objectID": "posts/2024-12-04-python-parameterized-report/index.html#average-measurements-for-different-penguin-species-by-island-and-sex-in-the-year-2007",
    "href": "posts/2024-12-04-python-parameterized-report/index.html#average-measurements-for-different-penguin-species-by-island-and-sex-in-the-year-2007",
    "title": "Creating Parameterized Reports",
    "section": "Average Measurements For Different Penguin Species By Island and Sex in the Year 2007",
    "text": "Average Measurements For Different Penguin Species By Island and Sex in the Year 2007"
  },
  {
    "objectID": "about.html#quarto-extensions",
    "href": "about.html#quarto-extensions",
    "title": "Resources",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension"
  },
  {
    "objectID": "about.html#dashboard-example",
    "href": "about.html#dashboard-example",
    "title": "Resources",
    "section": "",
    "text": "University of Oregon Graduate School Dashboard"
  },
  {
    "objectID": "about.html#docker-image",
    "href": "about.html#docker-image",
    "title": "Resources",
    "section": "",
    "text": "Shiny App for Bayesian Network\n\nGitHub Repository: (Link)"
  },
  {
    "objectID": "about.html#shiny-examples",
    "href": "about.html#shiny-examples",
    "title": "Resources",
    "section": "",
    "text": "Neuropsychology Assessment Calculations\n\nGitHub Repository: (Link)\n\nProphet Stock Price Parameters Shiny App\n\nGitHub Repository: (Link)"
  },
  {
    "objectID": "about.html#visualizations-with-write-ups",
    "href": "about.html#visualizations-with-write-ups",
    "title": "Resources",
    "section": "",
    "text": "Reductions in Physical Activity in California Counties\nFunding for University of Oregon Teaching Assistants\nThe Importance of Including Variables in Models"
  },
  {
    "objectID": "about.html#current-projectsanalyses",
    "href": "about.html#current-projectsanalyses",
    "title": "Resources",
    "section": "",
    "text": "Suspension Levels for Students with Disabilities in California Districts"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension"
  },
  {
    "objectID": "resources.html#quarto-extensions",
    "href": "resources.html#quarto-extensions",
    "title": "Resources",
    "section": "",
    "text": "Link to invoice Quarto Extension\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension"
  },
  {
    "objectID": "resources.html#dashboard-example",
    "href": "resources.html#dashboard-example",
    "title": "Resources",
    "section": "Dashboard Example",
    "text": "Dashboard Example\nUniversity of Oregon Graduate School Dashboard"
  },
  {
    "objectID": "resources.html#docker-image",
    "href": "resources.html#docker-image",
    "title": "Resources",
    "section": "Docker Image",
    "text": "Docker Image\nShiny App for Bayesian Network\n\nGitHub Repository: (Link)"
  },
  {
    "objectID": "resources.html#shiny-examples",
    "href": "resources.html#shiny-examples",
    "title": "Resources",
    "section": "Shiny Examples",
    "text": "Shiny Examples\nNeuropsychology Assessment Calculations\n\nGitHub Repository: (Link)\n\nProphet Stock Price Parameters Shiny App\n\nGitHub Repository: (Link)"
  },
  {
    "objectID": "resources.html#visualizations-with-write-ups",
    "href": "resources.html#visualizations-with-write-ups",
    "title": "Resources",
    "section": "Visualizations With Write Ups",
    "text": "Visualizations With Write Ups\nReductions in Physical Activity in California Counties\nFunding for University of Oregon Teaching Assistants\nThe Importance of Including Variables in Models"
  },
  {
    "objectID": "resources.html#current-projectsanalyses",
    "href": "resources.html#current-projectsanalyses",
    "title": "Resources",
    "section": "Current Projects/Analyses",
    "text": "Current Projects/Analyses\nSuspension Levels for Students with Disabilities in California Districts"
  },
  {
    "objectID": "penguin_report_2007.html",
    "href": "penguin_report_2007.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Jonathan Andrew Pedroza PhD, but everyone calls me JP. I received my PhD in Prevention Science from the University of Oregon in 2021. My education in Prevention Science included training in program evaluation, implementation science, machine learning, inferential statistics, and survey design. I currently am a contractor for Posit Academy as a data science mentor. As a mentor, I train cohorts from organizations around the world to learn data science skills in R and Python, such as data wrangling, data visualization, functional programming, model building, and communicating results using dashboards and reports. I am open to learning more about a new field, especially since working with data from various fields with my academy cohorts.\n\n\n\n\n\n\nMy posts often include using R and Python for data analyses and machine learning. Some of these topics include Bayesian statistics with Stan, interactive documents, and shiny apps. When I am away from my computer, I enjoy roasting and brewing coffee, hiking, fishing, cooking, and playing with my cats. I’m currently on BlueSky for social media and infrequently check Mastodon. You can also email me at  jonpedroza1228@gmail.com with any inquires.\n\nData Science Colleagues"
  },
  {
    "objectID": "penguin_report_2008.html",
    "href": "penguin_report_2008.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Jonathan Andrew Pedroza PhD, but everyone calls me JP. I received my PhD in Prevention Science from the University of Oregon in 2021. My education in Prevention Science included training in program evaluation, implementation science, machine learning, inferential statistics, and survey design. I currently am a contractor for Posit Academy as a data science mentor. As a mentor, I train cohorts from organizations around the world to learn data science skills in R and Python, such as data wrangling, data visualization, functional programming, model building, and communicating results using dashboards and reports. I am open to learning more about a new field, especially since working with data from various fields with my academy cohorts.\n\n\n\n\n\n\nMy posts often include using R and Python for data analyses and machine learning. Some of these topics include Bayesian statistics with Stan, interactive documents, and shiny apps. When I am away from my computer, I enjoy roasting and brewing coffee, hiking, fishing, cooking, and playing with my cats. I’m currently on BlueSky for social media and infrequently check Mastodon. You can also email me at  jonpedroza1228@gmail.com with any inquires.\n\nData Science Colleagues"
  },
  {
    "objectID": "penguin_report_2009.html",
    "href": "penguin_report_2009.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Jonathan Andrew Pedroza PhD, but everyone calls me JP. I received my PhD in Prevention Science from the University of Oregon in 2021. My education in Prevention Science included training in program evaluation, implementation science, machine learning, inferential statistics, and survey design. I currently am a contractor for Posit Academy as a data science mentor. As a mentor, I train cohorts from organizations around the world to learn data science skills in R and Python, such as data wrangling, data visualization, functional programming, model building, and communicating results using dashboards and reports. I am open to learning more about a new field, especially since working with data from various fields with my academy cohorts.\n\n\n\n\n\n\nMy posts often include using R and Python for data analyses and machine learning. Some of these topics include Bayesian statistics with Stan, interactive documents, and shiny apps. When I am away from my computer, I enjoy roasting and brewing coffee, hiking, fishing, cooking, and playing with my cats. I’m currently on BlueSky for social media and infrequently check Mastodon. You can also email me at  jonpedroza1228@gmail.com with any inquires.\n\nData Science Colleagues"
  },
  {
    "objectID": "posts/2024-12-05-python-parameterized-report/index.html",
    "href": "posts/2024-12-05-python-parameterized-report/index.html",
    "title": "Looping Through Parameterized Reports",
    "section": "",
    "text": "Photo by Jonny Gios on Unsplash\nI wanted to make a quick post about looping through a parameterized report using Quarto and Python because I could not find much information on this topic using Python. When I was creating this, I came across a wonderful video explaining a more in-depth example of looping through a couple of parameters for Quarto documents in Python (video here, Quarto document code here, for loop code here).\nCode\nimport pandas as pd\nimport numpy as np\nimport plotnine as pn\nfrom matplotlib import rcParams\nfrom IPython.display import display, Markdown\nfrom great_tables import GT\nimport plotly.express as px\nfrom palmerpenguins import load_penguins"
  },
  {
    "objectID": "posts/2024-12-05-python-parameterized-report/index.html#exploratory-data-analysis",
    "href": "posts/2024-12-05-python-parameterized-report/index.html#exploratory-data-analysis",
    "title": "Looping Through Parameterized Reports",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBelow I am doing some basic calculations to have for a visualization and a great tables output table. To render reports based on the year’s data, I filtered the data as penguin so it stratified the plot and table for each year within each parameterized report.\n\nFinding Number of Years\n\n\nyear\n2009    120\n2008    114\n2007    110\nName: count, dtype: int64\n\n\n\n\nFiltering by Year\n\n\nVisualization of Species Characteristics\n\n\n\n\n\n\n\n\n\n\n\nTable of Species Characteristics\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nAdelie\nBiscoe\nfemale\n37.48\n18.58\n181.8\n3470.0\n\n\nAdelie\nBiscoe\nmale\n39.16\n18.3\n181.6\n3770.0\n\n\nAdelie\nDream\nfemale\n37.86\n17.84\n185.0\n3269.44\n\n\nAdelie\nDream\nmale\n40.38\n19.43\n188.6\n4102.5\n\n\nAdelie\nTorgersen\nfemale\n38.28\n18.16\n187.62\n3475.0\n\n\nAdelie\nTorgersen\nmale\n39.9\n20.49\n192.29\n4139.29\n\n\nChinstrap\nDream\nfemale\n46.57\n17.84\n188.69\n3569.23\n\n\nChinstrap\nDream\nmale\n50.88\n19.13\n196.15\n3819.23\n\n\nGentoo\nBiscoe\nfemale\n45.06\n13.99\n211.06\n4618.75\n\n\nGentoo\nBiscoe\nmale\n49.0\n15.36\n218.88\n5552.94"
  },
  {
    "objectID": "posts/2024-12-05-python-parameterized-report/index.html#parameters",
    "href": "posts/2024-12-05-python-parameterized-report/index.html#parameters",
    "title": "Looping Through Parameterized Reports",
    "section": "Parameter(s)",
    "text": "Parameter(s)\nIn order to prepare a Quarto document to have parameters for individual reports, we need to include a tags evaluation option as #| tags: [parameters]. I have set up my year parameter for 2007. I usually run the parameterized report to make sure that the report renders correctly. When everything works like it should, I move forward with the for loop to render multiple reports.\n\n\nCode\nyear = 2007"
  },
  {
    "objectID": "posts/2024-12-05-python-parameterized-report/index.html#looping-through-years",
    "href": "posts/2024-12-05-python-parameterized-report/index.html#looping-through-years",
    "title": "Looping Through Parameterized Reports",
    "section": "Looping Through Years",
    "text": "Looping Through Years\nTo render the report above, we are going to create a python file (.py) to house our for loop for the reports. Rather than link to an additional file, I have the code below of what would be on the .py file. The only package we’ll need is the os package. Since this is a simple example, I’m going to loop through one parameter (year) and since I already went through the palmerpenguins dataset, I know there are three years of data. For this example, I hard coded the years into a list to loop through, but I could have also used unique() functions from pandas or numpy to get the unique values for the years, which would be more beneficial if the data would be updated in the future.\nThe for loop then calls for each value in the year_list list. I also included the year in the final html file to make it easier to distinguish which report belongs to what data. Then I like to organize the command argument into the separate components of the quarto command with the name of the parameterized report template first followed by the parameter I’m looping over and then the output file name. This should render three reports, one for each year, with the first name being penguin_report_2007.html."
  }
]