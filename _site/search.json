[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Under Development\n\nUseful Resources\nLink to Resume Quarto Extension\nLink to Cover Letter Quarto Extension\n\n\nData Science Colleagues"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! I am Jonathan Andrew Pedroza PhD, but I have gotten used to everyone just referring to me as JP. I am currently on the job market looking for any data analyst, data scientist, or psychometrician openings. I received my PhD in Prevention Science from the University of Oregon in 2021. My training as a Prevention Scientist included training in program evaluation, implementation science, machine learning, inferential statistics, and survey design. I am also a Posit (Previously RStudio) Academy Mentor to cohorts around the world where I help cohort members with learning R while providing tips about additional packages and best practices using R and the Tidyverse.\nMy past research interests included the examination of physical activity, sedentary behaviors, and environmental factors that contribute to inequities of access and engagement in health behaviors. Currently my research interests are in using machine learning to draw attention to accessibility concerns for students with significant cognitive disabilities. Additional interests include creating visualizations to disseminate findings to a wide audience, including practitioners, other resaerchers, and state officials and Bayesian analyses. However, I am interested in all types of data and the complex problems associated with them.\nMy main programming language is R; however, I am competent in using Python and have working knowledge of SQL, Stan, and Julia. When I am away from my computer, I enjoy EVERYTHING about coffee, hiking, cycling, and playing with my cats. Away from my computer I enjoy roasting and tasting different coffees, hiking, cycling, fishing, and playing with my cats. If you have any questions or requests for statistics/machine learning consultation, you can reach me on twitter or through email cpppedroza@gmail.com."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Posts",
    "section": "",
    "text": "Fabricating Some Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Net Pt. 2\n\n\n\nBayesian\n\n\nBayesian Network\n\n\nbayes net\n\n\nR\n\n\nstan\n\n\ncmdstanr\n\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Nets\n\n\n\nbayesian\n\n\nbayesian network\n\n\nbayes net\n\n\nR\n\n\nstan\n\n\ncmdstanr\n\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation in R & Python\n\n\n\ndata manipulation\n\n\ndplyr\n\n\npandas\n\n\nnumpy\n\n\npython\n\n\nR\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Typst To Create Documents\n\n\n\nR\n\n\nTypst\n\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProphet Model\n\n\n\nVisualizations\n\n\nAnalysis\n\n\nForecast\n\n\nTidyModels\n\n\nModeltime\n\n\n\n\n\n\n\n\n\n\nJun 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student Satisfaction Exit Surveys\n\n\n\nVisualizations\n\n\nShiny\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday Coffee Ratings\n\n\n\nVisualizations\n\n\nAnalysis\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter Conference Presentation\n\n\n\nVisualizations\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html",
    "href": "posts/2022-06-02-prophet-model/index.html",
    "title": "Prophet Model",
    "section": "",
    "text": "As I start looking for non-academic positions, I wanted to practice forecasting as I didn’t really have much experience with these types of models. NOTE: This is for practicing forecasting skills and you should not trust this model with your own stocks. After plenty of reading,\nI finally have some understanding of how to utilize these models. This post started because even after a BA, 2 masters degrees, and a doctorate, my brother still has no clue what I do. He, along with most of my family think I am a Clinical Psychologist.\nSo for me to try and make my brother understand what I do, I thought I would show him with something that he has become interested with recently; stocks. So for this post, I’ll\nBelow are all the sites for the packages I used.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(prophet)\nlibrary(lubridate)\nlibrary(modeltime)\nlibrary(timetk)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "href": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "title": "Prophet Model",
    "section": "Loading Data",
    "text": "Loading Data\nTo load the Google Finance data, I decided to pick a stock that my brother had, which in this case was JetBlue. A cool feature about Google Finance and Google Sheets is that you can use the following formula in a Google Sheet on the first cell of the first column =GOOGLEFINANCE(\"JBLU\", \"price\", DATE(2000,1,1), DATE(2025, 1, 1), \"DAILY\") and it will give you the date and stock closing values for whatever period you’d like. The example above provides Google financial data for JBLU or the abbreviation for JetBlue stock. It also provides the price of the stock from the first day that there is data on JetBlue stocks, which in this case is April 12th 2002. You can also choose the period of time for the stock prices. I decided to look at daily data.\nJetBlue Sheet\nHere I have a copy of my Google Sheet for JetBlue that I will use to train and test my Prophet model. Instead of having a .csv file on my local machine, I decided to keep this on Google Drive so that it constantly updates with the Google Finance function. This meant that I had to use the googlesheets4 package to load the data from a Google Sheet. I also changed the name and class of the date variable to make it a date variable instead of a date and time variable.\n\ngooglesheets4::gs4_deauth()\n\ntheme_set(theme_light())\n\njet &lt;- \n  googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1SpRXsC3kXDaQLUfC6cPIOvsqxDF6updhgHRJeT8PTog/edit#gid=0\", sheet = 1, range = \"A1:B1000\") %&gt;% \n  janitor::clean_names() %&gt;%\n  mutate(ds = as_date(date))\n\n\nCleaning Up the Data\nBased on some visualizations below, I also decided to create some additional variables from the date variable. Specifically, I used lubridate's wday() function to create a new variable that gives you the actual day from the corresponding cell’s date. I also used the ts_clean_vec function from time_tk to clean for outliers in the stock price values. There are additional arguments for the function, like applying a Box-Cox transformation but that is for a multiplicative trend, which this model does not appear to fit since the variation in the outcome does not grow exponentially. I’ll also include 2002 as the reference year for the year variable and make sure that my data is arranged by date.\n\njetblue &lt;- jet %&gt;% \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) %&gt;% \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %&gt;% \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002')) %&gt;% \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') %&gt;%\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) %&gt;% \n  select(-drop) %&gt;% \n  arrange(ds)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "href": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "title": "Prophet Model",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nStarting with some quick visualizations, we can see that the only area that there is a difference in the variation of the stock prices is in the beginning of 2020. I wonder what that could have been .\n\njetblue %&gt;% \n  group_by(year_num, month_num) %&gt;% \n  summarize(var_value = sd(close)^2) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(month_num, var_value)) + \n  geom_point() + \n  facet_wrap(vars(year_num))\n\n\n\n\n\n\n\n\nNext, we can look at the histograms for the outcome of interest. If we look at the histograms, we can see that there are potential outliers in the original stock prices data. We can also see that cleaning the variable removed the potential outliers.\n\nonly_numeric &lt;- jetblue %&gt;% \n  select(close, clean)\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~ggplot(data = only_numeric,\n             aes(.x)) + \n       geom_histogram(color = 'white',\n                      fill = 'dodgerblue') +\n       geom_vline(xintercept = mean(.x) +\n                    sd(.x) +\n                    sd(.x) +\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       geom_vline(xintercept = mean(.x) -\n                    sd(.x) -\n                    sd(.x) -\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       labs(title = .y))\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nThere will also be a lot of use of the purrr package and the map functions, which are part of the tidyverse. We can also see that in the plot series visualization using modeltime's plot_time_series function, that the cleaned stock prices remove the outliers. So from here on out, I’ll be using the cleaned stock prices.\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~only_numeric %&gt;% \n       plot_time_series(jetblue$ds,\n                        .x,\n                        .interactive = FALSE) + \n       labs(title = .y))\n\n$close\n\n\n\n\n\n\n\n\n\n\n$clean\n\n\n\n\n\n\n\n\n\nWe can also look for anomalies, or points that deviate from the trend. Using the plot_anomaly_diagnostics function from the modeltime package, I can see all the anomalies in the data. I also used ggplot to create my own visualization using the same data. Lastly, we’ll deal with those anomalies by removing them from the dataset. This is not too much of a problem because the Prophet model should be able to handle this fairly easy.\n\njetblue %&gt;% \n  plot_anomaly_diagnostics(ds,\n                           clean,\n                           .facet_ncol = 1,\n                           .interactive = FALSE)\n\n\n\n\n\n\n\njetblue %&gt;% \n  tk_anomaly_diagnostics(ds,\n                         clean) %&gt;% \n  ggplot(aes(ds, observed)) + \n  geom_line() + \n  geom_point(aes(color = anomaly)) +\n  viridis::scale_color_viridis(option = 'D',\n                               discrete = TRUE,\n                               begin = .5,\n                               end = 0)\n\n\n\n\n\n\n\nanomaly &lt;- jetblue %&gt;%\n  tk_anomaly_diagnostics(ds,\n                         clean)\n\njetblue &lt;- left_join(jetblue, anomaly) %&gt;%\n  filter(anomaly != 'Yes')\n\nWe can also look into additional regressors to include in the model by looking into seasonality. We can see some fluctuation in stock prices across the years. We’ll include the year variable as another regressor on the stock prices.\n\njetblue %&gt;% \n  plot_seasonal_diagnostics(ds,\n                            clean,\n                            .interactive = FALSE)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "href": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "title": "Prophet Model",
    "section": "Training the Prophet Model",
    "text": "Training the Prophet Model\nBefore we begin, I’m going to designate 10 cores to process any models run.\n\nset.seed(05262022)\n\nparallel::detectCores()\n\n[1] 12\n\nparallel_start(10,\n               .method = 'parallel')\n\nFirst, instead of the normal initial_split used for training and testing splits, we’ll use the initial_time_split function from tidymodels to separate the first 80% of the data into training set and the other 20% into the testing set.\n\nset.seed(05262022)\njet_split &lt;- initial_time_split(jetblue)\n\n\nProphet Model Function\nI decided to create my own Prophet function to be able to use for both training the model and testing it. In this function, I’ve also included parameters that can be changed to see if the model performs better or worse. Lastly, the train = TRUE allows us to practice with the training dataset and then when we’re happy with the model, we can use it to test our model. For our model, we’ll be predicting stock prices with date and comparing each year to the reference year (2002).\n\nprophet_mod &lt;- function(splits,\n                        changepoints = .05,\n                        seasonality = .01,\n                        holiday = .01,\n                        season_type = 'additive',\n                        day_season = 'auto',\n                        week_season = 'auto',\n                        year_season = 'auto',\n                        train = TRUE){\n  library(tidyverse)\n  library(tidymodels)\n  library(modeltime)\n  library(prophet)\n  \n  analy_data &lt;- analysis(splits)\n  assess_data &lt;- assessment(splits)\n  \n  model &lt;- prophet_reg() %&gt;% \n    set_engine(engine = 'prophet',\n               verbose = TRUE) %&gt;% \n    set_args(prior_scale_changepoints = changepoints,\n             prior_scale_seasonality = seasonality,\n             prior_scale_holidays = holiday,\n             season = season_type,\n             seasonality_daily = day_season,\n             seasonality_weekly = week_season,\n             seasonality_yearly = year_season) %&gt;% \n    fit(clean ~ ds + year_num, \n        data = analy_data)\n  \n  if(train == TRUE){\n    train_cali &lt;- model %&gt;% \n      modeltime_calibrate(new_data = analy_data)\n    \n    train_acc &lt;- train_cali %&gt;% \n      modeltime_accuracy()\n    \n    return(list(train_cali, train_acc))\n  }\n  \n  else{\n    test_cali &lt;- model %&gt;% \n      modeltime_calibrate(new_data = assess_data)\n    \n    test_acc &lt;- test_cali %&gt;% \n      modeltime_accuracy()\n    \n    return(list(test_cali, test_acc))\n  }\n}\n\nIt is worth noting that I’m using the modeltime package to run the prophet model because I believe it is easier to use (especially for later steps) than from Prophet but both can be implemented in this function. Let’s try running this model with the some random parameters I chose from the Prophet website until realizing that the modeltime parameters are log transformed.\n\nset.seed(05262022)\nbaseline &lt;- prophet_mod(jet_split,\n                 train = TRUE) %&gt;% \n  pluck(2)\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nConverting to Modeltime Table.\n\nbaseline\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted 0.629  4.06  1.93  4.05 0.831 0.957\n\n\nSo with the model, we can see that the Mean Absolute Scaled Error (MASE) is 1.9270147 and the Root Mean Square Error (RMSE) is 0.8312475. Not bad for an initial run. Let’s look at how the model fits the training data.\n\nprophet_mod(jet_split,\n                 train = TRUE) %&gt;%  \n  pluck(1) %&gt;% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Prophet Baseline Model')\n\n\n\n\n\n\n\n\nSo the model appears to follow the trend line. We’ll try to tune some of these parameters to see if we can make the model better.\n\n\nTuning the Model\nNow, I’ll tune the prior scale values for the model. I’ll use the grid_latin_hypercube from the dials package in tidymodels to choose 5 sets of parameter values to run. I’m also using the rolling_origin from the rsample package in tidymodels because we are working with time series data. This does not create random samples but instead has samples with data points with consecutive values.\n\nset.seed(05262022)\n\nproph_model &lt;- prophet_reg() %&gt;%\n  set_engine(engine = 'prophet',\n             verbose = TRUE) %&gt;%\n  set_args(prior_scale_changepoints = tune(),\n           prior_scale_seasonality = tune(),\n           prior_scale_holidays = tune(),\n           season = 'additive',\n           seasonality_daily = 'auto',\n           seasonality_weekly = 'auto',\n           seasonality_yearly = 'auto')\n\nproph_rec &lt;-\n  recipe(clean ~ ds + year_num,\n         data = training(jet_split))\n\n\nset.seed(05262022)\ntrain_fold &lt;-\n  rolling_origin(training(jet_split),\n                 initial = 270,  \n                 assess = 90, \n                 skip = 30,\n                 cumulative = TRUE)\n\nset.seed(05262022)\ngrid_values &lt;-\n  grid_latin_hypercube(prior_scale_changepoints(),\n                       prior_scale_seasonality(),\n                       prior_scale_holidays(),\n                       size = 5)\n\nset.seed(05262022)\nproph_fit &lt;- tune_grid(object = proph_model,\n                       preprocessor = proph_rec,\n                       resamples = train_fold,\n                       grid = grid_values,\n                       control = control_grid(verbose = TRUE,\n                                              save_pred = TRUE,\n                                              allow_par = TRUE))\n\n\ntuned_metrics &lt;- collect_metrics(proph_fit)\ntuned_metrics %&gt;%\n  filter(.metric == 'rmse') %&gt;% \n  arrange(mean)\n\n# saveRDS(tuned_metrics,\n#         file = 'tuned_metrics.rds')\n\n\nmetrics &lt;-\n  readr::read_rds(here::here('posts/2022-06-02-prophet-model/tuned_metrics.rds'))\n\nmetrics %&gt;% \n  filter(.metric == 'rmse') %&gt;% \n  arrange(mean)\n\n# A tibble: 5 × 9\n  prior_scale_changepoints prior_scale_seasonality prior_scale_holidays .metric\n                     &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;  \n1                  3.53                    0.0170               1.12    rmse   \n2                  0.884                  36.4                  0.0131  rmse   \n3                  0.00139                 0.00166              0.00172 rmse   \n4                  0.0549                  0.261                0.231   rmse   \n5                 43.0                     3.80                12.2     rmse   \n# ℹ 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;,\n#   .config &lt;chr&gt;\n\n\nFor the sake of not waiting for this to render, I decided to make a RDS file of the metrics gathered from the tuned Prophet model. We can see that the RMSE value was 2.4252669 and the prior scale changepoint value was 3.5347457, the prior scale seasonality value was 0.0170306, and the prior scale holiday value was 1.1198542.\n\n\nFinal Training Model\nI then decided to run the prophet model on the training dataset with the new parameter values.\n\nfinal_train &lt;- prophet_mod(jet_split,\n                 changepoints = 3.53,\n                 seasonality = .017,\n                 holiday = 1.12,\n                 train = TRUE) %&gt;%  \n  pluck(2)\n\nfinal_train\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Fitted 0.501  3.24  1.54  3.23 0.659 0.973\n\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = TRUE) %&gt;%  \n  pluck(1) %&gt;% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Training Model')\n\n\n\n\n\n\n\n\nWe can see that when using the whole training set, we have a RMSE of 0.6588232 and a MASE of 1.5354125 so both metrics reduced slightly."
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "href": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "title": "Prophet Model",
    "section": "Testing the Model",
    "text": "Testing the Model\nFinally, let’s test our Prophet model to see how well the model fits.\n\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %&gt;%\n  pluck(1) %&gt;% \n  modeltime_forecast(new_data = testing(jet_split),\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Testing Model')\n\n\n\n\n\n\n\ntest_model &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %&gt;%\n  pluck(2)\n\ntest_model\n\n# A tibble: 1 × 9\n  .model_id .model_desc           .type   mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         1 PROPHET W/ REGRESSORS Test   1.11  8.80  4.57  8.58  1.35 0.0196\n\n\nWell, that doesn’t look very good and we can see that with the metrics. The MASE has gotten much worse (4.5736587) and so has the RMSE (1.3542052)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "href": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "title": "Prophet Model",
    "section": "Forecasting Ahead a Year",
    "text": "Forecasting Ahead a Year\nWell our model did not fit well to the testing data, but let’s see how it model looks when refit to the full data and forecasted forward a year. So in a year, it seems that JetBlue stock will remain roughly around the same value. It is important to note that the confidence intervals are large and with 95% confidence that values could be between 52.49 and -28.39 (not possible), there is not much confidence that JetBlue stock prices will remain where they are now in a year.\n\nfuture &lt;- jetblue %&gt;% \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture &lt;-\n  future %&gt;%\n  select(-year_num, -month_num, -day_num) %&gt;%\n  mutate(date2 = ds) %&gt;%\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %&gt;%\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num)) %&gt;% \n  arrange(ds)\n\nglimpse(future)\n\nRows: 1,357\nColumns: 17\n$ close         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ ds            &lt;date&gt; 2002-04-12, 2002-04-15, 2002-04-16, 2002-04-17, 2002-04…\n$ actual_day    &lt;ord&gt; Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, M…\n$ clean         &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ observed      &lt;dbl&gt; 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, …\n$ season        &lt;dbl&gt; 0.017592887, 0.001065775, -0.001828751, -0.005043169, -0…\n$ trend         &lt;dbl&gt; 13.47847, 13.48774, 13.49702, 13.50629, 13.51556, 13.524…\n$ remainder     &lt;dbl&gt; -0.16606354, -0.08880934, 0.07481227, -0.14124623, -0.40…\n$ seasadj       &lt;dbl&gt; 13.31241, 13.39893, 13.57183, 13.36504, 13.11179, 12.912…\n$ remainder_l1  &lt;dbl&gt; -3.129268, -3.129268, -3.129268, -3.129268, -3.129268, -…\n$ remainder_l2  &lt;dbl&gt; 3.177322, 3.177322, 3.177322, 3.177322, 3.177322, 3.1773…\n$ anomaly       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ recomposed_l1 &lt;dbl&gt; 10.36680, 10.35954, 10.36592, 10.37198, 10.37451, 10.413…\n$ recomposed_l2 &lt;dbl&gt; 16.67339, 16.66613, 16.67251, 16.67857, 16.68110, 16.719…\n$ year_num      &lt;fct&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 20…\n$ month_num     &lt;fct&gt; 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 05, …\n$ day_num       &lt;dbl&gt; 12, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 29, 30, 1, 2…\n\ntest_model1 &lt;- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %&gt;%\n  pluck(1)\n\ntest_model1 %&gt;% \n  modeltime_refit(data = future) %&gt;% \n  modeltime_forecast(new_data = future,\n                     actual_data = jetblue) %&gt;% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Forecasted JetBlue Stock Prices')"
  },
  {
    "objectID": "posts/2024-03-09-bayesian-networks-pt-1/index.html",
    "href": "posts/2024-03-09-bayesian-networks-pt-1/index.html",
    "title": "Bayes Nets Pt. 1",
    "section": "",
    "text": "Under Development\nAs I am continuing to grow in understanding and conducting bayesian networks, this page and series may change in the future. -JP\nOkay, I will be the first to state that I am not an expert in the field of conducint bayeaian networks, bayesian analyses, statistics (the list goes on), but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses the programming language Stan. There are several tutorials on how to download Stan using either R or Python, so I will not be covering that. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using Python and Stan. Additionally, I will be creating data that will represent educational assessment data, with latent variables representing proficiency in certain skills (e.g., math, English/language arts, and science) for students. While most of my experience of using bayes nets is to represent measurement models, bayes net can be used outside of this field. Bayes net is similar to path analysis and structural equation modeling; however, EXPLAIN DIFFERENCE BETWEEN THE TWO METHODS. I will also start referring to everything in this series in a bayesian network framework. For instance, instead of using variables, whether they are observed or unobserved (latent), I will be referring to them as nodes and latent nodes, respectively. When it comes to showing the “paths” between nodes, I wwill now be referring to them as edges. Lastly, any image that shows all of the nodes and the edges connecting to one another will be referred to as a directed acyclic graph or DAG.\nOkay, now on to this post. For this post I will simply discuss creating the data in R to be used in Stan, as well as creating the object of data that will be used in the Stan calculations. One last comment before diving in, I will be using cmdstanr instead of rstan for my Stan computations."
  },
  {
    "objectID": "posts/2024-03-09-bayesian-networks-pt-1/index.html#getting-the-data-set-up",
    "href": "posts/2024-03-09-bayesian-networks-pt-1/index.html#getting-the-data-set-up",
    "title": "Bayes Nets Pt. 1",
    "section": "Getting the Data Set Up",
    "text": "Getting the Data Set Up\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(bayestestR)\nlibrary(bayesplot)\nlibrary(posterior)\n\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 1000, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .8),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nThe first thing I am going to do is load in all the necessary libraries that you need. Then I decided to create a function that would create a binomial distribution with a single trial, so essentially a bernoulii distribution. I decided on some random numbers for the probabilities of correct responses to the 15 different items and decided to create some fake studentids for each row.\n\n\n\n\n\n\n\n\n\nI decided to create a simple table that shows all of the students and their responses for the 15 items in this assessment. I’m not sure why I have all the data in the table, but I used some pagination so there is not a laundry list of rows with 0s and 1s clogging up this post…hopefully.\n\n# map(y |&gt; select(-studentid), table)\n# map(y |&gt; select(-studentid), ~round(prop.table(table(.x)), 2))\n\nmap(y |&gt; select(-studentid), table)[[1]]\n\n\n  0   1 \n199 801 \n\n\nAfter seeing that the data looks correct, I am also neurotic and need to make sure that my created data is how I imagined it would be. So I looped through each of my items to make sure the proportions are correct. More importantly, I like to see the counts of the data and get an understanding of how many are answering each item correctly. I commented out the loop and am only going to show the counts for the first item. So seeing at how my function had approximately 80% of the students answering the item correctly, I can now see that 801 answered item 1 correctly.\n\nQ Matrix\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  gt::gt() |&gt;\n  gt::opt_interactive(\n    use_filters = TRUE,\n    use_resizers = TRUE,\n    use_highlight = TRUE,\n    use_compact_mode = TRUE\n  )\n\n\n\n\n\n\n\n\nOkay, now on to the Q-matrix. This is the only other piece of information we may need for our model in Stan. WARNING I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The gt table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column. So now I believe we have everything we need to get started on a bayes net using Stan and Markov chain Monte Carlo (MCMC) sampling.\n\n\nStan Data\n\nstan_data &lt;- list(\n  J = nrow(y[, -1]), # Number of students/rows\n  I = ncol(y[, -1]), # Number of items\n  K = ncol(q_matrix[, -1]), #Number of latent attributes/skills\n  y = y[,-1], # Student responses on all items\n  Q = q_matrix[,-1] # Items that measure each attribute\n)\n\nprint(stan_data)\nglimpse(stan_data)"
  },
  {
    "objectID": "posts/2024-03-16-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-03-16-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Under Development - Not Complete\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ndata { int&lt;lower=1&gt; J; // number of examinees int&lt;lower=1&gt; I; // number of items int&lt;lower=1&gt; K; // number of latent variables int&lt;lower=1&gt; C; // number of classes matrix[J, I] X; // response matrix x matrix[I, K] Q; // Q matrix Q matrix[C, K] alpha; // attribute profile matrix } parameters { simplex[C] nu; // class probabilities vector&lt;lower=0, upper=1&gt;[I] false_pos; vector&lt;lower=0, upper=1&gt;[I] true_pos; real&lt;lower=0, upper=1&gt; lambda1; real&lt;lower=0, upper=1&gt; lambda20; real&lt;lower=0, upper=1&gt; lambda21; real&lt;lower=0, upper=1&gt; lambda30; real&lt;lower=0, upper=1&gt; lambda31; real&lt;lower=0, upper=1&gt; lambda40; real&lt;lower=0, upper=1&gt; lambda41; real&lt;lower=0, upper=1&gt; lambda50; real&lt;lower=0, upper=1&gt; lambda51; } transformed parameters { vector[C] log_nu; log_nu = log(nu); } model { vector[2] theta_log1; vector[2] theta_log2; vector[2] theta_log3; vector[2] theta_log4; vector[2] theta_log5; vector[C] theta1; vector[C] theta2; vector[C] theta3; vector[C] theta4; vector[C] theta5; matrix[I, C] delta; real pie; vector[I] log_item; vector[C] log_lik;\n// Priors lambda1 ~ beta(25, 5); lambda20 ~ beta(10, 20); lambda21 ~ beta(20, 10); lambda30 ~ beta(5, 25); lambda31 ~ beta(25, 5); lambda40 ~ beta(5, 25); lambda41 ~ beta(25, 5); lambda50 ~ beta(12, 18); lambda51 ~ beta(18, 12);\nfor (i in 1 : I) { false_pos[i] ~ beta(4, 26); true_pos[i] ~ beta(26, 4); }\ntheta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1); theta_log1[2] = bernoulli_lpmf(1 | lambda1);\ntheta_log2[1] = bernoulli_lpmf(1 | lambda20); theta_log2[2] = bernoulli_lpmf(1 | lambda21);\ntheta_log3[1] = bernoulli_lpmf(1 | lambda30); theta_log3[2] = bernoulli_lpmf(1 | lambda31);\ntheta_log4[1] = bernoulli_lpmf(1 | lambda40); theta_log4[2] = bernoulli_lpmf(1 | lambda41);\ntheta_log5[1] = bernoulli_lpmf(1 | lambda50); theta_log5[2] = bernoulli_lpmf(1 | lambda51);\nfor (c in 1 : C) { if (alpha[c, 1] &gt; 0) { theta1[c] = theta_log1[2]; } else { theta1[c] = theta_log1[1]; } if (alpha[c, 2] &gt; 0) { theta2[c] = theta_log2[2]; } else { theta2[c] = theta_log2[1]; } if (alpha[c, 3] &gt; 0) { theta3[c] = theta_log3[2]; } else { theta3[c] = theta_log3[1]; } if (alpha[c, 4] &gt; 0) { theta4[c] = theta_log4[2]; } else { theta4[c] = theta_log4[1]; } if (alpha[c, 5] &gt; 0) { theta5[c] = theta_log5[2]; } else { theta5[c] = theta_log5[1]; } }\n//Likelihood for (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2]) * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4]) * pow(exp(theta5[c]), Q[i, 5]);\n    pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n    log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n  }\n  log_lik[c] = log_nu[c] + sum(log_item);\n}\ntarget += log_sum_exp(log_lik);\n} } generated quantities { vector[2] theta_log1; vector[2] theta_log2; vector[2] theta_log3; vector[2] theta_log4; vector[2] theta_log5; vector[C] theta1; vector[C] theta2; vector[C] theta3; vector[C] theta4; vector[C] theta5; matrix[I, C] delta; real pie; vector[I] log_item;\nmatrix[J, C] prob_resp_class; // posterior probabilities of respondent j being in latent class c matrix[J, K] prob_resp_attr; // posterior probabilities of respondent j being a master of attribute k row_vector[C] prob_joint; vector[C] prob_attr_class;\nmatrix[J, I] x_rep;\ntheta_log1[1] = bernoulli_lpmf(1 | 1 - lambda1); theta_log1[2] = bernoulli_lpmf(1 | lambda1);\ntheta_log2[1] = bernoulli_lpmf(1 | lambda20); theta_log2[2] = bernoulli_lpmf(1 | lambda21);\ntheta_log3[1] = bernoulli_lpmf(1 | lambda30); theta_log3[2] = bernoulli_lpmf(1 | lambda31);\ntheta_log4[1] = bernoulli_lpmf(1 | lambda40); theta_log4[2] = bernoulli_lpmf(1 | lambda41);\ntheta_log5[1] = bernoulli_lpmf(1 | lambda50); theta_log5[2] = bernoulli_lpmf(1 | lambda51);\nfor (c in 1 : C) { if (alpha[c, 1] &gt; 0) { theta1[c] = theta_log1[2]; } else { theta1[c] = theta_log1[1]; } if (alpha[c, 2] &gt; 0) { theta2[c] = theta_log2[2]; } else { theta2[c] = theta_log2[1]; } if (alpha[c, 3] &gt; 0) { theta3[c] = theta_log3[2]; } else { theta3[c] = theta_log3[1]; } if (alpha[c, 4] &gt; 0) { theta4[c] = theta_log4[2]; } else { theta4[c] = theta_log4[1]; } if (alpha[c, 5] &gt; 0) { theta5[c] = theta_log5[2]; } else { theta5[c] = theta_log5[1]; } }\nfor (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { delta[i, c] = pow(exp(theta1[c]), Q[i, 1]) * pow(exp(theta2[c]), Q[i, 2]) * pow(exp(theta3[c]), Q[i, 3]) * pow(exp(theta4[c]), Q[i, 4]) * pow(exp(theta5[c]), Q[i, 5]);\n    pie = pow(true_pos[i], delta[i, c]) * pow(false_pos[i], (1 - delta[i, c]));\n    log_item[i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie);\n    }\n  prob_joint[c] = nu[c] * exp(sum(log_item)); //here is where the problem starts with trying to correctly classify students with proficiency mastery\n}\nprob_resp_class[j] = prob_joint / sum(prob_joint);\n}\nfor (j in 1 : J) { for (k in 1 : K) { for (c in 1 : C) { // Calculate the probability of mastering attribute k given class c prob_attr_class[c] = prob_resp_class[j, c] * alpha[c, k]; } // Sum the probabilities to get the posterior probability of mastering attribute k prob_resp_attr[j, k] = sum(prob_attr_class); } }\nfor (j in 1 : J) { for (c in 1 : C) { for (i in 1 : I) { x_rep[j, i] = X[j, i] * log(pie) + (1 - X[j, i]) * log(1 - pie); } } } }"
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html",
    "title": "Using Typst To Create Documents",
    "section": "",
    "text": "Under Development - Not Complete\nI have been using Typst, an awesome app for working on pdf files at the same time as colleagues. You could think of this as something similar to Google Docs or GitHub for code. This also got me thinking about creating a small series of blog posts about using Typst and then creating Typst templates for documents using Quarto. The latter topics would be using Quarto extensions and if following along, you would need Quarto version 1.4 at least to be able to use Typst code chunks on a Quarto document. So first, I will show the Typst file I will be using because let’s face it I’m on the job market and free publicity is always good.\n.\nHere is the link for the Typst resume to view. If you want, you can just copy and paste that into Typst and change the information. I will walk through each section of the document with Typst code in the post, as well as a cover letter post, and then end the series with a Quarto extension to create a Typst template so you can just write your resume and/or cover letter in Quarto."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#typst-documentation",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#typst-documentation",
    "title": "Using Typst To Create Documents",
    "section": "Typst Documentation",
    "text": "Typst Documentation\nI will be the first to state that Typst documentation is a little difficult to follow at first. Hopefully with this tutorial you will get a better understanding of the basics of Typst code. As someone who tried to learn LaTeX to edit the previous resume I had found a template for, I wish Typst existed earlier."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "title": "Using Typst To Create Documents",
    "section": "Creating a Typst Document",
    "text": "Creating a Typst Document\nWorking in the Typst app is pretty straightforward with you creating a username, followed by your dashboard with nothing there. This will be the location of all of your documents as you get started with Typst. While there are Typst templates already for resumes I really wanted to create something similar to the resume I had in LaTeX. Working in Quarto, you will have to learn how to create Typst code chunks. They are slightly different from other languages’ code chunks but you can still use all of the Quarto code chunk arguments.\nFrom what I have seen online, there does not seem to be much difference in the ordering of some of the beginning Typst documentation. I have decided to start my Typst document with any variables I will be including, followed by any Typst packages I will need, and then setting up the general parameters for the document. These general parameters are for the document overall. Things like setting the font to a specific font, size and maybe weight would be a good parameter to set at the top of your document."
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "title": "Using Typst To Create Documents",
    "section": "Breakdown of Variables, Packages, and # Set function for parameters",
    "text": "Breakdown of Variables, Packages, and # Set function for parameters\n\n//Variables\n#let name = [Jonathan A. Pedroza Ph.D]\n\n//Packages\n#import \"@preview/tablex:0.0.8\": tablex\n#import \"lib-gen.typ\": *\n#import \"lib-impl.typ\": *\n#import \"lib.typ\": *\n\n#set page(\n  margin: (\n    top: 0cm,\n    bottom: 0cm,\n    left: .5cm,\n    right: 0cm\n  )\n)\n\n#set block(spacing: 0.5em)\n\n#set rect(\n  width: 37%,\n  height: 100%\n)\n\nAbove is the beginning Typst code for the resume I created. I’m going to walk through the code a little, but for more detailed information, check out the help documentation. The // syntax can be included wherever to include comments. Since I have been showing others how to use Typst to create quick pretty PDFs, I have been including a lot of comments for things like variables. To create variables in Typst, you will need to use the #let function followed by your variable name, an equal sign, and the information you want to include. So I created the variable name, which would be used as #name in Typst and the document will spell out my full name. After that, just as the comment states, I included the tablex package, which I have found to be useful for creating tables and grids. If you’d like you can use the #table or #grid functions from Typst. Additionally, I also included the files for using the FontAwesome Typst package. You can find all the icons and other information about FontAwesome at the FontAwesome website. I was just being lazy as I wanted to create my resume quick so I could get it out into the world ASAP. Next I set the margins to maximize the amount of space I would have for my resume and I created a block after the titles Education and Professional Experience since I did not want the default amount of space before my education and experience entries. The #set function creates rules for the document as a whole. So For the whole document I have the same margins throughout, a block to create more space between headers and text underneath and the rectangle to separate the sections of the document. Lastly, I set a rectangle for 37% of the document’s width and 100% of the height. There are other metrics that can be used to create the rectangle but I personally was enjoying using percentages for this document. This rectangle is for the right side of the document that includes the contact information."
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html",
    "title": "Data Manipulation in R & Python",
    "section": "",
    "text": "One of my favorite posts is the comparison between data.table and the tidyverse’s dplyr packages. Here is the link to that post. I have used that when trying to build my competence in using data.table. Now I’m going to try and expand on that by creating this post that compares cases of using dplyr, data.table, and now pandas. Hopefully this can be as useful as the comparison between dplyr an data.table post was for me. This is not an extensive way of comparing them but just to get started for anyone that wants to use python more."
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-integers",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-integers",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Integers)",
    "text": "Filtering (Integers)\n\nr_data |&gt;\n  filter(\n    x &gt; 1\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 3\n      x     x2     y\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.09 -0.457     1\n2  1.71  2.05      0\n3  1.15 -3.56      1\n4  1.37  3.76      1\n5  1.30  3.83      1\n6  3.01  2.09      0\n\n\n\nhead(\n  r_table[x &gt; 1]\n)\n\n          x         x2     y\n      &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1: 1.144979 -5.2480202     1\n2: 2.284941  5.9637749     0\n3: 1.196674 -0.6543321     1\n4: 1.890398 -1.8352037     0\n5: 1.850590 -1.2532883     1\n6: 2.428723  0.2854589     0\n\n\n\npy_data[py_data[\"x\"] &gt; 1].head()\n\n           x        x2  y\n0   1.625495 -2.513493  1\n1   1.375477  1.419490  1\n3   1.279369  3.155540  0\n5   1.212755  2.003532  0\n11  1.412002 -2.758251  1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-categorical",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-categorical",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering (Categorical)",
    "text": "Filtering (Categorical)\n\nr_data |&gt;\n  filter(\n    y == 1\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 3\n       x     x2     y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.09  -0.457     1\n2 -0.363 -1.66      1\n3  1.15  -3.56      1\n4  0.674  0.632     1\n5  1.37   3.76      1\n6 -1.82  -0.946     1\n\n\n\nhead(\n  r_table[y == 1]\n)\n\n            x         x2     y\n        &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1:  0.3012969 -0.1892336     1\n2:  1.1449794 -5.2480202     1\n3:  0.6039375  3.4301031     1\n4: -2.8776286 -5.6320773     1\n5:  1.1966740 -0.6543321     1\n6:  1.8505904 -1.2532883     1\n\n\n\npy_data[py_data[\"y\"] == 1].head()\n\n          x        x2  y\n0  1.625495 -2.513493  1\n1  1.375477  1.419490  1\n4 -0.992507  2.701708  1\n8 -0.284489 -0.317201  1\n9 -0.110960 -0.468932  1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#filtering-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Filtering Multiple Columns",
    "text": "Filtering Multiple Columns\n\nr_data |&gt;\n  filter(\n    y == 1 &\n    x2 &lt; 0\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 3\n       x     x2     y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.09  -0.457     1\n2 -0.363 -1.66      1\n3  1.15  -3.56      1\n4 -1.82  -0.946     1\n5  0.446 -1.72      1\n6 -0.369 -6.86      1\n\n\n\nhead(\n  r_table[\n    y == 1 &\n    x2 &gt; 0\n  ]\n)\n\n            x        x2     y\n        &lt;num&gt;     &lt;num&gt; &lt;int&gt;\n1:  0.6039375 3.4301031     1\n2: -0.6132135 2.1888315     1\n3: -2.4182992 1.5879010     1\n4: -2.0788204 0.6814116     1\n5:  0.2546547 0.1272056     1\n6:  2.0922497 4.6317957     1\n\n\n\npy_data[\n  (py_data[\"y\"] == 1) & \n  (py_data[\"x2\"] &gt; 0)\n    ].head()\n\n           x        x2  y\n1   1.375477  1.419490  1\n4  -0.992507  2.701708  1\n10 -0.404824  0.780289  1\n14  0.237784  2.241245  1\n23  0.490488  0.160291  1"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#sorting-rows",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#sorting-rows",
    "title": "Data Manipulation in R & Python",
    "section": "Sorting Rows",
    "text": "Sorting Rows\n\nr_data |&gt; \n  arrange(y) |&gt;\n  head()\n\n# A tibble: 6 × 3\n       x     x2     y\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1  1.71   2.05      0\n2 -0.651  0.850     0\n3 -1.61   2.95      0\n4  0.196 -4.13      0\n5  3.01   2.09      0\n6  0.228  0.598     0\n\n\n\nhead(\n  r_table[order(y)]\n)\n\n            x         x2     y\n        &lt;num&gt;      &lt;num&gt; &lt;int&gt;\n1:  0.6129881  0.5847694     0\n2:  2.2849412  5.9637749     0\n3: -1.0921220  7.5136606     0\n4:  0.7583684 -1.8330110     0\n5:  1.8903981 -1.8352037     0\n6: -0.9942675  0.1554529     0\n\n\n\npy_data.sort_values(by = \"y\").head()\n\n            x        x2  y\n499  0.938749  0.135750  0\n546 -0.424556 -2.931917  0\n544  0.147953 -2.487124  0\n542 -1.651471  0.683126  0\n541 -0.263890  1.406574  0"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-specific-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-specific-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Specific Columns",
    "text": "Selecting Specific Columns\n\nr_data |&gt;\n  select(\n    y\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 1\n      y\n  &lt;int&gt;\n1     1\n2     0\n3     1\n4     1\n5     1\n6     0\n\n\n\nhead(\n  r_table[,\"y\"]\n)\n\n       y\n   &lt;int&gt;\n1:     1\n2:     1\n3:     0\n4:     1\n5:     1\n6:     0\n\n\n\npy_data[\"y\"].head()\n\n0    1\n1    1\n2    0\n3    0\n4    1\nName: y, dtype: int32\n\n\n# py_data.filter(items = \"y\").head()"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-multiple-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Multiple Columns",
    "text": "Selecting Multiple Columns\n\nr_data |&gt; \n  select(x, x2) |&gt; \n  head()\n\n# A tibble: 6 × 2\n       x     x2\n   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.09  -0.457\n2  1.71   2.05 \n3 -0.363 -1.66 \n4  1.15  -3.56 \n5  0.674  0.632\n6 -0.651  0.850\n\n\n\nhead(\n  r_table[,list(x, x2)]\n)\n\n            x         x2\n        &lt;num&gt;      &lt;num&gt;\n1:  0.3012969 -0.1892336\n2:  1.1449794 -5.2480202\n3:  0.6129881  0.5847694\n4:  0.6039375  3.4301031\n5: -2.8776286 -5.6320773\n6:  2.2849412  5.9637749\n\n\n\n# py_data[{\"x\", \"x2\"}].head()\n\npy_data.filter(items = [\"x\", \"x2\"]).head()\n\n          x        x2\n0  1.625495 -2.513493\n1  1.375477  1.419490\n2  0.896304  3.582105\n3  1.279369  3.155540\n4 -0.992507  2.701708"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-using-regex",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#selecting-using-regex",
    "title": "Data Manipulation in R & Python",
    "section": "Selecting Using Regex",
    "text": "Selecting Using Regex\n\nr_data |&gt;\n  select(\n    matches(\"x\")\n  ) |&gt;\n  head()\n\n# A tibble: 6 × 2\n       x     x2\n   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.09  -0.457\n2  1.71   2.05 \n3 -0.363 -1.66 \n4  1.15  -3.56 \n5  0.674  0.632\n6 -0.651  0.850\n\n\n\ncols &lt;- grep(\"^x\", names(r_table))\n\nhead(\n  r_table[, ..cols]\n)\n\n            x         x2\n        &lt;num&gt;      &lt;num&gt;\n1:  0.3012969 -0.1892336\n2:  1.1449794 -5.2480202\n3:  0.6129881  0.5847694\n4:  0.6039375  3.4301031\n5: -2.8776286 -5.6320773\n6:  2.2849412  5.9637749\n\n\n\npy_data.filter(regex = \"x\").head()\n\n          x        x2\n0  1.625495 -2.513493\n1  1.375477  1.419490\n2  0.896304  3.582105\n3  1.279369  3.155540\n4 -0.992507  2.701708"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#summarize-data",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#summarize-data",
    "title": "Data Manipulation in R & Python",
    "section": "Summarize Data",
    "text": "Summarize Data\n\nr_data |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n# A tibble: 1 × 1\n     avg\n   &lt;dbl&gt;\n1 0.0279\n\n  r_data |&gt;\n  summarize(\n    total = sum(x)\n  )\n\n# A tibble: 1 × 1\n  total\n  &lt;dbl&gt;\n1  27.9\n\n\n\nr_table[, .(avg = mean(x))]\n\n           avg\n         &lt;num&gt;\n1: -0.01425028\n\nr_table[, .(total = sum(x))]\n\n       total\n       &lt;num&gt;\n1: -14.25028\n\n\n\npy_data[\"x\"].mean()\n\n0.002775922653748875\n\npy_data[\"x\"].sum()\n\n2.775922653748875"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#addingupdatingdeleting-columns",
    "title": "Data Manipulation in R & Python",
    "section": "Adding/Updating/Deleting Columns",
    "text": "Adding/Updating/Deleting Columns\n\nr_data &lt;- r_data |&gt;\n  mutate(\n    x_mult = x*x2\n  )\nhead(r_data)\n\n# A tibble: 6 × 4\n       x     x2     y x_mult\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n1  1.09  -0.457     1 -0.500\n2  1.71   2.05      0  3.49 \n3 -0.363 -1.66      1  0.604\n4  1.15  -3.56      1 -4.10 \n5  0.674  0.632     1  0.426\n6 -0.651  0.850     0 -0.554\n\n\n\nr_table[, x_mult := x*x2]\nhead(r_table[, \"x_mult\"])\n\n        x_mult\n         &lt;num&gt;\n1: -0.05701551\n2: -6.00887504\n3:  0.35845670\n4:  2.07156775\n5: 16.20702631\n6: 13.62687485\n\n\n\npy_data[\"x_mult\"] = py_data[\"x\"] * py_data[\"x2\"]\npy_data[\"x_mult\"].head()\n\n0   -4.085671\n1    1.952477\n2    3.210656\n3    4.037102\n4   -2.681463\nName: x_mult, dtype: float64"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#counting",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#counting",
    "title": "Data Manipulation in R & Python",
    "section": "Counting",
    "text": "Counting\n\nr_data |&gt; count(y)\n\n# A tibble: 2 × 2\n      y     n\n  &lt;int&gt; &lt;int&gt;\n1     0   383\n2     1   617\n\n\n\nr_table[, .N, by = (y)]\n\n       y     N\n   &lt;int&gt; &lt;int&gt;\n1:     1   619\n2:     0   381\n\n\n\npy_data[\"y\"].value_counts()\n\ny\n1    590\n0    410\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#group-by",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#group-by",
    "title": "Data Manipulation in R & Python",
    "section": "Group By",
    "text": "Group By\n\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  )\n\n# A tibble: 2 × 2\n      y     avg\n  &lt;int&gt;   &lt;dbl&gt;\n1     0 0.0599 \n2     1 0.00810\n\n\n\nr_table[, .(avg = mean(x)), by = \"y\"]\n\n       y          avg\n   &lt;int&gt;        &lt;num&gt;\n1:     1 -0.009975402\n2:     0 -0.021195561\n\n\n\npy_data.groupby(\"y\")[[\"x\"]].mean()\n\n          x\ny          \n0  0.041513\n1 -0.024143"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#chain-expressions",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#chain-expressions",
    "title": "Data Manipulation in R & Python",
    "section": "Chain Expressions",
    "text": "Chain Expressions\n\nr_data |&gt;\n  group_by(y) |&gt;\n  summarize(\n    avg = mean(x)\n  ) |&gt;\n  filter(\n    y == 1\n  )\n\n# A tibble: 1 × 2\n      y     avg\n  &lt;int&gt;   &lt;dbl&gt;\n1     1 0.00810\n\n\n\nr_table[, \n  by = y,\n  .(avg = mean(x))\n  ][\n    y == 1\n  ]\n\n       y          avg\n   &lt;int&gt;        &lt;num&gt;\n1:     1 -0.009975402\n\n\n\npy_group = py_data.groupby(\"y\")[[\"x\"]].mean()\n\npy_group.loc[1]\n\nx   -0.024143\nName: 1, dtype: float64"
  },
  {
    "objectID": "posts/2024-07-09-dplyr-datatable-python/index.html#pivot-data",
    "href": "posts/2024-07-09-dplyr-datatable-python/index.html#pivot-data",
    "title": "Data Manipulation in R & Python",
    "section": "Pivot Data",
    "text": "Pivot Data\n\nr_data |&gt;\n  pivot_longer(\n    -y\n  )\n\n# A tibble: 3,000 × 3\n       y name    value\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1     1 x       1.09 \n 2     1 x2     -0.457\n 3     1 x_mult -0.500\n 4     0 x       1.71 \n 5     0 x2      2.05 \n 6     0 x_mult  3.49 \n 7     1 x      -0.363\n 8     1 x2     -1.66 \n 9     1 x_mult  0.604\n10     1 x       1.15 \n# ℹ 2,990 more rows\n\n\n\nmelt(r_table, id.vars = \"y\")\n\n          y variable      value\n      &lt;int&gt;   &lt;fctr&gt;      &lt;num&gt;\n   1:     1        x  0.3012969\n   2:     1        x  1.1449794\n   3:     0        x  0.6129881\n   4:     1        x  0.6039375\n   5:     1        x -2.8776286\n  ---                          \n2996:     0   x_mult -2.3109308\n2997:     0   x_mult -4.0893226\n2998:     0   x_mult  0.3624624\n2999:     0   x_mult  2.1338842\n3000:     0   x_mult -4.9910036\n\n\n\npy_data[\"id\"] = py_data.index\n\npy_data.head()\n\n          x        x2  y    x_mult  id\n0  1.625495 -2.513493  1 -4.085671   0\n1  1.375477  1.419490  1  1.952477   1\n2  0.896304  3.582105  0  3.210656   2\n3  1.279369  3.155540  0  4.037102   3\n4 -0.992507  2.701708  1 -2.681463   4\n\npy_pivot = py_data.pivot(index='id', columns='y', values=['x', 'x2', 'x_mult'])\n\nprint(py_pivot.head())\n\n           x                  x2              x_mult          \ny          0         1         0         1         0         1\nid                                                            \n0        NaN  1.625495       NaN -2.513493       NaN -4.085671\n1        NaN  1.375477       NaN  1.419490       NaN  1.952477\n2   0.896304       NaN  3.582105       NaN  3.210656       NaN\n3   1.279369       NaN  3.155540       NaN  4.037102       NaN\n4        NaN -0.992507       NaN  2.701708       NaN -2.681463"
  },
  {
    "objectID": "posts/2024-07-02-resume-cover-letter-typst/index.html#grid-of-entries",
    "href": "posts/2024-07-02-resume-cover-letter-typst/index.html#grid-of-entries",
    "title": "Using Typst To Create Documents",
    "section": "Grid of Entries",
    "text": "Grid of Entries\n\n#grid(\n  columns: (70%, 82%),\n  [\n    #linebreak()\n\n    #set text(\n      font: \"Source Sans Pro\",\n      size: 10pt\n    )\n    #set align(center)\n    \n    = Education #fa-graduation-cap()\n    #line(\n      length: 94%,\n      stroke: black\n    )\n\nI decided to separate some of these functions because it might be easier to talk about. So the grid here is actually separating the main text and the contact information text. As I am reading this, I can see that I probably should have set the font to be 10pt throughout the document rather than within the grid."
  },
  {
    "objectID": "posts/2024-07-09-optimal-threshold/index.html",
    "href": "posts/2024-07-09-optimal-threshold/index.html",
    "title": "Finding Optimal Thresholds",
    "section": "",
    "text": "Under Development - Not Complete\nNow that I’m playing catch up with some posts I have wanted to write, I thought now would be an excellent time to write about this method I have been trying out to figure out the closest optimal threshold. While I found other ways to find the optimal threshold at a much faster rate, I still thought this was an interesting use of machine learning to try and figure out the optimal threshold. Particularly, this is a method to try and find an optimal threshold when the truth is not known. From what I could find there was not much literature on trying to find an optimal threshold when the truth was not known. So I’ll first show this method when the truth is known followed by trying this method out when there is no truth."
  },
  {
    "objectID": "posts/2024-07-09-optimal-threshold/index.html#fabricating-some-data",
    "href": "posts/2024-07-09-optimal-threshold/index.html#fabricating-some-data",
    "title": "Finding Optimal Thresholds",
    "section": "Fabricating Some Data",
    "text": "Fabricating Some Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071)\ncat_map &lt;- purrr::map\n\nn &lt;- 500\nseed &lt;- 12345\ntruth &lt;- rbinom(n = n, size = 1, prob = .6)\nestimates &lt;- bayestestR::distribution_beta(n = n, shape1 = 12, shape2 = 18)\n\ndata &lt;- tibble(\n  estimates = estimates,\n  truth = truth\n)\n\ndata |&gt; head()\n\n# A tibble: 6 × 2\n  estimates truth\n      &lt;dbl&gt; &lt;int&gt;\n1     0.160     0\n2     0.181     0\n3     0.192     0\n4     0.200     1\n5     0.206     1\n6     0.212     1\n\ndata_train &lt;- data |&gt; slice_sample(prop = .75)\ndata_test &lt;- anti_join(data, data_train)\n\nJoining with `by = join_by(estimates, truth)`\n\n\n\nset.seed(seed)\nclass_weights &lt;- data_train |&gt; count(truth) |&gt; arrange(n)\ncls_weights &lt;- class_weights[2, 2]/class_weights[1, 2]\n\nsvm_mod &lt;- svm(\n  truth ~ estimates,\n  data = data_train,\n  type = \"C-classification\",\n  kernel = \"radial\",\n  cost = 10,\n  class.weights = c(\"0\" = as.numeric(cls_weights), \"1\" = 1),\n  probability = TRUE\n)\nsummary(svm_mod)\n\n\nCall:\nsvm(formula = truth ~ estimates, data = data_train, type = \"C-classification\", \n    kernel = \"radial\", cost = 10, class.weights = c(`0` = as.numeric(cls_weights), \n        `1` = 1), probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  10 \n\nNumber of Support Vectors:  354\n\n ( 198 156 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\nsvm_pred &lt;- predict(svm_mod, data_test)\nsvm_pred |&gt; as_tibble() |&gt; count(value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;fct&gt; &lt;int&gt;\n1 0        69\n2 1        56\n\n\n\nmap_dbl(\n  data$estimates,\n  ~rbinom(n = 1, size = 1, prob = .x)\n)\n\n  [1] 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n [38] 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1\n [75] 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n[112] 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1\n[149] 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0\n[186] 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n[223] 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n[260] 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1\n[297] 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n[334] 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n[371] 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n[408] 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n[445] 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1\n[482] 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0\n\n\n\nprobably_threshold &lt;- function(\n  r6,\n  task,\n  threshold,\n  output = TRUE\n){\n  thresholds &lt;- c(threhold, 1 - threshold)\n  names(thresholds) &lt;- {{task}}$class_names\n\n  if(output == TRUE){\n    confuse &lt;- {{r6}}$clone(deep = TRUE)$set_threshold(threhsolds)$confusion |&gt; t()\n    score &lt;- {{r6}}$clone(deep = TRUE)$set_threshold$score(msrs(c(\"classif.mcc\", \"classif.acc\", \"classif.auc\", \"classif.ce\", \"classif.sensitivity\", \"classif.specificity\")))\n\n    list(confuse, score)\n  }\n  else{\n    preds &lt;- {{r6}}$clone(deep = TRUE)$set_threshold(thresholds)\n\n    preds\n  }\n}"
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html",
    "title": "Bayes Nets",
    "section": "",
    "text": "As I am continuing to grow in understanding and conducting bayesian networks, this page and series may change in the future. -JP\nOkay, I will be the first to state that I am not an expert in the field of conducting psychometric models, Bayesian networks, Bayesian analyses, but I have been struggling to find any blog posts about conducting a bayes net with latent variables that uses the programming language Stan. The purpose of this post is to walk through Stan and some bayes net terminology to get a basic understanding of some psychometric models conducting using Bayesian inference.\nTo get started, make sure you follow the detailed instructions on installing RStan. I know if using Mac, make sure to also download Xcode so that Stan will work correctly. For this post, I will be doing all my programming in R, while calling on Stan to conduct the Markov Chain Monte Carlo (MCMC) sampling. Maybe a future post will follow this tutorial using PyStan or Cmdstanpy but there are just more readily available tools using R so I will be using R instead. Additionally, I will be creating dichotomous data that will represent an education assessment where a 1 indicates that a student has answered the item correctly and a 0 indicates they did not answer the item correctly. The model will also include three latent attributes/skills/variables where a 1 would indicate that the student has mastered the skill and a 0 would indicate that they do not have mastery of the skill.\nWhile I will be discussing bayes net through an educational measurement lens, bayes net can be used outside of education to show that individuals have skills that are not directly measured. Instead of items on an assessment, tasks that capture each skill can be assessed. Before walking through some bayes net terminology, it is important to note that this model is simply for educational purposes. Components of the psychometric models I will be writing about (e.g., Diagnostic Classification Model (DCMs) and bayes net) require expert opinion. For example, DCMs and bayes net models require expert opinions on the assignment of items to skills. Additionally, bayes net models require expert opinion on the priors for the lambda (\\(\\lambda\\)) parameters.\nSince there is different opinions on using different terms, I am going to stick to the following terms.\nFor this introductory post into bayes net, I thought it would be best to create some artificial data and show visually the models I will be planning on creating using R and Stan. I will be using cmdstanr instead of rstan for my Stan computations. The main difference between the two packages is that rstan avoids using R6 classes, while cmdstanr uses R6 classes. If you’d like more information on trade-offs of different object-oriented programming classes, you can read more here. Finally, I will state that while this is introductory to a bayes net model, this post assumes that you have a decent understanding of Bayesian inference."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#q-matrix",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#q-matrix",
    "title": "Bayes Nets",
    "section": "Q Matrix",
    "text": "Q Matrix\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nq_matrix |&gt;\n  react_table()\n\n\n\n\n\nOkay, now on to the Q-matrix. As previously stated, I am creating this q-matrix to be as simple as possible. This means that in a realistic scenario, you would either want to use a structural learning algorithm to see what nodes have edges to our three latent nodes, or you should probably have experts on your latent attributes to declare what items measure what latent attribute.\nAbove, I created a q-matrix that follows a pattern where each attribute has 5 items that correspond to that attribute. The table above allows you to search which items correspond to each attribute by typing 1 into the filter bar above each column."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#attribute-profile-matrix",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#attribute-profile-matrix",
    "title": "Bayes Nets",
    "section": "Attribute Profile Matrix",
    "text": "Attribute Profile Matrix\nIf we only wanted to examine how the posterior distributions compare to each student and their responses, then I would only need to have my student data and the Q-matrix. However, I also want to put students into latent classes. Because I want to put students into latent classes, I also have to create an attribute profile matrix. I am going to create this matrix by creating every possible combination of skills, which will create every potential latent class. Then I will just add each row as a numbered class. Below is the final matrix created for 3 skills.\n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\nalpha |&gt; react_table()\n\n\n\n\n\nNote: Latent classes are different from our latent nodes/attributes/skills. The matrix created above (alpha) is a matrix where each row is a different latent class and each column corresponds to each of the skills.\nSo now we have everything to build our bayes net model. Before we get to that, I do want to visually show the three models I will be creating in this series."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#naive-bayes",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#naive-bayes",
    "title": "Bayes Nets",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nnaive_dag &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - lambda1\" [latent,pos=\"0.175,0.076\"]\n\"Q-matrix\" [pos=\"0.874,0.402\"]\natt1 [latent,pos=\"0.220,0.209\"]\natt2 [latent,pos=\"0.488,0.182\"]\natt3 [latent,pos=\"0.709,0.169\"]\ndelta [latent,pos=\"0.481,0.421\"]\nfalse_positive [latent,pos=\"0.572,0.888\"]\nlambda1 [latent,pos=\"0.252,0.082\"]\nlambda20 [latent,pos=\"0.450,0.076\"]\nlambda21 [latent,pos=\"0.522,0.081\"]\nlambda30 [latent,pos=\"0.679,0.068\"]\nlambda31 [latent,pos=\"0.741,0.069\"]\ntrue_positive [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - lambda1\" -&gt; att1\n\"Q-matrix\" -&gt; delta\natt1 -&gt; delta\natt2 -&gt; delta\natt3 -&gt; delta\ndelta -&gt; y1\ndelta -&gt; y10\ndelta -&gt; y11\ndelta -&gt; y12\ndelta -&gt; y13\ndelta -&gt; y14\ndelta -&gt; y15\ndelta -&gt; y2\ndelta -&gt; y3\ndelta -&gt; y4\ndelta -&gt; y5\ndelta -&gt; y6\ndelta -&gt; y7\ndelta -&gt; y8\ndelta -&gt; y9\nfalse_positive -&gt; y1\nfalse_positive -&gt; y10\nfalse_positive -&gt; y11\nfalse_positive -&gt; y12\nfalse_positive -&gt; y13\nfalse_positive -&gt; y14\nfalse_positive -&gt; y15\nfalse_positive -&gt; y2\nfalse_positive -&gt; y3\nfalse_positive -&gt; y4\nfalse_positive -&gt; y5\nfalse_positive -&gt; y6\nfalse_positive -&gt; y7\nfalse_positive -&gt; y8\nfalse_positive -&gt; y9\nlambda1 -&gt; att1\nlambda20 -&gt; att2\nlambda21 -&gt; att2\nlambda30 -&gt; att3\nlambda31 -&gt; att3\ntrue_positive -&gt; y1\ntrue_positive -&gt; y10\ntrue_positive -&gt; y11\ntrue_positive -&gt; y12\ntrue_positive -&gt; y13\ntrue_positive -&gt; y14\ntrue_positive -&gt; y15\ntrue_positive -&gt; y2\ntrue_positive -&gt; y3\ntrue_positive -&gt; y4\ntrue_positive -&gt; y5\ntrue_positive -&gt; y6\ntrue_positive -&gt; y7\ntrue_positive -&gt; y8\ntrue_positive -&gt; y9\n}\n')\n\nggdag(naive_dag) + theme_dag()\n\n\n\n\n\n\n\n\nThe first model I will go over is essentially a naive bayes model; however, naive bayes models do not correct for what I have labeled as true positive and false positive probabilities. These are priors that will be discussed in the next post. This model mimics a deterministic inputs, noisy “and” gate (DINA) model. Essentially, the model assumes that each student has mastered all skills in order to correctly respond to an assessment item. See here for an excellent post about the DINA model and what the bayes netmodel will compare to in this series"
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#dcm",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#dcm",
    "title": "Bayes Nets",
    "section": "DCM",
    "text": "DCM\nThe DCM is somewhat difficult to visualize. Based on the model in the blog post above, the skills are not determined by priors and instead of a delta parameter, the parameter is already created in the data. I will talk shortly about that when conducting the DCM. Additionally, while the other two models have priors for true positives and false positives, the DCM includes similar parameters with prior distributions; the slip (essentially the student slipped up and made a mistake even though they have the skills) parameter and the guess (got the answer correctly but do not have the necessary skills) parameter. Those priors are different from the ones created for the bayes net models so we’ll talk about those next post."
  },
  {
    "objectID": "posts/2024-07-09-bayes-net-introduction/index.html#bayes-net",
    "href": "posts/2024-07-09-bayes-net-introduction/index.html#bayes-net",
    "title": "Bayes Nets",
    "section": "Bayes Net",
    "text": "Bayes Net\n\nbayes_net &lt;- dagitty('dag {\nbb=\"0,0,1,1\"\n\"1 - lambda1\" [latent,pos=\"0.175,0.076\"]\n\"Q-matrix\" [pos=\"0.874,0.402\"]\natt1 [latent,pos=\"0.220,0.209\"]\natt2 [latent,pos=\"0.488,0.182\"]\natt3 [latent,pos=\"0.709,0.169\"]\ndelta [latent,pos=\"0.481,0.421\"]\nfalse_positive [latent,pos=\"0.572,0.888\"]\nlambda1 [latent,pos=\"0.252,0.082\"]\nlambda20 [latent,pos=\"0.450,0.076\"]\nlambda21 [latent,pos=\"0.522,0.081\"]\nlambda30 [latent,pos=\"0.679,0.068\"]\nlambda31 [latent,pos=\"0.741,0.069\"]\ntrue_positive [latent,pos=\"0.380,0.890\"]\ny1 [pos=\"0.124,0.652\"]\ny10 [pos=\"0.240,0.653\"]\ny11 [pos=\"0.511,0.648\"]\ny12 [pos=\"0.770,0.645\"]\ny13 [pos=\"0.276,0.654\"]\ny14 [pos=\"0.544,0.646\"]\ny15 [pos=\"0.814,0.643\"]\ny2 [pos=\"0.403,0.649\"]\ny3 [pos=\"0.658,0.657\"]\ny4 [pos=\"0.164,0.652\"]\ny5 [pos=\"0.442,0.648\"]\ny6 [pos=\"0.693,0.652\"]\ny7 [pos=\"0.200,0.653\"]\ny8 [pos=\"0.476,0.647\"]\ny9 [pos=\"0.732,0.648\"]\n\"1 - lambda1\" -&gt; att1\n\"Q-matrix\" -&gt; delta\natt1 -&gt; att2\natt1 -&gt; delta\natt2 -&gt; att3\natt2 -&gt; delta\natt3 -&gt; delta\ndelta -&gt; y1\ndelta -&gt; y10\ndelta -&gt; y11\ndelta -&gt; y12\ndelta -&gt; y13\ndelta -&gt; y14\ndelta -&gt; y15\ndelta -&gt; y2\ndelta -&gt; y3\ndelta -&gt; y4\ndelta -&gt; y5\ndelta -&gt; y6\ndelta -&gt; y7\ndelta -&gt; y8\ndelta -&gt; y9\nfalse_positive -&gt; y1\nfalse_positive -&gt; y10\nfalse_positive -&gt; y11\nfalse_positive -&gt; y12\nfalse_positive -&gt; y13\nfalse_positive -&gt; y14\nfalse_positive -&gt; y15\nfalse_positive -&gt; y2\nfalse_positive -&gt; y3\nfalse_positive -&gt; y4\nfalse_positive -&gt; y5\nfalse_positive -&gt; y6\nfalse_positive -&gt; y7\nfalse_positive -&gt; y8\nfalse_positive -&gt; y9\nlambda1 -&gt; att1\nlambda20 -&gt; att2\nlambda21 -&gt; att2\nlambda30 -&gt; att3\nlambda31 -&gt; att3\ntrue_positive -&gt; y1\ntrue_positive -&gt; y10\ntrue_positive -&gt; y11\ntrue_positive -&gt; y12\ntrue_positive -&gt; y13\ntrue_positive -&gt; y14\ntrue_positive -&gt; y15\ntrue_positive -&gt; y2\ntrue_positive -&gt; y3\ntrue_positive -&gt; y4\ntrue_positive -&gt; y5\ntrue_positive -&gt; y6\ntrue_positive -&gt; y7\ntrue_positive -&gt; y8\ntrue_positive -&gt; y9\n}\n')\n\nggdag(bayes_net) + theme_dag()\n\n\n\n\n\n\n\n\nLastly, the bayes net model is similar to the first model; however, now there are edges between the 3 skills. Other than that, nothing else has changed. In the next post I will be estimating the first bayes net model and doing some posterior checks to see how the model works."
  },
  {
    "objectID": "posts/2024-07-10-bayes-net-part2-estimation/index.html",
    "href": "posts/2024-07-10-bayes-net-part2-estimation/index.html",
    "title": "Bayes Net Pt. 2",
    "section": "",
    "text": "Under Development - Not Complete\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n\ntheme_set(theme_light())\noptions(\n  mc.cores = parallel::detectCores(),\n  scipen = 9999\n)\ncolor_scheme_set(\"viridis\")\n\nreact_table &lt;- function(data){\n  reactable::reactable(\n    {{data}},\n    filterable = TRUE,\n    sortable = TRUE,\n    highlight = TRUE,\n    searchable = TRUE\n  )\n  }\n\nAs mentioned in the previous post, the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the cmdstanr package to call on Stan for the Bayesian analyses, I am using the posterior package to manipulate the chains, iterations, and draws from the analyses and the bayesplot package to visualize the convergence of each parameter included in the bayes net model. I also love to use whatever table producing package I am interested at the time and create a function with html functionality. Specifically, I always include a feature to filter and highlight specific rows. This time I decided to use the reactable package.\n\nset.seed(12345)\nbern_dist &lt;- function(prob_value)(\n  rbinom(n = 30, size = 1, prob = prob_value)\n)\n\ny &lt;- tibble(\n  y1 = bern_dist(prob = .7),\n  y2 = bern_dist(prob = .74),\n  y3 = bern_dist(prob = .88),\n  y4 = bern_dist(prob = .90),\n  y5 = bern_dist(prob = .64),\n  y6 = bern_dist(prob = .61),\n  y7 = bern_dist(prob = .79),\n  y8 = bern_dist(prob = .89),\n  y9 = bern_dist(prob = .81),\n  y10 = bern_dist(prob = .54),\n  y11 = bern_dist(prob = .60),\n  y12 = bern_dist(prob = .46),\n  y13 = bern_dist(prob = .37),\n  y14 = bern_dist(prob = .3),\n  y15 = bern_dist(prob = .65),\n) |&gt;\n  rowid_to_column() |&gt;\n  rename(\n    studentid = rowid\n  )\n\nq_matrix &lt;- tibble(\n  item_id = map_chr(1:15, ~paste0(\"y\", .x)),\n  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),\n  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),\n  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)\n) \n\nskills &lt;- 3\nskill_combo &lt;- rep(list(0:1), skills)\nalpha &lt;- expand.grid(skill_combo)\n\nalpha &lt;- alpha |&gt;\n  rename(\n    att1 = Var1,\n    att2 = Var2,\n    att3 = Var3\n  ) |&gt;\n  mutate(\n    class = seq(1:nrow(alpha)),\n    .before = att1\n  )\n\nThe code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.\n\nstan_file &lt;- list(\n  J = nrow(y[,-1]),\n  I = ncol(y[,-1]),\n  K = ncol(q_matrix[,-1]),\n  C = nrow(alpha),\n  X = y[,-1],\n  Q = q_matrix[, -1],\n  alpha = alpha[,-1]\n)\n\nNext, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The J, I, K, and C list values are all important for looping through:\n\nJ = The number of rows of data; in this case there are 30 “students”\nI = The number of columns in the dataset; which is 15 excluding the first column\nK = The number of latent attributes/skills\nC = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.\n\nAdditionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from y for the actual data and then X in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.\n\nset.seed(12345)\nmod &lt;- cmdstan_model(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan\"))\n\nWarning in readLines(stan_file): incomplete final line found on\n'C:/Users/Jonathan/Documents/GitHubRepos/log-of-jandp/posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan'\n\nfit &lt;- mod$sample(\n  data = stan_file,\n  seed = 12345,\n  iter_warmup = 2000,\n  iter_sampling = 2000\n)\n\nRunning MCMC with 4 chains, at most 12 in parallel...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 37.1 seconds.\nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 40.9 seconds.\nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 41.0 seconds.\nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 41.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 40.1 seconds.\nTotal execution time: 41.4 seconds.\n\n# fit$save_object(\"simple_bayes_net.RDS\")\n\nSo this next part will be different depending on whether or not you are using RStan or like in this case cmdstanR. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For cmdstanR, you call on your Stan file. Below is the Stan code or if you’d like to see it side-by-side, the Stan file can be found here. I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations.\n\n\n# fit &lt;- read_rds(here::here(\"posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.RDS\"))\n\nfit$diagnostic_summary()\n\n$num_divergent\n[1] 0 0 0 0\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.9748978 0.9855169 0.9468038 0.9679686\n\nbn_converge &lt;- summarize_draws(fit$draws(), default_convergence_measures())\nbn_measure &lt;- summarize_draws(fit$draws(), default_summary_measures())\n\nbn_converge |&gt; arrange(desc(rhat)) |&gt; head()\n\n# A tibble: 6 × 4\n  variable      rhat ess_bulk ess_tail\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__          1.00    3019.    4683.\n2 pie           1.00   11711.    6570.\n3 log_item[15]  1.00   11711.    6570.\n4 x_rep[1,1]    1.00   11711.    6570.\n5 x_rep[2,1]    1.00   11711.    6570.\n6 x_rep[3,1]    1.00   11711.    6570.\n\nbn_measure |&gt; mutate(across(-variable, ~round(.x, 3))) |&gt; react_table()\n\n\n\n\n\nI also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.\n\ny_rep &lt;- fit$draws(\"x_rep\") |&gt; as_draws_matrix()\nstu_resp_attr &lt;- fit$draws(\"prob_resp_attr\") |&gt; as_draws_matrix()\n\nI decided to extract the replicated values for the items and the probabilities oof each student’s mastery of each of the three latent attributes.\n\nmcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +\n  scale_y_continuous(limits = c(0, 1))\n\n\n\n\n\n\n\ny |&gt; react_table()\n\n\n\n\n\nNext, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the originaly data, the original responses and the probabilities with a probability threshold of 0.5 match one another.\n\nmcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\nmcmc_areas(exp(y_rep[,seq(1, 450, 30)]))\n\n\n\n\n\n\n\nppc_intervals(\n  y = y |&gt; pull(y1) |&gt; as.vector(),\n  yrep = exp(y_rep[, 1:30])\n) +\ngeom_hline(yintercept = .5, color = \"black\", linetype = 2) +\ncoord_flip()\n\n\n\n\n\n\n\n\nI enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.\n\nlibrary(loo)\n\nThis is loo version 2.8.0\n\n\n- Online documentation and vignettes at mc-stan.org/loo\n\n\n- As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. \n\n\n- Windows 10 users: loo may be very slow if 'mc.cores' is set in your .Rprofile file (see https://github.com/stan-dev/loo/issues/94).\n\nloo(y_rep)\n\n\nComputed from 8000 by 450 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -303.2  7.6\np_loo         8.1  0.3\nlooic       606.4 15.2\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume independent draws (r_eff=1).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nwaic(y_rep)\n\n\nComputed from 8000 by 450 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -303.2  7.6\np_waic         8.1  0.3\nwaic         606.4 15.2\n\nbn_resid &lt;- y[,-1] - exp(y_rep)\n\nbn_resid^2 |&gt; \n  as_tibble() |&gt;\n  rowid_to_column() |&gt;\n  ggplot(\n    aes(\n      rowid,\n      y2\n    )\n  ) +\n  geom_point(\n    alpha = .7\n  )\n\n\n\n\n\n\n\n\n\nactual_stu_resp_attr &lt;- tibble(\n  studentid = 1:nrow(y),\n  att1 = runif(nrow(y), 0, 1),\n  att2 = runif(nrow(y), 0, 1),\n  att3 = runif(nrow(y), 0, 1)\n) |&gt;\n  mutate(\n    across(\n      -studentid,\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nThe last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.\n\nstu_resp_attr_mean &lt;- stu_resp_attr |&gt;\n  as_tibble() |&gt;\n  summarize(\n    across(\n      everything(),\n      ~mean(.x)\n      )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_mean |&gt;\n  mutate(\n    across(\n      everything(),\n      ~if_else(.x &gt; .5, 1, 0)\n    )\n  )\n\nstu_resp_attr_class &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(\n    everything()\n  ) |&gt;\n  separate(\n    name,\n    into = c(\"stu\", \"att\"),\n    sep = \",\"\n  ) |&gt;\n  mutate(\n    stu = str_remove(stu, \"\\\\[\"),\n    att = str_remove(att, \"\\\\]\"),\n    att = paste0(\"att\", att),\n    stu = str_remove(stu, \"prob_resp_attr\")\n  ) |&gt;\n  pivot_wider(\n    names_from = att,\n    values_from = value\n  )\n\nFor the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute.\n\nmap2(\n  stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~table(.x, .y)\n)\n\n$att1\n   .y\n.x   0  1\n  1 10 20\n\n$att2\n   .y\n.x   0  1\n  1 11 19\n\n$att3\n   .y\n.x   0  1\n  0  1  2\n  1 15 12\n\nmap2(\n stu_resp_attr_class[,2:4],\n  actual_stu_resp_attr[,2:4],\n  ~prop.table(\n    table(.x, .y)\n  )\n)\n\n$att1\n   .y\n.x          0         1\n  1 0.3333333 0.6666667\n\n$att2\n   .y\n.x          0         1\n  1 0.3666667 0.6333333\n\n$att3\n   .y\n.x           0          1\n  0 0.03333333 0.06666667\n  1 0.50000000 0.40000000\n\n\nAs shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model.\n\nstu_resp_attr_long &lt;- stu_resp_attr_class |&gt;\n  pivot_longer(-stu)\n\nactual_stu_resp_attr_long &lt;- actual_stu_resp_attr |&gt;\n  pivot_longer(-studentid)\n\naccuracy_att &lt;- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)\naccuracy_att\n\n[1] 0.5777778\n\n\nFinally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of 0.5777778. This is a good starting point, but this may indicate that the model needs better definied priors and may require the edges between the attributes to show latent relationships."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html",
    "title": "Using Typst To Create Documents",
    "section": "",
    "text": "Under Development - Not Complete\nI have been using Typst, an awesome app for working on pdf files at the same time as colleagues. You could think of this as something similar to Google Docs or GitHub for code. This also got me thinking about creating a small series of blog posts about using Typst and then creating Typst templates for documents using Quarto. The latter topics would be using Quarto extensions and if following along, you would need Quarto version 1.4 at least to be able to use Typst code chunks on a Quarto document. So first, I will show the Typst file I will be using because let’s face it I’m on the job market and free publicity is always good.\n.\nHere is the link for the Typst resume to view. If you want, you can just copy and paste that into Typst and change the information. I will walk through each section of the document with Typst code in the post, as well as a cover letter post, and then end the series with a Quarto extension to create a Typst template so you can just write your resume and/or cover letter in Quarto."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#typst-documentation",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#typst-documentation",
    "title": "Using Typst To Create Documents",
    "section": "Typst Documentation",
    "text": "Typst Documentation\nI will be the first to state that Typst documentation is a little difficult to follow at first. Hopefully with this tutorial you will get a better understanding of the basics of Typst code. As someone who tried to learn LaTeX to edit the previous resume I had found a template for, I wish Typst existed earlier."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#creating-a-typst-document",
    "title": "Using Typst To Create Documents",
    "section": "Creating a Typst Document",
    "text": "Creating a Typst Document\nWorking in the Typst app is pretty straightforward with you creating a username, followed by your dashboard with nothing there. This will be the location of all of your documents as you get started with Typst. While there are Typst templates already for resumes I really wanted to create something similar to the resume I had in LaTeX. Working in Quarto, you will have to learn how to create Typst code chunks. They are slightly different from other languages’ code chunks but you can still use all of the Quarto code chunk arguments.\nFrom what I have seen online, there does not seem to be much difference in the ordering of some of the beginning Typst documentation. I have decided to start my Typst document with any variables I will be including, followed by any Typst packages I will need, and then setting up the general parameters for the document. These general parameters are for the document overall. Things like setting the font to a specific font, size and maybe weight would be a good parameter to set at the top of your document."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#breakdown-of-variables-packages-and-set-function-for-parameters",
    "title": "Using Typst To Create Documents",
    "section": "Breakdown of Variables, Packages, and # Set function for parameters",
    "text": "Breakdown of Variables, Packages, and # Set function for parameters\n\n//Variables\n#let name = [Jonathan A. Pedroza Ph.D]\n\n//Packages\n#import \"@preview/tablex:0.0.8\": tablex\n#import \"lib-gen.typ\": *\n#import \"lib-impl.typ\": *\n#import \"lib.typ\": *\n\n#set page(\n  margin: (\n    top: 0cm,\n    bottom: 0cm,\n    left: .5cm,\n    right: 0cm\n  )\n)\n\n#set block(spacing: 0.5em)\n\n#set rect(\n  width: 37%,\n  height: 100%\n)\n\nAbove is the beginning Typst code for the resume I created. I’m going to walk through the code a little, but for more detailed information, check out the help documentation. The // syntax can be included wherever to include comments. Since I have been showing others how to use Typst to create quick pretty PDFs, I have been including a lot of comments for things like variables. To create variables in Typst, you will need to use the #let function followed by your variable name, an equal sign, and the information you want to include. So I created the variable name, which would be used as #name in Typst and the document will spell out my full name. After that, just as the comment states, I included the tablex package, which I have found to be useful for creating tables and grids. If you’d like you can use the #table or #grid functions from Typst. Additionally, I also included the files for using the FontAwesome Typst package. You can find all the icons and other information about FontAwesome at the FontAwesome website. I was just being lazy as I wanted to create my resume quick so I could get it out into the world ASAP. Next I set the margins to maximize the amount of space I would have for my resume and I created a block after the titles Education and Professional Experience since I did not want the default amount of space before my education and experience entries. The #set function creates rules for the document as a whole. So For the whole document I have the same margins throughout, a block to create more space between headers and text underneath and the rectangle to separate the sections of the document. Lastly, I set a rectangle for 37% of the document’s width and 100% of the height. There are other metrics that can be used to create the rectangle but I personally was enjoying using percentages for this document. This rectangle is for the right side of the document that includes the contact information."
  },
  {
    "objectID": "posts/2024-07-10-resume-cover-letter-typst/index.html#grid-of-entries",
    "href": "posts/2024-07-10-resume-cover-letter-typst/index.html#grid-of-entries",
    "title": "Using Typst To Create Documents",
    "section": "Grid of Entries",
    "text": "Grid of Entries\n\n#grid(\n  columns: (70%, 82%),\n  [\n    #linebreak()\n\n    #set text(\n      font: \"Source Sans Pro\",\n      size: 10pt\n    )\n    #set align(center)\n    \n    = Education #fa-graduation-cap()\n    #line(\n      length: 94%,\n      stroke: black\n    )\n\nI decided to separate some of these functions because it might be easier to talk about. So the grid here is actually separating the main text and the contact information text. As I am reading this, I can see that I probably should have set the font to be 10pt throughout the document rather than within the grid."
  }
]