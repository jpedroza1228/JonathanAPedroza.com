---
title: "Bayes Net Pt. 2" 
subtitle: |
  Estimation
image: /posts/2024-07-10-bayes-net-part2-estimation/network_playground.jpg
categories: [Bayesian, Bayesian Network, bayes net, R, stan, cmdstanr]
date: 2024-07-10
# citation:
  # url: 
execute:
    freeze: true
params:
  slug: Bayes-Net-part-2
  date: 2024-07-10
---

**Under Development - Not Complete**

```{r}
```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false

library(tidyverse)
library(cmdstanr)
library(posterior)
library(bayesplot)

theme_set(theme_light())
options(
  mc.cores = parallel::detectCores(),
  scipen = 9999
)
color_scheme_set("viridis")

react_table <- function(data){
  reactable::reactable(
    {{data}},
    filterable = TRUE,
    sortable = TRUE,
    highlight = TRUE,
    searchable = TRUE
  )
  }
```


As mentioned in the [previous post](https://log-of-jandp.com/posts/2024-07-09-bayes-net-introduction/), the first model I will be running in Stan is a Bayesian Network with 3 latent attributes with edges from these attributes to the 15 observed items. I will be creating the data and I am sure there will be issues since the items and the Q-matrix are being created randomly. Along with using the `cmdstanr` package to call on Stan for the Bayesian analyses, I am using the `posterior` package to manipulate the chains, iterations, and draws from the analyses and the `bayesplot` package to visualize the convergence of each parameter included in the bayes net model. I also love to use whatever table producing package I am interested at the time and create a function with html functionality. Specifically, I always include a feature to filter and highlight specific rows. This time I decided to use the `reactable` package. 

```{r}
set.seed(12345)
bern_dist <- function(prob_value)(
  rbinom(n = 30, size = 1, prob = prob_value)
)

y <- tibble(
  y1 = bern_dist(prob = .7),
  y2 = bern_dist(prob = .74),
  y3 = bern_dist(prob = .88),
  y4 = bern_dist(prob = .90),
  y5 = bern_dist(prob = .64),
  y6 = bern_dist(prob = .61),
  y7 = bern_dist(prob = .79),
  y8 = bern_dist(prob = .89),
  y9 = bern_dist(prob = .81),
  y10 = bern_dist(prob = .54),
  y11 = bern_dist(prob = .60),
  y12 = bern_dist(prob = .46),
  y13 = bern_dist(prob = .37),
  y14 = bern_dist(prob = .3),
  y15 = bern_dist(prob = .65),
) |>
  rowid_to_column() |>
  rename(
    studentid = rowid
  )

q_matrix <- tibble(
  item_id = map_chr(1:15, ~paste0("y", .x)),
  att1 = c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0),
  att2 = c(0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0),
  att3 = c(0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1)
) 

skills <- 3
skill_combo <- rep(list(0:1), skills)
alpha <- expand.grid(skill_combo)

alpha <- alpha |>
  rename(
    att1 = Var1,
    att2 = Var2,
    att3 = Var3
  ) |>
  mutate(
    class = seq(1:nrow(alpha)),
    .before = att1
  )
```

The code above is from the previous post as well that discusses the creation of the binary data, the Q-matrix, and the attribute profile matrix.

```{r}
stan_file <- list(
  J = nrow(y[,-1]),
  I = ncol(y[,-1]),
  K = ncol(q_matrix[,-1]),
  C = nrow(alpha),
  X = y[,-1],
  Q = q_matrix[, -1],
  alpha = alpha[,-1]
)
```

Next, it is easiest to put your Stan data into a list. So here I take all the tibbles I created for my data, the Q-matrix, and the attribute profile matrix and only call on the columns and rows that are crucial for the bayes net model. For instance, The **J**, **I**, **K**, and **C** list values are all important for looping through:

- J = The number of rows of data; in this case there are 30 "students"

- I = The number of columns in the dataset; which is 15 excluding the first column

- K = The number of latent attributes/skills

- C = The number of rows in the attribute profile matrix. Each row is a latent class with a different set of attribute mastery proficiencies.

Additionally, we also need to include the actual data to be referenced in our analysis in Stan. For some reason, I decided to shift from **y** for the actual data and then **X** in the analyses. I think I did this from some of the resources I used having X and then other resources using y. In any case, I will draw attention to the data and reference back to the values either being y or X.

```{r}
#| echo: true
#| eval: true

set.seed(12345)
mod <- cmdstan_model(here::here("posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan"))

fit <- mod$sample(
  data = stan_file,
  seed = 12345,
  iter_warmup = 2000,
  iter_sampling = 2000
)

# fit$save_object("simple_bayes_net.RDS")
```

So this next part will be different depending on whether or not you are using `RStan` or like in this case `cmdstanR`. If you look up the RStan documentation, I am sure you can find a beginner-friendly tutorial that shows how you would conduct the analysis. For `cmdstanR`, you call on your Stan file. Below is the Stan code or if you'd like to see it side-by-side, the Stan file can be found [here](https://raw.githubusercontent.com/jpedroza1228/log-of-jandp/main/posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.stan). I have kept the defaults and I always include the arguments for the number of warmup iterations and the sampling iterations, just in case I want to make changes to the number of iterations. 

```{.stan include="simple_bayes_net.stan"}

```

```{r}
#| echo: true
#| eval: true

# fit <- read_rds(here::here("posts/2024-07-10-bayes-net-part2-estimation/simple_bayes_net.RDS"))

fit$diagnostic_summary()

bn_converge <- summarize_draws(fit$draws(), default_convergence_measures())
bn_measure <- summarize_draws(fit$draws(), default_summary_measures())

bn_converge |> arrange(desc(rhat)) |> head()
bn_measure |> mutate(across(-variable, ~round(.x, 3))) |> react_table()
```

I also included the summarized convergence measures and summary statistics for all of the draws. I also will create an interactive table to look over the posterior draws of each parameter.

```{r}
#| echo: true
#| eval: true

y_rep <- fit$draws("x_rep") |> as_draws_matrix()
stu_resp_attr <- fit$draws("prob_resp_attr") |> as_draws_matrix()
```

I decided to extract the replicated values for the items and the probabilities oof each student's mastery of each of the three latent attributes.  

```{r}
#| echo: true
#| eval: true

mcmc_trace(exp(y_rep[,seq(1, 450, 30)])) +
  scale_y_continuous(limits = c(0, 1))

y |> react_table()
```

Next, I decided to compare the probabilities of each item for one student by having a sequence going from 1 to 450 (end of the columns in the matrix) to get the draws for the first student. As we can see from the traceplots for the first student and the originaly data, the original responses and the probabilities with a probability threshold of 0.5 match one another.  

```{r}
#| echo: true
#| eval: true

mcmc_intervals(exp(y_rep[,seq(1, 450, 30)]))

mcmc_areas(exp(y_rep[,seq(1, 450, 30)]))

ppc_intervals(
  y = y |> pull(y1) |> as.vector(),
  yrep = exp(y_rep[, 1:30])
) +
geom_hline(yintercept = .5, color = "black", linetype = 2) +
coord_flip()
```

I enjoy using traceplots just to see how the iterations are doing and that they look like radio static. If we do not wish to use traceplots, we can also use intervals or areas. Both of which show the credible intervals of our probabilities. Lastly, there is the option to use the posterior predictive check intervals where we can compare the probabilities to the actual responses in the same plot. At least for the first student we can see that the replicated data matches the actual data well.

```{r}
#| echo: true
#| eval: true

library(loo)

loo(y_rep)
waic(y_rep)

bn_resid <- y[,-1] - exp(y_rep)

bn_resid^2 |> 
  as_tibble() |>
  rowid_to_column() |>
  ggplot(
    aes(
      rowid,
      y2
    )
  ) +
  geom_point(
    alpha = .7
  )
```

```{r}
#| echo: false
#| eval: false

y_pred_mean <- exp(y_rep) |>
  as_tibble() |>
  summarize(
    across(
      everything(),
      ~mean(.x)
      )
  )

y_pred_class <- y_pred_mean |>
  mutate(
    across(
      everything(),
      ~if_else(.x > .5, 1, 0)
    )
  )

y_pred_class <- y_pred_class |>
  pivot_longer(
    everything()
  ) |>
  separate(
    name,
    into = c("stu", "item"),
    sep = ","
  ) |>
  mutate(
    stu = str_remove(stu, "\\["),
    item = str_remove(item, "\\]"),
    item = paste0("item", item),
    stu = str_remove(stu, "x_rep")
  ) |>
  pivot_wider(
    names_from = item,
    values_from = value
  )

map2(
  y_pred_class[,-1],
  y[,-1],
  ~table(.x, .y)
)

map2(
  y_pred_class[,-1],
  y[,-1],
  ~prop.table(
    table(.x, .y)
  )
)

y_pred_long <- y_pred_class |>
  pivot_longer(-stu)

y_long <- y |>
  pivot_longer(-studentid)

accuracy <- mean(y_pred_long$value == y_long$value)
accuracy

precision <- sum(y_pred_long$value == 1 & y_long == 1) / sum(y_pred_long$value == 1)
recall <- sum(y_pred_long$value == 1 & y_long == 1) / sum(y_long == 1)
f1_score <- 2 * (precision * recall) / (precision + recall)

round(precision, 2)
round(recall, 2)
round(f1_score, 2)

library(pROC)

# Have to make 
# roc_curve <- roc(y, y_pred_mean)
# auc_value <- auc(roc_curve)

# print(paste("AUC: ", auc_value))
# plot(roc_curve, main = "ROC Curve")
```


```{r}
#| echo: true
#| eval: true

actual_stu_resp_attr <- tibble(
  studentid = 1:nrow(y),
  att1 = runif(nrow(y), 0, 1),
  att2 = runif(nrow(y), 0, 1),
  att3 = runif(nrow(y), 0, 1)
) |>
  mutate(
    across(
      -studentid,
      ~if_else(.x > .5, 1, 0)
    )
  )

```

The last thing I thought to do for this analysis was to create an artificial dataset of all the student attribute mastery responses. I will use this to compare the accuracy of our model in correctly classifying if students had mastery on all of the attributes.  

```{r}
#| echo: true
#| eval: true

stu_resp_attr_mean <- stu_resp_attr |>
  as_tibble() |>
  summarize(
    across(
      everything(),
      ~mean(.x)
      )
  )

stu_resp_attr_class <- stu_resp_attr_mean |>
  mutate(
    across(
      everything(),
      ~if_else(.x > .5, 1, 0)
    )
  )

stu_resp_attr_class <- stu_resp_attr_class |>
  pivot_longer(
    everything()
  ) |>
  separate(
    name,
    into = c("stu", "att"),
    sep = ","
  ) |>
  mutate(
    stu = str_remove(stu, "\\["),
    att = str_remove(att, "\\]"),
    att = paste0("att", att),
    stu = str_remove(stu, "prob_resp_attr")
  ) |>
  pivot_wider(
    names_from = att,
    values_from = value
  )
```

For the probabilities, first I will get the average of the draws for each student and their probability of mastery for each of the three attributes. I again used a threshold of 0.5 for classifying whether a student met mastery of each attribute and then did some manipulation to get the data into a wide format where each attribute is its own column and each student is a row. We can how see how well the model classifies these students into mastery of each attribute. 

```{r}
#| echo: true
#| eval: true

map2(
  stu_resp_attr_class[,2:4],
  actual_stu_resp_attr[,2:4],
  ~table(.x, .y)
)

map2(
 stu_resp_attr_class[,2:4],
  actual_stu_resp_attr[,2:4],
  ~prop.table(
    table(.x, .y)
  )
)
```

As shown above, we can see the counts and percentages for the classification of each attribute. We can see that the problem is that the model is predicting that students are mastering each attribute when they have not mastered the attribute. This is resulting in several false positives for the model. 

```{r}
#| echo: true
#| eval: true

stu_resp_attr_long <- stu_resp_attr_class |>
  pivot_longer(-stu)

actual_stu_resp_attr_long <- actual_stu_resp_attr |>
  pivot_longer(-studentid)

accuracy_att <- mean(stu_resp_attr_long$value == actual_stu_resp_attr_long$value)
accuracy_att
```

Finally, I pivoted the data to be long so I could compare the values for the actual responses and the posterior draws to show the accuracy of this model. The accuracy value shows that this model was not that accurate with a value of `r accuracy_att`. This is a good starting point, but this may indicate that the model needs better definied priors and may require the edges between the attributes to show latent relationships. 